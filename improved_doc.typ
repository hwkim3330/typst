#set page(
  paper: "a4",
  margin: (x: 2.2cm, y: 2.5cm),
  numbering: "1",
)

#set text(
  font: "Linux Libertine",
  size: 11pt,
  lang: "ko",
)

#set par(
  justify: true,
  leading: 0.75em,
  first-line-indent: 1em,
)

#set heading(numbering: "1.1")

#show heading.where(level: 1): it => {
  pagebreak(weak: true)
  v(1.5em)
  block(
    width: 100%,
    fill: rgb("#1e40af"),
    inset: 18pt,
    radius: 3pt,
    text(fill: white, size: 22pt, weight: "bold")[
      #counter(heading).display() #h(0.5em) #it.body
    ]
  )
  v(1.2em)
}

#show heading.where(level: 2): it => {
  v(1.2em)
  block(
    width: 100%,
    fill: rgb("#dbeafe"),
    inset: 12pt,
    radius: 2pt,
    text(fill: rgb("#1e40af"), size: 15pt, weight: "bold")[
      #counter(heading).display() #h(0.5em) #it.body
    ]
  )
  v(0.8em)
}

#show heading.where(level: 3): it => {
  v(1em)
  text(size: 13pt, fill: rgb("#1e40af"), weight: "bold")[
    #counter(heading).display() #h(0.5em) #it.body
  ]
  v(0.5em)
}

// 제목 페이지
#align(center)[
  #v(4cm)

  #block(
    width: 90%,
    fill: rgb("#1e40af"),
    inset: 25pt,
    radius: 5pt,
  )[
    #text(size: 32pt, weight: "bold", fill: white)[
      End-to-End 자율주행 기술의\
      현황과 미래 전망
    ]
  ]

  #v(1.5cm)

  #text(size: 18pt, fill: rgb("#334155"), weight: "semibold")[
    Foundation Models와 생성 AI 기반 자율주행 시스템의\
    기술적 진화와 산업 생태계 분석
  ]

  #v(3cm)

  #block(
    width: 70%,
    stroke: 1pt + rgb("#cbd5e1"),
    inset: 15pt,
    radius: 3pt,
  )[
    #text(size: 16pt, weight: "bold")[
      한국전자통신연구원 (KETI)
    ]

    #v(0.5em)

    #text(size: 13pt, fill: rgb("#64748b"))[
      지능형 모빌리티 연구센터
    ]
  ]

  #v(1cm)

  #text(size: 14pt, fill: rgb("#64748b"))[
    2025년 10월
  ]

  #v(2cm)

  #align(left)[
    #block(
      width: 100%,
      fill: rgb("#f8fafc"),
      inset: 15pt,
      radius: 3pt,
    )[
      #text(size: 10pt, fill: rgb("#475569"))[
        *요약*: 본 보고서는 자율주행 기술의 최신 패러다임인 End-to-End 학습 방식과 Foundation Models의 융합을 중심으로, 글로벌 자율주행 산업의 기술적 발전 현황과 주요 기업들의 상용화 전략을 종합적으로 분석한다. 특히 Tesla의 FSD, Waymo의 Robotaxi, 그리고 중국 시장의 급속한 발전을 심층 분석하고, 2025년부터 2030년까지의 기술 로드맵과 산업 전망을 제시한다.
      ]
    ]
  ]
]

#pagebreak()

// 목차
#outline(
  title: [목차],
  depth: 3,
  indent: auto,
)

#pagebreak()

= 서론

== 연구 배경 및 목적

자율주행 기술은 21세기 교통 혁명의 핵심으로 부상하고 있다. 지난 수십 년간 자율주행 시스템은 규칙 기반 알고리즘과 모듈식 아키텍처를 중심으로 발전해왔으나, 최근 인공지능 기술의 비약적 발전과 함께 End-to-End 학습 방식이라는 새로운 패러다임으로 전환되고 있다. 이러한 패러다임 전환은 단순히 기술적 접근 방법의 변화를 넘어, 자율주행 시스템의 근본적인 설계 철학과 개발 방법론의 혁신을 의미한다.

전통적인 모듈식 자율주행 시스템은 인지, 예측, 계획, 제어라는 네 가지 독립적인 모듈로 구성되어 있다. 각 모듈은 명확하게 정의된 입출력 인터페이스를 가지며, 순차적으로 정보를 처리하여 최종적으로 차량 제어 명령을 생성한다. 이러한 접근 방식은 시스템의 각 구성 요소를 독립적으로 개발하고 테스트할 수 있다는 장점이 있으며, 문제 발생 시 디버깅이 상대적으로 용이하다는 특징을 가진다. 또한 각 모듈의 동작을 사람이 이해하고 해석할 수 있어, 안전성 검증과 규제 준수 측면에서도 유리한 점이 많았다.

그러나 모듈식 접근 방식은 근본적인 한계를 내포하고 있다. 첫째, 각 모듈 간의 인터페이스에서 정보 손실이 불가피하게 발생한다. 예를 들어, 인지 모듈이 3차원 환경 정보를 객체 목록으로 축약하여 예측 모듈에 전달할 때, 원본 센서 데이터에 포함되어 있던 미세한 정보들이 소실된다. 둘째, 각 모듈이 독립적으로 최적화되기 때문에 전체 시스템의 관점에서는 차선의 성능을 보일 수 있다. 셋째, 모듈 간 오류가 누적되는 현상이 발생할 수 있으며, 특히 복잡한 도심 환경에서 이러한 문제가 두드러진다.

이와 대조적으로, End-to-End 학습 방식은 센서 입력에서 차량 제어 명령까지의 전체 과정을 하나의 신경망으로 직접 학습한다. 이는 1980년대 말 ALVINN 프로젝트에서 처음 제안된 개념이지만, 당시의 컴퓨팅 성능과 데이터 규모의 한계로 인해 실용화되지 못했다. 그러나 2010년대 중반 이후 딥러닝 기술의 발전, 특히 합성곱 신경망과 Transformer 아키텍처의 등장, 그리고 GPU 컴퓨팅 성능의 급격한 향상과 대규모 주행 데이터의 축적으로 인해 End-to-End 학습이 다시 주목받기 시작했다.

2016년 NVIDIA가 발표한 연구는 End-to-End 학습의 가능성을 보여주는 중요한 전환점이 되었다. 연구팀은 단순한 합성곱 신경망을 사용하여 카메라 이미지로부터 직접 조향각을 예측하는 시스템을 개발했으며, 명시적인 차선 검출이나 경로 계획 없이도 도로 주행이 가능함을 입증했다. 이후 Tesla는 Autopilot 시스템에 End-to-End 학습 요소를 점진적으로 도입하기 시작했으며, 2022년부터 본격적으로 Full Self-Driving 베타 버전에서 순수 비전 기반의 End-to-End 아키텍처를 채택했다.

최근 몇 년간 자율주행 분야에서 가장 주목할 만한 발전은 자연어 처리와 컴퓨터 비전 분야에서 성공을 거둔 Foundation Models의 도입이다. GPT, BERT, CLIP과 같은 대규모 사전 학습 모델들이 다양한 downstream 작업에서 탁월한 성능을 보이면서, 자율주행 분야에서도 유사한 접근 방식이 시도되고 있다. Vision Transformer는 이미지 인식 분야에서 기존 합성곱 신경망을 능가하는 성능을 보였으며, 자율주행의 인지 시스템에도 적용되기 시작했다. 특히 Bird's Eye View 표현 학습과 결합된 Transformer 기반 아키텍처는 다중 카메라의 정보를 효과적으로 통합하고 3차원 공간 이해 능력을 크게 향상시켰다.

생성 AI 기술 또한 자율주행 분야에 혁신을 가져오고 있다. World Models는 자율주행 시스템이 미래의 환경 변화를 예측하고 시뮬레이션할 수 있게 해주며, 이는 더 안전하고 지능적인 의사결정을 가능하게 한다. Diffusion Models는 현실적인 주행 시나리오를 생성하여 테스트 데이터를 증강하고, 드물게 발생하는 위험 상황에 대한 대응 능력을 향상시킨다. Wayve의 GAIA-1, NVIDIA의 Neural World Model과 같은 연구들은 생성 AI가 자율주행에서 단순한 보조 도구를 넘어 핵심 구성 요소가 될 수 있음을 보여주고 있다.

본 보고서는 이러한 기술적 진화의 맥락에서 End-to-End 자율주행 시스템의 현황을 종합적으로 분석하고자 한다. 구체적으로, 주요 학습 방법론인 Imitation Learning, Reinforcement Learning, Reinforcement Learning from Human Feedback의 원리와 적용 사례를 살펴보고, Foundation Models와 생성 AI가 자율주행 시스템에 어떻게 통합되고 있는지 분석한다. 또한 글로벌 주요 기업들의 기술 개발 현황과 상용화 전략을 조사하며, 특히 Tesla의 vision-only 접근 방식, Waymo의 Level 4 Robotaxi 서비스, 그리고 중국 시장의 급속한 발전을 중점적으로 다룬다.

더 나아가, 본 보고서는 자율주행 기술이 직면한 핵심 과제들을 식별하고 해결 방향을 제시한다. Long-tail 이벤트 처리, 안전성의 통계적 검증, 새로운 환경에 대한 일반화, 실시간 처리를 위한 컴퓨팅 효율성 등 기술적 과제뿐만 아니라, 규제 프레임워크, 윤리적 의사결정, 사회적 수용성과 같은 비기술적 과제들도 함께 논의한다. 마지막으로, 2025년부터 2030년까지의 기술 로드맵을 제시하고, 자율주행이 교통 시스템과 도시 구조, 나아가 우리의 생활 방식에 미칠 영향을 전망한다.

== 연구 방법 및 범위

본 연구는 문헌 조사, 산업 동향 분석, 기술 벤치마킹을 종합하는 다층적 접근 방식을 채택하였다. 먼저 학술 문헌 조사를 통해 End-to-End 학습의 이론적 기반과 최신 연구 동향을 파악하였다. 주요 학회인 CVPR, ICCV, NeurIPS, ICRA의 최근 3년간 논문들을 중심으로 분석하였으며, 특히 자율주행 관련 워크숍과 튜토리얼 자료를 참고하였다. arXiv 사전공개 서버를 통해 아직 출판되지 않은 최신 연구 동향도 파악하였다.

산업 동향 분석을 위해서는 주요 자율주행 기업들의 공개 자료, 기술 블로그, 투자자 발표 자료, 특허 출원 정보 등을 종합적으로 검토하였다. Tesla, Waymo, Cruise, Baidu Apollo 등 선도 기업들의 기술 개발 방향과 상용화 전략을 분석하였으며, 각 기업의 센서 구성, 컴퓨팅 플랫폼, 소프트웨어 아키텍처를 비교하였다. 또한 Gartner, McKinsey, ARK Invest 등의 산업 분석 보고서를 참고하여 시장 전망과 비즈니스 모델을 평가하였다.

기술 벤치마킹은 공개된 데이터셋과 리더보드를 기반으로 수행하였다. nuScenes, Waymo Open Dataset, Argoverse 등의 벤치마크에서 상위권을 차지한 알고리즘들의 특징을 분석하였으며, 이들의 성능 향상 추이를 통해 기술 발전 속도를 가늠하였다. 오픈소스 프로젝트인 CARLA 시뮬레이터, Apollo 플랫폼, Autoware 등도 참고하여 실제 구현 수준의 기술 세부사항을 파악하였다.

연구의 시간적 범위는 2020년부터 2025년 현재까지의 기술 발전을 중심으로 하되, 필요한 경우 2010년대 초기 연구까지 소급하여 기술 진화의 맥락을 제공하였다. 미래 전망은 2030년까지를 대상으로 하였으며, 이는 대부분의 산업 분석 기관들이 완전 자율주행의 상용화 시점으로 예측하는 시기와 일치한다.

지리적 범위는 전 세계를 대상으로 하되, 특히 미국, 중국, 유럽, 한국을 중점적으로 다루었다. 미국은 Waymo와 Tesla를 중심으로 한 기술 선도 지역으로, 중국은 빠른 상용화와 대규모 시장을 가진 지역으로, 유럽은 엄격한 안전 규제와 전통 자동차 산업의 전환 지역으로, 한국은 후발 주자로서의 도전과 기회를 가진 지역으로 각각 분석하였다.

기술적 범위는 End-to-End 학습을 핵심으로 하되, 이와 밀접하게 연관된 Foundation Models, 생성 AI, Bird's Eye View 표현, Transformer 아키텍처 등을 포괄하였다. 센서 기술, 차량 제어, 통신 기술 등 주변 기술들도 필요한 범위 내에서 다루었으나, 본 보고서의 주된 초점은 소프트웨어와 AI 알고리즘에 있다.

산업 분석은 OEM 자동차 제조사, Tier 1 부품 공급사, 자율주행 전문 기업, Robotaxi 서비스 제공자로 구분하여 각 주체의 역할과 전략을 분석하였다. 또한 자율주행 개발 조직의 구조와 필요 인력, 직무별 요구 역량 등 인적 자원 측면도 조사하였다.

본 보고서는 기술적 정확성과 객관성을 유지하기 위해 노력하였으나, 많은 최신 기술들이 아직 연구 개발 단계에 있고 상용화되지 않았음을 밝혀둔다. 또한 기업들의 내부 기술 정보는 공개되지 않는 경우가 많아, 공개 자료와 간접적 정보를 바탕으로 추론한 부분도 포함되어 있다. 기술 전망과 미래 예측은 현재의 기술 발전 추세와 산업 동향을 바탕으로 한 합리적 추정이지만, 실제 발전 경로는 예상과 다를 수 있다.

#pagebreak()

= End-to-End 자율주행의 기술적 기반

== 패러다임의 전환: 모듈식에서 통합식으로

자율주행 기술의 발전 과정에서 가장 근본적인 변화는 시스템 아키텍처의 패러다임 전환이다. 초기 자율주행 연구는 인간 운전자의 의사결정 과정을 모방하여 인지, 예측, 계획, 제어라는 네 가지 단계로 분해하고 각각을 독립적인 모듈로 구현하는 방식을 택했다. 이러한 모듈식 아키텍처는 2007년 DARPA Urban Challenge에서 우승한 Carnegie Mellon University의 Boss 시스템에서 전형적으로 나타나며, 이후 십여 년간 자율주행 시스템 개발의 표준적인 접근 방식으로 자리잡았다.

모듈식 아키텍처의 첫 번째 단계인 인지 모듈은 카메라, LiDAR, 레이더 등의 센서로부터 원시 데이터를 수집하여 주변 환경을 이해한다. 구체적으로는 차량, 보행자, 자전거, 교통 표지판 등의 객체를 검출하고 분류하며, 차선과 도로 경계를 인식하고, 교통 신호등의 상태를 파악한다. 2010년대 초반에는 주로 수작업으로 설계된 특징 추출기와 지지 벡터 머신, 랜덤 포레스트 같은 전통적 기계학습 알고리즘이 사용되었으나, 2015년 이후부터는 깊은 합성곱 신경망이 인지 모듈의 핵심 기술로 자리잡았다. AlexNet, VGG, ResNet과 같은 ImageNet에서 사전 학습된 네트워크들이 전이 학습을 통해 자율주행에 적용되었으며, YOLO, SSD, Faster R-CNN 같은 실시간 객체 검출 알고리즘들이 개발되었다.

두 번째 단계인 예측 모듈은 인지 모듈이 출력한 객체 정보를 받아 각 객체의 미래 행동을 예측한다. 예를 들어, 앞 차량이 차선을 변경할 것인지, 보행자가 도로를 횡단할 것인지, 교차로에서 다른 차량이 어떤 경로로 진행할 것인지 등을 판단한다. 초기에는 일정 속도 모델이나 칼만 필터와 같은 단순한 물리 기반 모델이 사용되었으나, 점차 다항식 궤적 피팅, 가우시안 프로세스, 그리고 최근에는 순환 신경망과 Transformer 기반의 딥러닝 모델들이 활용되고 있다. 특히 사회적 상호작용을 고려한 예측 모델들이 개발되어, 여러 에이전트가 서로 영향을 주고받는 복잡한 교통 상황에서의 예측 정확도가 크게 향상되었다.

세 번째 단계인 계획 모듈은 예측 결과를 바탕으로 자차의 주행 궤적을 생성한다. 이는 다시 경로 계획과 행동 계획으로 나뉠 수 있다. 경로 계획은 출발지에서 목적지까지의 전체 경로를 결정하는 것으로, 주로 HD 맵과 네비게이션 정보를 활용하여 A-star, Dijkstra와 같은 그래프 탐색 알고리즘으로 해결된다. 행동 계획은 현재 교통 상황에서 어떤 기동을 할 것인지 결정하는 것으로, 유한 상태 기계나 의사결정 트리가 전통적으로 사용되었다. 궤적 생성 단계에서는 장애물 회피, 차선 준수, 승차감, 법규 준수 등의 다양한 제약 조건을 만족하는 최적 궤적을 찾아야 하며, 이를 위해 최적화 기법, 샘플링 기반 계획, 격자 기반 탐색 등이 활용된다.

마지막 단계인 제어 모듈은 계획된 궤적을 실제 차량 동작으로 변환한다. 조향, 가속, 제동 등의 제어 명령을 생성하여 차량이 원하는 궤적을 정확히 추종하도록 한다. PID 제어기, 모델 예측 제어, 선형 이차 제어 등의 고전 제어 이론이 주로 사용되며, 차량의 동역학 모델을 기반으로 현재 상태와 목표 상태의 차이를 최소화한다.

모듈식 아키텍처는 몇 가지 명확한 장점을 가진다. 첫째, 각 모듈을 독립적으로 개발하고 테스트할 수 있어 개발 프로세스가 체계적이고 효율적이다. 여러 팀이 동시에 다른 모듈을 개발할 수 있으며, 특정 모듈의 성능 문제를 격리하여 개선할 수 있다. 둘째, 시스템의 동작을 인간이 이해하고 해석할 수 있다. 각 모듈의 중간 출력을 검사하여 시스템이 환경을 어떻게 인식하고 어떤 추론을 거쳐 결정을 내렸는지 추적할 수 있다. 이는 디버깅과 안전성 검증에 매우 유용하다. 셋째, 도메인 지식을 효과적으로 통합할 수 있다. 물리 법칙, 교통 규칙, 인간 운전 습관 등의 선행 지식을 각 모듈에 명시적으로 인코딩할 수 있다.

그러나 모듈식 아키텍처는 근본적인 한계를 가지고 있으며, 이는 시스템 성능의 천장을 만든다. 가장 심각한 문제는 모듈 간 인터페이스에서 발생하는 정보 손실이다. 인지 모듈은 고차원의 센서 데이터를 객체 목록이라는 저차원 표현으로 압축한다. 예를 들어, 1920x1080 해상도의 이미지는 약 200만 픽셀의 정보를 담고 있지만, 이를 처리한 인지 모듈의 출력은 수십 개의 bounding box와 클래스 레이블로 요약된다. 이 과정에서 미세한 질감, 조명 조건, 깊이의 불확실성 등 많은 정보가 소실되며, 예측과 계획 모듈은 이 축약된 정보만으로 의사결정을 해야 한다.

두 번째 문제는 오류의 누적이다. 인지 모듈이 차량을 잘못 검출하거나 분류하면, 예측 모듈은 잘못된 정보를 바탕으로 미래를 예측하고, 계획 모듈은 잘못된 예측을 바탕으로 위험한 궤적을 생성할 수 있다. 각 모듈의 오류가 연쇄적으로 증폭되어 최종 성능에 치명적인 영향을 미칠 수 있다. 특히 드물게 발생하는 복잡한 상황에서 이러한 문제가 두드러진다.

세 번째 문제는 부분 최적화로 인한 전체 성능 저하이다. 각 모듈은 자신만의 목적 함수를 최적화하도록 학습되거나 설계된다. 인지 모듈은 검출 정확도를 최대화하고, 예측 모듈은 예측 오차를 최소화하며, 계획 모듈은 궤적의 부드러움과 안전성을 최적화한다. 그러나 전체 시스템의 목표는 안전하고 효율적이며 편안한 주행을 달성하는 것이며, 이는 개별 모듈의 목표와 다를 수 있다. 예를 들어, 인지 모듈이 모든 객체를 완벽하게 검출하는 것보다 주행에 중요한 객체만 정확히 검출하는 것이 더 유용할 수 있다.

네 번째 문제는 확장성의 한계이다. 새로운 센서를 추가하거나, 새로운 유형의 객체를 인식하거나, 새로운 주행 시나리오를 처리하려면 여러 모듈을 동시에 수정해야 한다. 인터페이스를 변경하면 연결된 모든 모듈에 영향을 미치므로, 시스템의 진화가 어렵다.

이러한 모듈식 아키텍처의 한계를 극복하기 위해 End-to-End 학습 방식이 제안되었다. End-to-End 접근은 센서 입력에서 제어 출력까지의 전체 매핑을 하나의 신경망으로 직접 학습한다. 중간 단계의 명시적인 표현 없이, 데이터로부터 직접 입출력 관계를 학습하는 것이다. 이는 1989년 Pomerleau가 제안한 ALVINN 시스템에서 처음 시도되었으나, 당시의 제한된 컴퓨팅 성능과 작은 데이터셋으로 인해 간단한 도로에서만 작동했다.

End-to-End 학습이 다시 주목받게 된 것은 2010년대 중반, 딥러닝의 부흥과 함께였다. 2016년 NVIDIA의 연구는 단 9층의 합성곱 신경망으로 카메라 이미지에서 조향각을 직접 예측할 수 있음을 보였다. 네트워크는 72시간의 인간 주행 데이터로 학습되었으며, 명시적인 차선 검출이나 경로 계획 없이도 다양한 도로에서 주행할 수 있었다. 더 놀라운 점은 네트워크가 스스로 유용한 중간 표현을 학습했다는 것이다. 네트워크 내부의 활성화를 시각화한 결과, 초기 층은 에지와 텍스처를 검출하고, 중간 층은 차선과 도로 경계를 감지하며, 후기 층은 주행 가능한 영역을 표현하는 것으로 나타났다. 이는 명시적으로 지도하지 않았음에도 네트워크가 주행에 유용한 특징들을 자동으로 학습했음을 의미한다.

End-to-End 아키텍처의 핵심 장점은 전체 시스템을 하나의 목적 함수로 최적화할 수 있다는 것이다. 주행 성능을 직접 측정하여 역전파 알고리즘으로 네트워크의 모든 파라미터를 업데이트한다. 이는 부분 최적화의 문제를 해결하며, 시스템의 모든 구성 요소가 최종 목표를 향해 협력하도록 만든다. 또한 모듈 간 인터페이스가 없으므로 정보 손실이 최소화된다. 센서 데이터의 모든 정보가 네트워크를 통해 흐르며, 각 결정에 필요한 정보를 직접 추출할 수 있다.

두 번째 장점은 데이터 기반 학습을 통한 암묵적 지식의 획득이다. 인간이 명시적으로 프로그래밍하기 어려운 복잡한 패턴과 규칙을 데이터로부터 자동으로 학습한다. 예를 들어, 다른 운전자의 의도를 미묘한 단서로부터 추론하거나, 복잡한 교차로에서의 우선순위를 암묵적으로 학습할 수 있다. 이는 수백만 마일의 주행 데이터에서 추출된 집단 지성을 활용하는 것이다.

세 번째 장점은 확장성이다. 더 많은 데이터와 더 큰 모델, 더 강력한 컴퓨팅 자원을 투입하면 성능이 지속적으로 향상되는 스케일링 법칙을 따른다. 이는 자연어 처리의 GPT 시리즈나 컴퓨터 비전의 Vision Transformer에서 입증되었으며, 자율주행에서도 유사한 현상이 관찰되고 있다. Tesla는 지속적으로 플릿 데이터를 수집하고 모델 크기를 증가시키며 성능을 개선하고 있으며, 이는 모듈식 시스템에서는 달성하기 어려운 지속적 진화이다.

물론 End-to-End 아키텍처에도 도전 과제가 있다. 가장 큰 문제는 해석 가능성의 부족이다. 네트워크가 왜 특정 결정을 내렸는지 이해하기 어렵다. 수백만 개의 파라미터로 구성된 블랙박스 시스템은 디버깅이 어렵고, 안전성을 검증하기 힘들다. 규제 기관과 보험 회사는 설명 가능한 시스템을 선호하므로, 이는 상용화의 장벽이 될 수 있다. 두 번째 문제는 대규모 고품질 데이터가 필요하다는 것이다. 모듈식 시스템은 각 모듈을 개별적으로 레이블된 데이터로 학습시킬 수 있지만, End-to-End 시스템은 입출력 쌍 전체가 필요하다. 드문 상황의 데이터를 충분히 수집하는 것은 어렵고 비용이 많이 든다.

현재의 추세는 순수한 End-to-End 시스템과 모듈식 시스템의 중간 지점을 탐색하는 것이다. 하이브리드 아키텍처는 End-to-End 학습의 장점을 취하면서도 일부 해석 가능성과 안전성 보장을 유지한다. 예를 들어, 인지와 예측은 End-to-End로 학습하되, 계획은 명시적인 안전 제약을 포함하는 최적화 문제로 정식화할 수 있다. 또는 전체 시스템을 End-to-End로 학습하되, 중간 표현을 명시적으로 감독하여 해석 가능성을 높일 수 있다. Tesla의 FSD는 이러한 하이브리드 접근의 대표적인 예이다. 시스템은 End-to-End 신경망을 핵심으로 하지만, 중간에 명시적인 3D 공간 표현과 객체 리스트를 생성하며, 최종 계획 단계에서는 물리적 제약과 안전 규칙을 명시적으로 확인한다.

향후 몇 년간 End-to-End 아키텍처는 더욱 정교해지고 널리 채택될 것으로 예상된다. Foundation Models의 발전과 함께, 대규모 사전 학습과 fine-tuning을 통해 데이터 효율성이 개선될 것이다. 설명 가능 AI 기법의 발전으로 블랙박스의 내부를 들여다보고 신뢰성을 높일 수 있을 것이다. 그리고 시뮬레이션 기술의 발전으로 드문 상황의 데이터를 합성하여 학습에 활용할 수 있을 것이다. 모듈식에서 End-to-End로의 패러다임 전환은 단순한 기술적 변화가 아니라, 자율주행 시스템을 설계하고 개발하는 근본적인 철학의 변화를 의미한다.

== 학습 방법론의 진화

End-to-End 자율주행 시스템을 학습시키는 방법론은 크게 세 가지로 분류할 수 있다. 모방 학습, 강화학습, 그리고 인간 피드백 기반 강화학습이다. 각 방법론은 고유한 장단점을 가지고 있으며, 실제 시스템에서는 이들을 조합하여 사용하는 경우가 많다.

모방 학습은 가장 직관적이고 널리 사용되는 방법이다. 기본 아이디어는 전문가인 인간 운전자의 행동을 관찰하고 이를 모방하는 정책을 학습하는 것이다. 수학적으로는 감독 학습 문제로 정식화되며, 입력은 센서 데이터, 출력은 조향각과 가속 페달 값이다. 학습 데이터는 인간 운전자가 실제로 주행한 로그에서 수집되며, 신경망은 주어진 센서 입력에서 인간이 취한 행동을 예측하도록 학습된다.

모방 학습의 가장 단순한 형태는 행동 복제이다. 데이터셋의 각 샘플에 대해 신경망의 예측과 인간의 실제 행동 사이의 오차를 계산하고, 이를 최소화하도록 네트워크 파라미터를 업데이트한다. 이는 일반적인 감독 학습과 동일하며, 구현이 간단하고 학습이 안정적이다. 대규모 데이터셋이 있다면 상당히 좋은 성능을 달성할 수 있다. Waymo는 수천만 마일의 주행 데이터를 수집했으며, 이를 바탕으로 모방 학습으로 강력한 초기 정책을 획득했다.

그러나 행동 복제는 분포 이동 문제로 인해 한계를 가진다. 학습 데이터는 인간 운전자가 만든 상태 분포를 따른다. 즉, 인간이 주로 경험하는 상황들로 구성되어 있다. 그러나 학습된 정책이 실제로 주행할 때는 작은 오류로 인해 학습 데이터와 다른 상태에 놓일 수 있다. 예를 들어, 정책이 도로의 중앙에서 약간 벗어나면, 이는 인간 운전자가 거의 경험하지 않는 상태이므로 학습 데이터에 충분한 예시가 없다. 정책은 이 상황에서 어떻게 행동해야 할지 모르고 더 큰 오류를 만들 수 있으며, 이는 누적되어 궁극적으로 실패로 이어진다.

이 문제를 해결하기 위해 데이터셋 집계 알고리즘이 제안되었다. 핵심 아이디어는 학습된 정책이 만드는 상태 분포에서 추가 데이터를 수집하는 것이다. 구체적인 절차는 다음과 같다. 먼저 초기 정책을 인간 데이터로 학습한다. 이 정책을 실제 환경이나 시뮬레이션에서 실행하여 새로운 상태들을 경험하게 한다. 이 새로운 상태들에서 인간 전문가가 어떻게 행동했을지 레이블을 달거나, 실제로 인간이 제어권을 가져와 행동을 기록한다. 이 새로운 데이터를 기존 데이터셋에 추가하고, 확장된 데이터셋으로 정책을 재학습한다. 이 과정을 여러 번 반복하면, 정책은 자신이 실제로 경험할 상태 분포에서 학습하게 되므로 분포 이동 문제가 완화된다.

데이터셋 집계는 이론적으로 우수하지만 실용적인 어려움이 있다. 매 반복마다 인간 전문가가 새로운 상태에 레이블을 달아야 하므로 비용이 많이 든다. 또한 안전성 문제가 있다. 학습 중인 정책은 아직 완벽하지 않으므로, 실제 차량에서 실행하면 위험할 수 있다. 따라서 시뮬레이션 환경에서 데이터셋 집계를 수행하는 것이 일반적이지만, 시뮬레이션과 실제의 차이로 인해 전이 문제가 발생할 수 있다.

모방 학습의 또 다른 발전은 조건부 모방 학습이다. 기본 모방 학습은 동일한 상황에서도 인간이 다른 행동을 취할 수 있다는 점을 고려하지 못한다. 예를 들어, 교차로에서 직진, 좌회전, 우회전 모두 가능하며, 어떤 행동을 택할지는 목적지에 따라 달라진다. 조건부 모방 학습은 고수준 명령을 추가 입력으로 받아 이를 해결한다. 네트워크는 센서 데이터와 함께 네비게이션 명령을 받아, 해당 명령에 맞는 행동을 출력한다. 이는 정책의 다중 모드성을 명시적으로 모델링하며, 동일한 네트워크로 다양한 주행 기동을 수행할 수 있게 한다.

강화학습은 모방 학습의 대안적 접근이다. 인간의 시연을 직접 모방하는 대신, 보상 신호를 통해 좋은 행동과 나쁜 행동을 구별하고 시행착오를 통해 학습한다. 에이전트는 환경과 상호작용하며, 각 행동에 대한 보상을 받는다. 목표는 장기적인 누적 보상을 최대화하는 정책을 찾는 것이다. 강화학습은 이론적으로 최적 정책을 학습할 수 있으며, 인간보다 더 나은 성능을 달성할 가능성도 있다.

자율주행에서 강화학습을 적용하는 핵심 과제는 보상 함수를 설계하는 것이다. 보상 함수는 시스템이 무엇을 최적화해야 하는지 정의한다. 자율주행의 목표는 안전하고 효율적이며 편안한 주행을 달성하는 것이지만, 이를 수학적 함수로 정확히 표현하기는 어렵다. 보상 함수는 여러 요소의 가중합으로 구성되는 경우가 많다. 안전성을 위해서는 충돌 시 큰 음의 보상을 주고, 다른 차량이나 장애물과의 거리가 가까울수록 음의 보상을 준다. 효율성을 위해서는 목표 속도를 유지하거나 목적지에 빠르게 도달할수록 양의 보상을 준다. 승차감을 위해서는 급격한 조향이나 가감속에 음의 보상을 준다. 또한 교통 규칙을 준수하도록 차선 이탈, 신호 위반 등에 벌점을 부여한다.

보상 함수 설계의 어려움은 이들 요소 간의 상충 관계에 있다. 예를 들어, 안전을 극대화하려면 매우 느리게 주행하거나 아예 멈춰야 하지만, 이는 효율성과 배치된다. 승차감을 위해 부드럽게 주행하면 긴급 상황에서 충돌을 피하지 못할 수 있다. 각 요소의 가중치를 어떻게 설정할지는 주관적이며, 잘못된 가중치는 바람직하지 않은 행동을 유도할 수 있다. 보상 해킹 문제도 발생할 수 있다. 에이전트가 보상 함수의 허점을 찾아 의도와 다른 방식으로 보상을 최대화하는 것이다.

강화학습의 또 다른 과제는 샘플 효율성이다. 에이전트는 많은 시행착오를 통해 학습하며, 좋은 정책을 얻기 위해 수백만 번의 상호작용이 필요할 수 있다. 시뮬레이션에서는 빠르게 많은 샘플을 생성할 수 있지만, 실제 차량에서는 불가능하다. 또한 안전성 문제로 인해 실제 환경에서 무작위 탐험을 하기 어렵다. 학습 초기의 정책은 성능이 좋지 않으므로, 실제로 주행하면 사고를 일으킬 위험이 크다.

이러한 문제들을 완화하기 위해 여러 전략이 사용된다. 첫째, 시뮬레이션에서 먼저 학습하고 실제 환경으로 전이한다. CARLA, LGSVL과 같은 고품질 시뮬레이터에서 수백만 번의 주행을 시뮬레이션하여 초기 정책을 학습한 후, 실제 환경에서 fine-tuning한다. 시뮬레이션과 실제의 차이를 줄이기 위해 도메인 랜덤화 기법을 사용한다. 시뮬레이션의 시각적 외관, 물리 파라미터, 센서 노이즈 등을 무작위로 변화시켜 다양성을 늘리면, 학습된 정책이 실제 환경에 더 잘 전이된다.

둘째, 모델 기반 강화학습을 사용하여 샘플 효율성을 높인다. 환경의 동역학 모델을 학습하고, 이 모델 내에서 계획하거나 추가 학습을 수행한다. 실제 환경과의 상호작용으로 모델을 학습하고, 학습된 모델로 시뮬레이션된 경험을 생성하여 정책을 개선한다. 이는 제한된 실제 데이터를 효율적으로 활용할 수 있게 한다.

셋째, 오프라인 강화학습을 활용한다. 이는 온라인 상호작용 없이 과거에 수집된 데이터셋만으로 정책을 학습하는 방법이다. 인간이나 다른 정책이 수집한 주행 로그를 사용하여 강화학습 알고리즘을 적용한다. 이는 안전하며, 기존 데이터를 재활용할 수 있다는 장점이 있다. 그러나 데이터셋이 커버하지 않는 상태-행동 쌍에 대한 학습이 어렵고, 분포 밖 일반화 문제가 발생할 수 있다.

최근에는 모방 학습과 강화학습을 결합하는 방법이 주목받고 있다. 모방 학습으로 인간 수준의 초기 정책을 빠르게 획득한 후, 강화학습으로 이를 더욱 개선하는 것이다. 이는 두 방법의 장점을 취하며, 인간을 뛰어넘는 성능을 달성할 가능성을 제공한다.

인간 피드백 기반 강화학습은 보상 함수 설계의 어려움을 우회하는 방법이다. 명시적인 보상 함수를 엔지니어가 작성하는 대신, 인간의 선호도로부터 보상 모델을 학습한다. 절차는 두 단계로 구성된다. 첫 번째 단계에서는 여러 주행 궤적 쌍을 인간에게 보여주고, 어느 쪽이 더 나은지 평가하게 한다. 예를 들어, 두 개의 교차로 통과 영상을 보여주고 어느 쪽이 더 안전하고 부드러운지 선택하게 한다. 이러한 비교 데이터를 수집하여 보상 모델을 학습한다. 보상 모델은 주행 궤적을 입력으로 받아 점수를 출력하는 신경망이며, 인간이 선호하는 궤적에 더 높은 점수를 부여하도록 학습된다.

두 번째 단계에서는 학습된 보상 모델을 사용하여 강화학습을 수행한다. 에이전트는 환경과 상호작용하며, 각 행동에 대해 보상 모델로부터 보상을 받는다. 이 보상을 최대화하도록 정책을 학습한다. 학습이 진행됨에 따라 새로운 정책 행동에 대한 인간 피드백을 추가로 수집하여 보상 모델을 개선할 수 있다.

인간 피드백 기반 강화학습의 장점은 복잡하고 주관적인 목표를 다룰 수 있다는 것이다. 승차감, 자연스러움, 공손함 같은 개념은 수식으로 정의하기 어렵지만, 인간은 직관적으로 평가할 수 있다. 또한 인간의 암묵적 지식을 시스템에 전달할 수 있다. 교통 예절, 문화적 차이 등 명시적으로 표현되지 않은 규범들을 학습할 수 있다.

그러나 인간 피드백 수집에는 비용이 든다. 수천 또는 수만 쌍의 비교가 필요하며, 각각에 대해 인간이 신중하게 평가해야 한다. 또한 인간 평가자 간의 불일치, 평가자의 피로와 실수, 그리고 평가 기준의 모호함 등이 노이즈를 만든다. 능동 학습 기법을 사용하여 효율성을 높일 수 있다. 시스템은 가장 정보가 많은 비교 쌍, 즉 모델의 불확실성이 큰 경우를 우선적으로 인간에게 질의한다.

자율주행 분야에서 인간 피드백 기반 강화학습의 적용은 아직 초기 단계이지만, 유망한 결과들이 보고되고 있다. Wayve는 주행 스타일을 개선하기 위해 이 방법을 사용했다. 기술적으로 안전하지만 승객에게 불편함을 주는 행동들을 인간 피드백을 통해 식별하고 개선했다. 예를 들어, 너무 급격한 브레이크, 불필요하게 넓은 회전, 소심한 합류 등을 인간 평가자가 지적하면, 시스템은 이를 학습하여 더 자연스러운 주행 스타일을 획득했다.

게임 이론 기반 강화학습은 다중 에이전트 환경에서 상호작용을 고려하는 방법이다. 자율주행 차량은 혼자 주행하는 것이 아니라 다른 차량, 보행자, 자전거 등과 도로를 공유한다. 이들 각각은 자신의 목표를 가지고 행동하며, 서로 영향을 주고받는다. 단순히 다른 에이전트를 환경의 일부로 취급하면, 그들의 적응적 행동을 놓칠 수 있다.

게임 이론은 전략적 상호작용을 분석하는 수학적 틀을 제공한다. 내쉬 균형 개념은 각 에이전트가 다른 에이전트의 전략이 주어졌을 때 자신의 전략을 최적화한 상태를 나타낸다. 자율주행에서는 교차로 통과, 차선 변경, 합류 등의 상황이 게임으로 모델링될 수 있다. 각 차량은 안전하게 목표에 도달하려 하며, 다른 차량의 행동을 예측하고 이에 최적으로 대응한다.

레벨-k 추론은 인간의 제한된 합리성을 모델링하는 접근이다. 레벨-0 에이전트는 단순한 휴리스틱을 따르며, 다른 에이전트를 고려하지 않는다. 레벨-1 에이전트는 다른 에이전트가 레벨-0이라고 가정하고 최적 대응을 한다. 레벨-k 에이전트는 다른 에이전트가 레벨-(k-1)이라고 가정한다. 실제 인간 운전자는 보통 레벨-1이나 레벨-2 정도의 추론을 하는 것으로 알려져 있다. 자율주행 시스템이 이를 모델링하면, 인간 운전자와 더 자연스럽게 상호작용할 수 있다.

역 강화학습은 관찰된 행동으로부터 보상 함수를 추론한다. 다른 차량의 주행 로그를 보고, 그 차량이 암묵적으로 최적화하고 있는 목표가 무엇인지 역으로 계산한다. 예를 들어, 한 차량이 일관되게 빠른 속도를 유지하면서도 안전 거리를 확보한다면, 그 차량은 속도와 안전의 균형을 중시하는 보상 함수를 가진 것으로 추정할 수 있다. 이렇게 추론된 보상 함수를 사용하여 그 차량의 미래 행동을 예측할 수 있다.

다중 에이전트 강화학습 알고리즘들도 개발되고 있다. QMIX는 여러 에이전트의 Q-함수를 결합하여 중앙집중식으로 학습하되, 실행 시에는 각 에이전트가 독립적으로 행동한다. MADDPG는 중앙집중식 비평가와 분산 행위자를 사용하여 협력과 경쟁이 혼재된 환경에서 학습한다. 이러한 알고리즘들은 시뮬레이션 환경에서 흥미로운 행동 패턴을 보여주었으며, 실제 자율주행에 적용하는 연구가 진행 중이다.

학습 방법론의 선택은 데이터 가용성, 안전 요구사항, 성능 목표에 따라 달라진다. 대규모 인간 주행 데이터가 있다면 모방 학습이 빠르고 안전한 시작점을 제공한다. 더 나은 성능을 추구한다면 강화학습으로 정책을 fine-tuning할 수 있다. 주관적 품질을 중시한다면 인간 피드백을 활용한다. 복잡한 상호작용이 중요하다면 게임 이론적 접근을 고려한다. 실제로는 이들을 조합한 하이브리드 방법이 가장 효과적인 경우가 많으며, 자율주행 기업들은 자신의 상황에 맞는 최적의 조합을 찾기 위해 지속적으로 실험하고 있다.

#pagebreak()

= Foundation Models와 생성 AI의 융합

== Vision Transformer와 대규모 사전학습

컴퓨터 비전 분야는 오랫동안 합성곱 신경망의 지배를 받아왔다. 합성곱 연산은 이미지의 지역적 패턴을 효과적으로 포착하며, 평행 이동 불변성이라는 유용한 귀납적 편향을 제공한다. AlexNet이 2012년 ImageNet 대회에서 압도적 성능을 보인 이후, ResNet, EfficientNet 등 다양한 합성곱 기반 아키텍처들이 발전해왔다. 그러나 2020년 Google Research에서 발표한 Vision Transformer는 이러한 패러다임에 근본적인 도전을 제기했다.

Vision Transformer는 자연어 처리에서 성공을 거둔 Transformer 아키텍처를 이미지 인식에 적용한 것이다. Transformer의 핵심은 self-attention 메커니즘으로, 입력 시퀀스의 모든 위치 간의 관계를 직접 모델링한다. 이미지에 Transformer를 적용하기 위해, 이미지를 작은 패치들로 나누고 이를 시퀀스로 취급한다. 예를 들어, 224×224 이미지를 16×16 패치로 나누면 196개의 패치가 생성되며, 각 패치는 하나의 토큰으로 간주된다. 각 패치는 선형 변환을 통해 고차원 벡터로 임베딩되고, 위치 정보를 인코딩하는 벡터가 추가된다. 이렇게 만들어진 토큰 시퀀스가 Transformer encoder에 입력된다.

Transformer encoder는 여러 층의 self-attention과 feed-forward 네트워크로 구성된다. Self-attention은 각 토큰이 다른 모든 토큰과 상호작용하도록 하여, 이미지 전체의 문맥을 고려할 수 있게 한다. 이는 합성곱의 제한된 수용 영역과 대조적이다. 합성곱 신경망은 여러 층을 쌓아야 전역적 정보를 통합할 수 있지만, Vision Transformer는 단일 attention 층에서도 이미지의 먼 부분들 간의 관계를 직접 포착한다.

Vision Transformer의 초기 결과는 놀라웠다. 충분히 큰 데이터셋에서 학습하면 최고 성능의 합성곱 신경망과 동등하거나 더 나은 성능을 보였다. 특히 JFT-300M이라는 3억 개 이미지의 대규모 데이터셋에서 사전 학습한 Vision Transformer는 ImageNet에서 당시 최고 성능을 기록했다. 더 흥미로운 점은 모델 크기를 키울수록 성능이 지속적으로 향상되는 스케일링 법칙을 보인다는 것이다. 합성곱 신경망은 일정 크기 이상에서 성능 향상이 정체되는 경향이 있지만, Transformer는 더 많은 파라미터와 데이터를 투입할수록 성능이 계속 개선된다.

자율주행 분야에서 Vision Transformer의 도입은 인지 시스템의 성능을 크게 향상시켰다. 먼저 사전 학습의 이점이 크다. ImageNet이나 더 큰 데이터셋에서 사전 학습된 Vision Transformer는 일반적인 시각 표현을 학습하며, 이를 자율주행 작업에 전이할 수 있다. 적은 양의 자율주행 데이터로도 fine-tuning을 통해 좋은 성능을 달성할 수 있다. 이는 특히 드물게 발생하는 상황에서 유용하다. 예를 들어, 눈 오는 날의 주행 데이터는 제한적이지만, 사전 학습된 모델은 일반적인 눈 패턴을 이미 알고 있으므로 빠르게 적응할 수 있다.

둘째, Vision Transformer의 장거리 의존성 모델링 능력은 복잡한 장면 이해에 도움이 된다. 도심 주행에서는 여러 객체들이 서로 상호작용하며, 먼 곳의 교통 신호나 표지판이 현재 행동에 영향을 줄 수 있다. Vision Transformer는 이미지의 서로 다른 부분들 간의 관계를 효과적으로 포착하여, 이러한 복잡한 문맥을 이해할 수 있다.

셋째, Vision Transformer는 다양한 해상도와 종횡비의 이미지를 다루기 용이하다. 자율주행에서는 여러 카메라가 다른 해상도와 시야각을 가질 수 있으며, Vision Transformer는 패치 단위로 처리하므로 이러한 변화에 유연하게 대응한다.

Tesla는 FSD 시스템에서 Vision Transformer를 적극 활용하고 있다. 다중 카메라 이미지를 처리하는 백본 네트워크로 Transformer를 사용하며, 이를 통해 8개 카메라의 정보를 효과적으로 통합한다. 각 카메라 이미지에서 추출된 특징들이 Transformer의 attention 메커니즘을 통해 상호작용하며, 전체 차량 주변의 일관된 표현을 생성한다.

Vision Transformer의 성공은 대규모 사전 학습의 중요성을 재확인시켰다. Foundation Models라는 개념은 범용적인 대규모 모델을 먼저 학습하고, 이를 다양한 downstream 작업에 적용한다는 아이디어이다. GPT-3, BERT와 같은 언어 모델들이 이 접근의 효과를 입증했으며, 이제 비전과 자율주행에서도 동일한 패러다임이 자리잡고 있다.

CLIP은 Vision과 Language를 결합한 Foundation Model의 대표적 예이다. OpenAI가 개발한 CLIP은 4억 개의 이미지-텍스트 쌍으로 학습되었으며, 이미지와 텍스트를 공동 임베딩 공간에 매핑한다. 학습은 contrastive learning 방식으로 이루어진다. 매칭되는 이미지-텍스트 쌍은 임베딩 공간에서 가깝게, 그렇지 않은 쌍은 멀게 배치되도록 학습한다. 이를 통해 CLIP은 시각적 개념과 언어적 설명 간의 연결을 학습한다.

CLIP의 흥미로운 특성은 zero-shot 일반화 능력이다. 학습 중에 보지 못한 새로운 객체 클래스도 텍스트 설명만 제공하면 인식할 수 있다. 예를 들어, CLIP이 "공사 중인 도로"라는 개념을 명시적으로 학습하지 않았더라도, 이 텍스트 설명과 유사한 이미지를 찾을 수 있다. 자율주행에서는 다양하고 예측 불가능한 상황을 마주치므로, 이러한 zero-shot 능력이 매우 유용하다.

자율주행에서 CLIP을 활용하는 방법은 여러 가지이다. 첫째, 이상 상황 감지에 사용할 수 있다. 정상적인 도로 장면과 다른 이상한 상황을 텍스트 쿼리로 찾을 수 있다. "도로를 막고 있는 쓰레기", "뒤집힌 차량" 같은 드문 상황을 학습 데이터 없이도 인식할 수 있다. 둘째, 사람의 자연어 명령을 이해하는 데 활용할 수 있다. "빨간 건물 앞에 세워주세요"와 같은 지시를 시각 정보와 연결하여 해석한다. 셋째, 플릿 데이터의 자동 레이블링에 사용할 수 있다. 텍스트 쿼리로 특정 유형의 장면을 검색하여, 레이블링 비용을 크게 줄일 수 있다.

최근에는 자율주행 전용 Foundation Models를 개발하려는 시도들이 나타나고 있다. 일반적인 이미지가 아닌 대규모 주행 영상으로 사전 학습하는 것이다. Waymo는 자사의 방대한 주행 데이터를 활용하여 자율주행 특화 사전 학습을 수행하고 있다. 수천만 마일의 주행 로그에서 추출한 수십억 프레임의 이미지와 LiDAR 포인트 클라우드로 대규모 모델을 학습한다. 이러한 모델은 자율주행에 특화된 표현을 학습하며, 일반 이미지에서 사전 학습한 모델보다 더 나은 성능을 보인다.

Self-supervised learning은 레이블 없이 데이터로부터 표현을 학습하는 방법이다. 주행 데이터는 풍부하지만, 모든 프레임에 정확한 레이블을 다는 것은 비용이 많이 든다. Self-supervised learning은 데이터 자체로부터 감독 신호를 만들어낸다. Masked image modeling은 이미지의 일부를 가리고 나머지로부터 가려진 부분을 복원하도록 학습한다. 이는 BERT의 masked language modeling과 유사한 아이디어이다. 모델은 문맥을 이용하여 빠진 정보를 추론하는 능력을 획득하며, 이는 downstream 작업에 유용한 표현을 제공한다.

Contrastive learning은 유사한 샘플들을 가깝게, 다른 샘플들을 멀게 배치하도록 학습한다. 자율주행에서는 시간적으로 가까운 프레임들이 유사한 장면을 담고 있으므로, 이들을 positive pair로 사용할 수 있다. 또한 데이터 증강 기법을 사용하여 동일한 장면의 다른 view를 생성하고, 이들을 유사하게 만들도록 학습할 수 있다.

Temporal consistency는 연속된 프레임 간의 일관성을 활용한다. 차량이 부드럽게 움직이므로, 연속 프레임의 특징 표현도 부드럽게 변해야 한다. 이 제약을 학습 목적함수에 추가하여 시간적으로 안정적인 표현을 학습한다. 또한 광학 흐름이나 차량의 자세 변화 정보를 사용하여, 연속 프레임 간의 대응 관계를 학습할 수 있다.

Multi-modal pre-training은 여러 센서의 데이터를 함께 활용하여 학습한다. 카메라, LiDAR, 레이더, GPS, IMU 등의 센서는 서로 보완적인 정보를 제공한다. 카메라는 시각적 세부사항과 색상 정보를 제공하고, LiDAR는 정확한 3차원 거리 정보를 제공하며, 레이더는 속도 정보와 악천후 강건성을 제공한다. 이들을 통합하여 학습하면 각 센서의 장점을 결합한 강력한 표현을 얻을 수 있다.

BEVFormer는 이러한 다중 모드 사전 학습의 예이다. 여러 카메라 이미지를 Bird's Eye View 공간으로 변환하며, 이 과정에서 공간적 관계와 3차원 구조를 학습한다. 사전 학습 단계에서는 대규모 주행 데이터로 BEV 표현을 학습하고, fine-tuning 단계에서는 특정 작업에 맞게 조정한다. 이는 3D 객체 검출, occupancy 예측, 경로 계획 등 다양한 downstream 작업에서 우수한 성능을 보인다.

Foundation Models의 또 다른 장점은 few-shot learning 능력이다. 새로운 환경이나 새로운 유형의 객체에 빠르게 적응할 수 있다. 예를 들어, 특정 국가에서 사용되는 독특한 교통 표지판이나, 특정 지역의 건축 양식 등을 몇 가지 예시만으로 학습할 수 있다. 이는 자율주행 시스템을 새로운 지역으로 확장할 때 매우 유용하다.

Prompt learning은 Foundation Models를 활용하는 새로운 패러다임이다. 모델의 파라미터를 수정하지 않고, 적절한 입력 프롬프트를 설계하여 원하는 작업을 수행하게 한다. 예를 들어, "이 장면에서 주행 가능한 영역은 어디인가?"라는 텍스트 프롬프트와 함께 이미지를 입력하면, 모델이 해당 영역을 출력한다. 이는 작업별로 별도의 모델을 학습할 필요 없이, 하나의 Foundation Model로 다양한 작업을 수행할 수 있게 한다.

Meta-learning 또는 learning to learn은 여러 작업에서 학습한 경험을 바탕으로, 새로운 작업에 빠르게 적응하는 능력을 학습한다. Foundation Model을 meta-learning 프레임워크로 학습하면, 적은 데이터로도 새로운 시나리오에 빠르게 fine-tuning될 수 있다. 이는 장기적으로 자율주행 시스템의 지속적인 진화와 개선을 가능하게 한다.

Vision Transformer와 Foundation Models의 발전은 자율주행을 데이터 중심, 스케일 중심의 패러다임으로 이끌고 있다. 더 많은 데이터, 더 큰 모델, 더 강력한 컴퓨팅이 성능 향상의 핵심 동력이 되고 있다. 이는 자원이 풍부한 대기업에게 유리하며, 실제로 Tesla, Waymo, Baidu와 같은 기업들이 막대한 투자를 통해 데이터와 컴퓨팅 인프라를 구축하고 있다. 동시에, 오픈소스 Foundation Models의 발전은 작은 기업과 연구자들에게도 최신 기술에 접근할 기회를 제공하고 있다. 향후 몇 년간 이러한 Foundation Models는 더욱 크고 강력해질 것이며, 자율주행의 핵심 기술로 자리잡을 것으로 전망된다.

== World Models: 미래를 예측하는 생성 모델

World Models는 자율주행 시스템이 환경의 미래 상태를 예측하고 시뮬레이션할 수 있게 하는 생성 모델이다. 전통적인 자율주행 시스템은 현재 관측된 정보만을 바탕으로 결정을 내리지만, World Model을 갖춘 시스템은 가능한 행동들의 미래 결과를 상상하고 비교할 수 있다. 이는 인간 운전자가 "만약 지금 추월하면 어떻게 될까?"와 같은 가상 시나리오를 머릿속에서 시뮬레이션하는 것과 유사하다.

World Model의 핵심 아이디어는 환경의 동역학을 학습하는 것이다. 주어진 현재 상태와 행동에 대해, 다음 상태가 어떻게 될지 예측하는 모델을 학습한다. 수학적으로는 조건부 확률 분포로 표현된다. 이 모델이 정확하다면, 실제 환경과 상호작용하지 않고도 머릿속에서 또는 시뮬레이션에서 미래를 탐색할 수 있다. 이는 여러 측면에서 유용하다.

첫째, 더 안전한 의사결정이 가능하다. 위험한 행동을 실제로 취하기 전에 시뮬레이션에서 그 결과를 예측하여, 사고를 예방할 수 있다. 예를 들어, 급차선 변경이 인접 차량과의 충돌로 이어질지 미리 예측할 수 있다. 둘째, 모델 기반 계획이 가능하다. 여러 행동 시퀀스를 시뮬레이션하여 최적의 계획을 찾을 수 있다. 이는 체스에서 여러 수를 미리 읽어보는 것과 유사하다. 셋째, 샘플 효율적인 강화학습이 가능하다. 실제 환경과의 상호작용 없이 World Model 내에서 정책을 학습하고 개선할 수 있다.

Wayve가 개발한 GAIA-1은 자율주행을 위한 대규모 World Model의 선구적인 예이다. GAIA-1은 생성 AI의 최신 기술을 활용하여, 주행 영상의 미래 프레임을 예측한다. 모델은 과거 몇 프레임의 이미지와 차량의 행동 정보를 입력으로 받아, 다음 몇 프레임의 이미지를 생성한다. 학습 데이터는 수천 시간의 실제 주행 영상이며, 모델은 이로부터 도로의 물리적 법칙, 다른 차량의 행동 패턴, 날씨와 조명의 영향 등을 암묵적으로 학습한다.

GAIA-1의 흥미로운 특징은 다양한 조건을 제어할 수 있다는 것이다. 단순히 현재 행동만이 아니라, 고수준의 텍스트 설명도 조건으로 받을 수 있다. 예를 들어, "비 오는 날 좌회전" 또는 "야간 고속도로 주행"과 같은 텍스트를 입력하면, 해당 조건에 맞는 주행 영상을 생성한다. 이는 특정 시나리오의 데이터가 부족할 때 합성 데이터를 생성하는 데 유용하다. 또한 행동의 결과를 시각화하여 시스템의 의사결정을 이해하고 검증하는 데도 도움이 된다.

NVIDIA의 Neural World Model은 또 다른 접근 방식을 제시한다. 고화질 비디오 픽셀 수준에서 예측하는 대신, 저차원의 잠재 공간에서 동역학을 학습한다. 먼저 Encoder 네트워크가 고차원 센서 데이터를 저차원 잠재 벡터로 압축한다. 이 잠재 공간에서 동역학 모델이 미래 잠재 상태를 예측한다. 필요시 Decoder 네트워크가 잠재 벡터를 다시 이미지로 복원한다. 이러한 접근은 계산 효율성을 크게 향상시키며, 잠재 공간이 더 구조화되고 해석 가능한 표현을 제공할 수 있다.

물리 법칙의 통합은 World Model의 신뢰성을 높이는 중요한 요소이다. 순수 데이터 기반 모델은 물리적으로 불가능한 미래를 예측할 수 있다. 예를 들어, 차량이 갑자기 사라지거나, 중력을 무시하고 날아오를 수 있다. 물리 기반 모델링을 통합하면 이러한 비현실적인 예측을 방지할 수 있다. 차량의 동역학 모델, 충돌 물리, 마찰 계수 등을 명시적으로 인코딩하여, 물리 법칙을 만족하는 예측만 생성하도록 제약할 수 있다.

World Model의 학습은 대규모 주행 데이터로부터 이루어진다. 수백만 시간의 주행 로그에서 연속된 프레임과 행동 정보를 추출하여 학습 샘플을 만든다. 학습 목표는 모델이 예측한 미래 프레임과 실제 관측된 프레임 사이의 차이를 최소화하는 것이다. 비디오 예측은 어려운 문제인데, 미래는 본질적으로 불확실하고 다양한 가능성이 존재하기 때문이다. 예를 들어, 앞 차량이 직진할 수도, 좌회전할 수도, 우회전할 수도 있다. 단일 예측만 출력한다면 이러한 불확실성을 표현할 수 없다.

확률적 World Model은 이 문제를 해결한다. 미래의 여러 가능한 시나리오를 확률 분포로 표현한다. Variational Autoencoder나 Diffusion Models와 같은 생성 모델 기법을 사용하여, 조건부 분포로부터 샘플링할 수 있다. 이를 통해 하나의 현재 상태에서 출발하여 여러 가능한 미래 궤적을 생성하고, 각각의 가능성을 평가할 수 있다. 계획 단계에서는 가장 안전하거나 효율적인 궤적을 선택한다.

World Model의 응용은 다양하다. 안전성 검증에서는 위험한 시나리오를 시뮬레이션하여 시스템의 대응을 테스트한다. 실제로 위험한 상황을 만들 필요 없이, World Model 내에서 타이어 펑크, 급제동, 끼어들기 등을 시뮬레이션하고 시스템이 적절히 반응하는지 확인한다. 데이터 증강에서는 드문 상황의 합성 데이터를 생성한다. 눈 오는 날, 안개 낀 날, 복잡한 교차로 등 실제 데이터가 부족한 상황을 World Model로 생성하여 학습 데이터를 풍부하게 만든다.

모델 기반 계획에서는 여러 행동 시퀀스의 미래를 시뮬레이션하여 최적의 행동을 선택한다. 몬테카를로 트리 탐색이나 모델 예측 제어와 결합하여, 장기적 결과를 고려한 지능적인 결정을 내린다. 설명 가능성 향상에서는 시스템이 왜 특정 행동을 선택했는지 시각화한다. "이 행동을 취하면 이런 미래가 펼쳐질 것"이라는 예측을 보여줌으로써, 시스템의 추론 과정을 투명하게 만든다.

World Model 연구는 아직 초기 단계이지만, 빠르게 발전하고 있다. 생성 AI의 발전, 특히 Diffusion Models와 대규모 Transformer의 성공은 World Model의 품질을 크게 향상시켰다. 컴퓨팅 성능의 증가로 실시간 또는 준실시간으로 미래를 시뮬레이션하는 것이 가능해지고 있다. 향후 몇 년 내에 World Model은 자율주행 시스템의 핵심 구성 요소로 자리잡을 것으로 전망된다. 단순히 반응하는 시스템에서 미래를 예측하고 계획하는 진정으로 지능적인 시스템으로의 진화를 가능하게 할 것이다.

== Diffusion Models와 생성 AI의 응용

Diffusion Models는 최근 몇 년간 생성 AI 분야에서 가장 주목받는 기술로 부상했다. 이미지 생성 분야에서 DALL-E, Midjourney, Stable Diffusion과 같은 시스템들이 놀라운 품질의 이미지를 생성하면서 대중적 관심을 받았다. 이러한 기술이 자율주행 분야에도 적용되기 시작하면서, 데이터 증강, 시나리오 생성, 계획 등 다양한 측면에서 혁신을 가져오고 있다.

Diffusion Models의 기본 원리는 점진적인 노이즈 추가와 제거이다. Forward process에서는 깨끗한 데이터에 단계적으로 가우시안 노이즈를 추가하여, 최종적으로 순수한 노이즈로 만든다. Reverse process에서는 이를 역전시켜, 노이즈로부터 점진적으로 노이즈를 제거하여 깨끗한 데이터를 복원한다. 학습은 reverse process를 수행하는 신경망을 훈련시키는 것이며, 이 네트워크는 각 단계에서 추가된 노이즈를 예측하도록 학습된다.

자율주행에서 Diffusion Models의 첫 번째 응용은 궤적 생성이다. 안전한 주행 궤적을 생성하는 것은 복잡한 제약 최적화 문제이다. 충돌 회피, 차선 준수, 속도 제한, 승차감 등 다양한 제약을 동시에 만족해야 한다. 전통적 방법은 샘플링 기반 계획이나 최적화 알고리즘을 사용하지만, 복잡한 환경에서는 계산 비용이 크고 실시간 처리가 어렵다. Diffusion Models는 조건부 생성을 통해 이 문제를 우아하게 해결한다.

궤적 생성을 위한 Diffusion Model은 현재 상태, 목표 지점, 주변 장애물 정보를 조건으로 받아, 이를 만족하는 궤적을 생성한다. 학습 데이터는 인간 운전자나 기존 계획 알고리즘이 생성한 고품질 궤적들이다. 모델은 이러한 궤적의 분포를 학습하며, 추론 시에는 랜덤 노이즈에서 시작하여 점진적으로 정제된 궤적을 생성한다. Diffusion process의 각 단계에서 조건 정보를 주입하여, 생성되는 궤적이 제약을 만족하도록 유도한다.

이 접근의 장점은 다양성과 품질의 균형이다. 하나의 상황에서 여러 가능한 궤적을 생성할 수 있으며, 각각은 제약을 만족하면서도 다른 특성을 가진다. 예를 들어, 장애물을 왼쪽으로 피할지 오른쪽으로 피할지, 보수적으로 느리게 진행할지 적극적으로 빠르게 진행할지 등 다양한 옵션을 제공한다. 최종 선택은 안전성, 효율성, 승차감 등을 종합적으로 평가하여 이루어진다. 또한 Diffusion Models는 부드러운 궤적을 자연스럽게 생성한다. 점진적 정제 과정이 고주파 노이즈를 제거하여, 급격한 조향이나 가감속 없는 부드러운 궤적을 만든다.

두 번째 응용은 시나리오 생성이다. 자율주행 시스템을 철저히 테스트하려면 다양한 교통 상황을 경험해야 한다. 그러나 실제 주행으로는 드문 상황을 충분히 수집하기 어렵다. Diffusion Models는 현실적이면서도 다양한 테스트 시나리오를 합성적으로 생성할 수 있다. 예를 들어, 복잡한 교차로에서 여러 차량이 동시에 진입하는 상황, 보행자가 갑자기 뛰어드는 상황, 공사로 차선이 축소되는 상황 등을 생성한다.

시나리오 생성에서 중요한 것은 현실성과 다양성이다. 생성된 시나리오가 물리 법칙을 위반하거나 비현실적이면 테스트의 의미가 없다. Diffusion Models는 대규모 실제 데이터로부터 학습하므로, 현실적인 교통 패턴과 에이전트 행동을 재현한다. 동시에, 생성 과정의 랜덤성 덕분에 학습 데이터에 없던 새로운 조합과 변형을 만들어낸다. 이는 edge case와 corner case를 발견하는 데 유용하다.

적대적 시나리오 생성은 시스템의 약점을 찾는 데 특화된 접근이다. Diffusion Model을 강화학습과 결합하여, 자율주행 시스템을 실패하게 만드는 시나리오를 적극적으로 탐색한다. 생성 모델은 시나리오를 생성하고, 자율주행 시스템이 이를 처리하며, 결과에 따라 보상 신호가 생성 모델에 피드백된다. 생성 모델은 시스템을 더 어렵게 만드는 시나리오를 생성하도록 학습된다. 이를 통해 발견된 취약점은 시스템 개선에 활용된다.

세 번째 응용은 센서 데이터 합성이다. 고품질 센서 데이터, 특히 정확한 레이블이 달린 데이터는 수집하기 어렵고 비용이 많이 든다. Diffusion Models는 다양한 센서 데이터를 합성할 수 있다. 카메라 이미지 생성은 가장 직접적인 응용이다. 특정 날씨, 조명, 도로 조건에서의 이미지를 생성하여 학습 데이터를 증강한다. 예를 들어, 맑은 날 수집한 데이터를 비 오는 날이나 눈 오는 날로 변환할 수 있다. 이는 style transfer나 domain adaptation 기법과 결합되어 더욱 현실적인 결과를 만든다.

LiDAR 포인트 클라우드 생성도 가능하다. 3차원 장면의 기하학적 구조를 학습하여, 새로운 시점이나 객체 배치에 대한 LiDAR 데이터를 생성한다. 이는 카메라-LiDAR 융합 알고리즘을 학습시킬 때 유용하다. 레이더 신호 합성은 더 어렵지만 연구가 진행 중이다. 레이더는 도플러 효과를 통해 속도 정보를 제공하므로, 이를 정확히 모델링하려면 물리 기반 시뮬레이션이 필요하다.

네 번째 응용은 occupancy prediction이다. Occupancy grid는 주변 공간의 각 셀이 점유되어 있는지, 주행 가능한지를 나타내는 표현이다. 미래 occupancy를 예측하면, 안전한 경로를 계획하는 데 직접 활용할 수 있다. Diffusion Models는 현재 관측과 행동 계획을 조건으로, 미래 시점의 occupancy grid를 생성한다. 이는 명시적으로 객체를 검출하고 추적하지 않아도, 주행 가능한 공간을 예측할 수 있게 한다.

Diffusion Models는 불확실성 정량화에도 유리하다. 여러 번 샘플링하여 생성된 결과들의 분산을 통해, 예측의 불확실성을 추정할 수 있다. 예를 들어, 궤적 생성에서 여러 샘플이 비슷한 경로를 보인다면 높은 확신을, 샘플들이 크게 다르다면 높은 불확실성을 의미한다. 이러한 불확실성 정보는 의사결정에 활용될 수 있다. 불확실성이 높은 상황에서는 더 보수적으로 행동하거나, 추가 정보를 수집하려 시도할 수 있다.

최근 연구들은 Diffusion Models를 자율주행의 전체 파이프라인에 통합하려 시도하고 있다. UniAD는 인지, 예측, 계획을 하나의 Diffusion 기반 프레임워크로 통합한다. 각 단계가 조건부 생성으로 정식화되며, 전체 시스템이 end-to-end로 학습된다. MotionDiffuser는 다중 에이전트의 미래 모션을 공동으로 예측하는 Diffusion Model이다. 여러 차량과 보행자의 궤적을 동시에 생성하며, 이들 간의 상호작용과 사회적 규범을 학습한다.

Diffusion Models의 계산 비용은 여전히 과제이다. 수십 또는 수백 단계의 반복적 정제가 필요하므로, 단일 forward pass에 비해 느리다. 실시간 자율주행에서는 밀리초 단위의 지연도 중요하므로, 이는 실용성의 장벽이 될 수 있다. 그러나 여러 기법들이 이를 완화하고 있다. DDIM과 같은 빠른 샘플링 방법은 단계 수를 10배 이상 줄일 수 있다. Knowledge distillation은 느린 Diffusion Model의 출력을 빠른 단일 단계 모델로 증류한다. 전용 하드웨어 가속과 최적화 기법도 추론 속도를 크게 향상시킨다.

Diffusion Models와 생성 AI는 자율주행을 데이터 생성과 합성의 시대로 이끌고 있다. 실제 데이터 수집만으로는 불가능했던 다양성과 규모를 달성할 수 있다. 드문 상황, 위험한 상황, 비현실적이지만 테스트에 필요한 상황들을 안전하게 생성하고 활용할 수 있다. 이는 자율주행 시스템의 강건성과 안전성을 크게 향상시킬 것이다. 앞으로 생성 AI 기술의 지속적인 발전과 함께, 자율주행에서의 응용도 더욱 확대되고 정교해질 것으로 기대된다.

== Bird's Eye View 표현과 공간 이해

Bird's Eye View는 자율주행에서 다중 카메라 정보를 통합하고 3차원 공간을 이해하는 강력한 표현 방법이다. 전통적으로 각 카메라 이미지는 독립적으로 처리되었으며, 이들의 정보를 통합하는 것은 복잡한 기하학적 변환과 센서 융합 알고리즘을 필요로 했다. BEV 표현은 이를 단일하고 일관된 좌표계로 통합하여, downstream 작업을 크게 단순화한다.

BEV의 핵심 아이디어는 모든 센서 정보를 위에서 내려다본 조감도 관점으로 변환하는 것이다. 자율주행 차량은 주변 360도를 커버하는 여러 카메라를 가지며, 각 카메라는 다른 방향과 시야각을 가진다. BEV 변환은 이러한 다양한 관점의 이미지들을 단일한 2차원 격자로 투영한다. 이 격자의 각 셀은 실제 세계의 특정 위치에 대응하며, 그 위치에서의 특징이나 점유 상태를 나타낸다.

BEV 표현의 장점은 여러 가지이다. 첫째, 공간적 일관성이다. 원근 투영된 카메라 이미지에서는 거리에 따라 객체의 크기가 변하고, 멀리 있는 객체는 작게 보인다. BEV에서는 모든 위치가 동일한 metric 스케일을 가지므로, 거리와 크기가 직접적으로 해석된다. 이는 경로 계획과 제어에 매우 유용하다. 둘째, 다중 카메라 융합이 자연스럽다. 각 카메라의 정보가 공통의 BEV 공간에 투영되므로, 중복되는 영역에서는 정보가 자동으로 융합되고, 가려진 영역은 다른 카메라로 보완된다.

셋째, 시간적 통합이 용이하다. 차량이 움직이면서 관측되는 BEV는 차량의 자세 변화에 따라 변환될 수 있다. 과거 프레임의 BEV를 현재 좌표계로 변환하여 누적하면, 단일 프레임에서는 보이지 않던 영역도 시간에 따라 채워질 수 있다. 이는 부분 관측 문제를 완화한다. 넷째, downstream 작업이 단순해진다. 객체 검출, 차선 인식, 주행 가능 영역 분할 등이 모두 BEV 공간에서 수행되며, 이는 2차원 이미지에서보다 기하학적으로 직관적이다.

BEV 변환을 구현하는 방법은 여러 가지이다. 초기 접근은 명시적인 기하학적 투영이었다. 카메라의 내재 파라미터와 외재 파라미터를 사용하여, 각 이미지 픽셀이 BEV 그리드의 어느 위치에 대응하는지 계산한다. 그러나 이는 깊이 정보를 필요로 하며, 카메라만으로는 정확한 깊이를 알기 어렵다. 따라서 깊이 추정 네트워크를 먼저 학습하고, 이를 바탕으로 투영을 수행한다.

LSS는 이러한 접근을 체계화한 방법이다. 세 단계로 구성된다. Lift 단계에서는 각 이미지 픽셀에 대해 가능한 깊이 범위를 고려하여, 2차원 특징을 3차원 공간으로 확장한다. 각 픽셀이 여러 깊이 가설을 가지며, 깊이 확률 분포와 함께 3차원 특징 볼륨을 만든다. Splat 단계에서는 이 3차원 특징들을 BEV 그리드로 투영한다. 동일한 BEV 셀에 투영되는 여러 3차원 특징들을 합산하거나 평균내어 통합한다. Shoot 단계에서는 생성된 BEV 특징으로 객체 검출이나 분할 등의 작업을 수행한다.

BEVFormer는 Transformer 아키텍처를 활용한 더 발전된 방법이다. 명시적인 기하학적 투영 대신, attention 메커니즘을 통해 이미지 특징과 BEV 특징을 연결한다. BEV grid의 각 위치를 query로 표현하고, 이미지 특징들을 key와 value로 사용한다. Spatial cross-attention은 각 BEV query가 해당 위치와 관련된 이미지 영역에 attention을 주도록 학습된다. 예를 들어, BEV의 특정 위치는 그 위치를 바라보는 카메라들의 픽셀에 높은 attention weight를 가진다.

BEVFormer의 또 다른 혁신은 temporal attention이다. 현재 프레임의 BEV뿐만 아니라, 과거 프레임의 BEV도 활용한다. 차량의 움직임 정보를 사용하여 과거 BEV를 현재 좌표계로 정렬하고, temporal self-attention을 통해 시간적 정보를 통합한다. 이는 가려졌다가 나타나는 객체를 추적하거나, 부분 관측으로부터 완전한 장면을 재구성하는 데 도움이 된다.

BEVDet과 BEVDepth는 깊이 추정을 명시적으로 학습하는 방법이다. 카메라 이미지로부터 각 픽셀의 깊이 분포를 예측하는 네트워크를 학습한다. 감독 신호로는 LiDAR로부터 얻은 ground truth 깊이나, stereo 카메라로부터 계산된 깊이가 사용된다. 정확한 깊이 추정은 BEV 변환의 품질을 크게 향상시킨다. 특히 먼 거리의 객체나 작은 객체를 정확히 위치시키는 데 중요하다.

BEV 표현의 응용은 다양하다. 3D 객체 검출은 가장 직접적인 응용이다. BEV 공간에서 차량, 보행자, 자전거 등의 3D bounding box를 예측한다. 이는 이미지 기반 검출보다 거리와 크기를 정확히 추정할 수 있다. 지도 세그멘테이션은 차선, 도로 경계, 횡단보도, 주차 공간 등을 BEV에서 분할한다. 이는 HD map이 없는 환경에서도 로컬 지도를 생성할 수 있게 한다.

Occupancy prediction은 BEV의 각 셀이 점유되어 있는지, 어떤 유형의 객체인지 예측한다. 명시적인 bounding box 없이도 주행 가능 공간과 장애물을 이해할 수 있다. 이는 명시적으로 정의되지 않은 객체나 복잡한 형태의 장애물을 다루는 데 유리하다. Motion forecasting은 다른 에이전트의 미래 궤적을 BEV에서 예측한다. 공간적 맥락과 에이전트 간 상호작용을 고려하여, 사회적으로 타당한 예측을 생성한다.

경로 계획은 BEV occupancy와 motion forecast를 바탕으로 안전한 궤적을 생성한다. BEV 공간에서 직접 계획하므로, 이미지 좌표와 실제 좌표 간 변환이 불필요하다. End-to-end 학습에서는 BEV 특징이 계획 네트워크의 입력으로 직접 사용된다. Tesla의 FSD는 이러한 접근을 채택하여, BEV 표현을 중심으로 전체 파이프라인을 구성한다.

BEV 표현의 해상도와 범위는 trade-off 관계에 있다. 높은 해상도는 세밀한 정보를 제공하지만, 메모리와 계산 비용이 크다. 넓은 범위는 먼 거리까지 인식할 수 있지만, 동일한 해상도에서는 더 많은 셀을 필요로 한다. 일반적으로 차량 주변 50-100미터, 해상도 0.2-0.5미터 정도가 사용된다. 적응적 해상도 기법도 연구되고 있는데, 차량 근처는 높은 해상도로, 먼 곳은 낮은 해상도로 표현하여 효율성을 높인다.

BEV와 LiDAR의 통합은 센서 융합의 강력한 방법이다. LiDAR 포인트 클라우드는 본질적으로 BEV와 유사한 표현이므로, 카메라 기반 BEV와 자연스럽게 융합된다. 두 modality의 특징을 BEV 공간에서 concatenate하거나 attention으로 융합하여, 각각의 장점을 결합한다. 카메라는 시각적 세부사항과 semantic 정보를, LiDAR는 정확한 기하학과 거리 정보를 제공한다.

BEV 표현은 자율주행의 공간 이해를 혁신적으로 개선했다. 다중 센서와 시간 정보를 통합하는 자연스러운 프레임워크를 제공하며, 이는 인지, 예측, 계획의 모든 단계에서 활용된다. Transformer 아키텍처와의 결합은 end-to-end 학습을 가능하게 하며, 전체 시스템의 성능을 향상시킨다. Tesla, Waymo를 비롯한 주요 자율주행 기업들이 BEV를 핵심 기술로 채택하고 있으며, 이는 앞으로도 자율주행 시스템의 표준적인 표현 방법으로 자리잡을 것이다.

#pagebreak()

= 글로벌 자율주행 산업의 현황

== Tesla의 Vision-Only 접근과 FSD 시스템

Tesla는 자율주행 분야에서 가장 독특하고 논쟁적인 접근 방식을 취하고 있다. 대부분의 경쟁사들이 LiDAR, 레이더, 카메라를 조합한 다중 센서 시스템을 구축하는 동안, Tesla는 2021년부터 순수 카메라 기반, 즉 vision-only 접근을 채택했다. 이러한 선택은 기술적, 경제적, 철학적 이유가 복합적으로 작용한 결과이다.

Elon Musk와 Tesla의 엔지니어들은 인간이 주로 시각만으로 운전한다는 점을 강조한다. 인간 운전자는 LiDAR 없이도 안전하게 주행하며, 충분히 발전된 컴퓨터 비전 시스템이라면 동일한 성능을 달성할 수 있다는 것이 그들의 주장이다. 더 나아가, 도로 인프라와 교통 규칙은 인간의 시각 인지를 전제로 설계되었으므로, 카메라 기반 시스템이 가장 자연스럽다고 본다. 경제적 측면에서도 카메라는 LiDAR에 비해 훨씬 저렴하다. LiDAR 시스템은 차량당 수천에서 수만 달러의 비용이 들지만, 고품질 카메라는 수백 달러 수준이다. Tesla가 목표로 하는 대중 시장 자율주행을 실현하려면 센서 비용의 절감이 필수적이다.

또한 Tesla는 이미 생산되어 도로를 주행 중인 수백만 대의 차량을 가지고 있다. 이들 차량은 모두 카메라 기반 센서 구성을 가지며, 소프트웨어 업데이트만으로 자율주행 기능을 추가하거나 개선할 수 있다. 만약 LiDAR가 필수적이라면, 기존 차량들은 자율주행의 혜택을 받을 수 없게 된다. Vision-only 접근은 이러한 기존 플릿을 활용할 수 있게 한다.

Tesla의 센서 구성은 8개의 카메라로 이루어진다. 전방을 향한 3개의 카메라는 각각 다른 초점 거리를 가져 넓은 시야각과 먼 거리를 동시에 커버한다. 측면과 후방을 향한 5개의 카메라는 차량 주변 360도를 완전히 감지한다. 이들 카메라는 초당 36프레임으로 촬영하며, 약 250미터까지의 거리를 인식할 수 있다. Hardware 3.0 컴퓨터는 144 TOPS의 성능을 제공하며, 2023년부터 도입된 Hardware 4.0는 더 향상된 성능과 센서를 가진다.

FSD의 소프트웨어 아키텍처는 완전한 end-to-end 학습을 지향한다. 초기 버전에서는 여전히 모듈식 구성 요소들이 남아 있었지만, 버전이 올라감에 따라 점점 더 통합되었다. FSD Beta v12부터는 계획 단계도 신경망으로 대체되어, 카메라 입력에서 차량 제어까지 거의 전체가 신경망으로 처리된다. 이는 자율주행 분야에서 가장 극단적인 end-to-end 시스템 중 하나이다.

FSD의 핵심은 강력한 비전 시스템이다. Vision Transformer 기반의 백본 네트워크가 각 카메라 이미지로부터 특징을 추출한다. 이들 특징은 BEV 변환을 거쳐 단일한 조감도 표현으로 통합된다. BEV 공간에서 occupancy network가 각 위치의 점유 상태를 예측한다. 이는 명시적인 객체 검출 없이도 주행 가능 공간과 장애물을 이해할 수 있게 한다. 물론 차량, 보행자 등의 객체 검출과 추적도 수행되지만, 이는 보조적인 역할이다.

Planning 네트워크는 BEV 표현과 고수준 목표를 입력으로 받아, 미래 몇 초간의 주행 궤적을 출력한다. 이 네트워크는 수백만 마일의 인간 주행 데이터로 학습되었으며, 안전성, 효율성, 승차감을 동시에 고려하도록 훈련되었다. 흥미롭게도, Tesla는 명시적인 교통 규칙을 프로그래밍하지 않는다. 대신, 인간 운전자가 따르는 규칙을 데이터로부터 암묵적으로 학습한다. 예를 들어, 빨간 신호에서 멈추는 것, 교차로에서 양보하는 것, 제한 속도를 지키는 것 등이 모두 학습된 행동이다.

Tesla의 가장 큰 자산은 데이터이다. 2025년 기준으로 전 세계에 수백만 대의 Tesla 차량이 주행하고 있으며, 대부분이 데이터 수집에 동의했다. 이들 차량은 끊임없이 주행 영상과 차량 데이터를 수집하며, 특정 조건에서는 Tesla 서버로 전송한다. 모든 데이터를 전송하는 것은 불가능하므로, 차량 내에서 흥미로운 이벤트를 필터링한다. Shadow mode는 FSD 시스템이 백그라운드에서 작동하여 자신이 어떻게 주행했을지 기록하고, 실제 인간 운전자의 행동과 비교한다. 차이가 크거나 특이한 상황에서는 해당 데이터가 업로드 대상으로 표시된다.

이렇게 수집된 데이터는 Tesla의 데이터 엔진을 구동한다. 엔지니어들은 시스템이 실패하거나 어려움을 겪는 상황을 식별한다. 해당 상황의 데이터를 수집하고 레이블링하여 재학습에 사용한다. 이러한 순환은 지속적으로 반복되며, 시스템은 점점 더 다양한 상황을 처리할 수 있게 된다. Tesla는 이를 "flywheel" 효과라고 부른다. 더 많은 차량이 더 많은 데이터를 생성하고, 더 많은 데이터가 더 나은 시스템을 만들며, 더 나은 시스템이 더 많은 차량 판매로 이어진다.

자동 레이블링 파이프라인도 중요한 역할을 한다. 모든 데이터에 사람이 레이블을 다는 것은 불가능하므로, 이미 학습된 모델이 새로운 데이터에 자동으로 레이블을 달고, 사람은 불확실한 경우만 검토한다. 또한 여러 프레임과 다중 카메라의 정보를 통합하여 레이블의 일관성과 정확성을 높인다. 예를 들어, 한 프레임에서 가려진 객체도 다른 프레임이나 다른 카메라에서는 보일 수 있으며, 이를 종합하여 더 정확한 3D 위치를 추정한다.

FSD의 성능은 버전이 올라감에 따라 꾸준히 개선되고 있다. 초기 버전은 주로 고속도로에서만 작동했지만, 최근 버전은 복잡한 도심 환경에서도 주행할 수 있다. 개입 빈도, 즉 인간이 제어권을 가져가야 하는 빈도는 지속적으로 감소하고 있다. Tesla는 내부적으로 miles per intervention 지표를 추적하며, 이는 수백 마일에서 수천 마일로 향상되었다고 알려져 있다. 그러나 아직 완전 무인 자율주행 수준에는 이르지 못했다.

Vision-only 접근의 한계도 명확하다. 카메라는 악천후에 취약하다. 비, 눈, 안개는 시야를 가리며, 야간이나 역광 상황에서는 인식 성능이 저하된다. 깊이 추정은 본질적으로 어려운 문제이며, 특히 질감이 없는 표면이나 먼 거리에서는 부정확할 수 있다. LiDAR는 이러한 상황에서 더 강건하고 정확한 거리 정보를 제공한다. 또한 카메라는 redundancy가 부족하다. 카메라가 고장나거나 가려지면 해당 방향의 인식이 불가능해진다. 다중 센서 시스템은 하나의 센서가 실패해도 다른 센서로 보완할 수 있다.

비평가들은 Tesla의 접근이 안전성을 희생한다고 주장한다. 규제 기관과 안전 단체들은 FSD의 사고 사례를 지적하며, 더 엄격한 검증이 필요하다고 강조한다. Tesla는 통계적으로 FSD가 인간 운전자보다 안전하다고 주장하지만, 독립적인 검증 데이터는 제한적이다. 또한 FSD는 여전히 감독이 필요한 Level 2+ 시스템이며, 완전 자율주행인 Level 4/5와는 거리가 있다.

그럼에도 Tesla의 접근은 자율주행 분야에 큰 영향을 미치고 있다. 다른 기업들도 카메라 중심 시스템을 재고하고 있으며, 일부는 LiDAR를 선택 사항으로 만들거나 장기적으로 제거할 계획을 가지고 있다. Vision-only가 충분히 발전한다면, 이는 자율주행의 대중화를 크게 앞당길 수 있다. Tesla는 이러한 미래에 가장 가까이 있는 기업이며, 그들의 성공 여부는 자율주행 산업 전체의 방향을 결정할 것이다.

== Waymo의 Robotaxi 서비스와 Level 4 달성

Waymo는 자율주행 분야의 선구자이자 가장 보수적이고 체계적인 접근을 취하는 기업이다. Google의 자율주행 프로젝트로 시작하여 2016년 독립 회사가 된 Waymo는, 완전 무인 자율주행, 즉 SAE Level 4를 달성한 몇 안 되는 기업 중 하나이다. Tesla가 대중 시장과 빠른 반복을 추구한다면, Waymo는 안전성과 확실성을 최우선으로 하며 점진적으로 확장한다.

Waymo의 기술 철학은 "safety by design"이다. 시스템의 모든 구성 요소는 안전을 최우선으로 설계되며, 다중 redundancy가 핵심 원칙이다. Waymo Driver라고 불리는 자율주행 시스템은 29개의 카메라, 5개의 LiDAR, 6개의 레이더를 탑재한다. 이는 Tesla의 8개 카메라와 극명한 대조를 이룬다. 각 센서는 다른 센서의 약점을 보완하며, 하나가 실패해도 시스템은 계속 작동할 수 있다.

LiDAR는 Waymo 시스템의 중추이다. LiDAR는 레이저 펄스를 발사하고 반사되는 시간을 측정하여 거리를 정확히 계산한다. 이는 날씨와 조명에 관계없이 일관된 성능을 제공하며, 카메라가 어려움을 겪는 야간이나 역광 상황에서도 강건하다. Waymo는 자체 LiDAR 하드웨어를 개발하여 성능을 최적화하고 비용을 절감했다. 최신 세대 LiDAR는 300미터 이상의 거리를 감지하며, 작은 객체도 정확히 인식한다.

카메라는 시각적 세부사항과 색상 정보를 제공한다. 교통 신호등의 색상, 표지판의 텍스트, 차선 표시의 유형 등은 카메라로만 인식할 수 있다. Waymo의 카메라는 고동적 범위를 가지며, 밝은 하늘과 어두운 그림자를 동시에 처리할 수 있다. 레이더는 속도 정보를 제공하며, 도플러 효과를 통해 다가오는 차량의 상대 속도를 직접 측정한다. 이는 충돌 예측과 긴급 제동에 중요하다.

Waymo의 소프트웨어는 모듈식 아키텍처를 유지하면서도, 각 모듈을 높은 수준으로 발전시켰다. 인지 시스템은 다중 센서 융합을 통해 주변 환경의 3D 모델을 구축한다. 객체의 위치, 크기, 방향, 속도를 정확히 추정하며, 불확실성도 정량화한다. 예측 시스템은 다른 에이전트의 미래 행동을 예측한다. 단순한 궤적 예측을 넘어, 의도와 주의를 추론한다. 예를 들어, 보행자가 도로를 횡단하려는지, 단지 도로변에 서 있는 것인지 판단한다.

계획 시스템은 안전성을 최우선으로 하는 계층적 구조를 가진다. 상위 계획은 목적지까지의 전체 경로를 결정하며, HD map과 실시간 교통 정보를 활용한다. 중간 계획은 현재 도로에서의 전략을 결정한다. 차선 변경, 추월, 합류 등의 기동을 계획한다. 하위 계획은 구체적인 궤적을 생성하며, 장애물 회피와 승차감을 고려한다. 각 단계에서 여러 옵션이 평가되며, 안전성 검증을 통과한 것만 실행된다.

Waymo의 안전성 검증은 매우 엄격하다. 계획된 모든 궤적은 충돌 가능성을 확인하며, 조금이라도 위험이 있다면 거부된다. 최악의 경우 분석을 통해, 다른 차량이 예상과 다르게 행동해도 안전을 보장할 수 있는지 평가한다. 비상 제동 시스템은 항상 대기 상태이며, 밀리초 단위로 위험을 감지하고 대응한다. 하드웨어 redundancy도 중요하다. 중요한 구성 요소들은 이중화되어 있으며, 하나가 실패해도 시스템은 안전하게 정지하거나 제한된 기능으로 계속 작동할 수 있다.

Waymo One은 Waymo의 상용 Robotaxi 서비스이다. 2018년 피닉스에서 시작하여, 현재는 샌프란시스코, 로스앤젤레스, 오스틴 등으로 확장되었다. 2025년 기준으로 주당 15만 회 이상의 유료 승차를 제공하며, 24시간 운영된다. 완전 무인으로 운영되며, 안전 운전자가 동승하지 않는다. 사용자는 Waymo One 앱으로 차량을 호출하며, 경험은 Uber나 Lyft와 유사하다.

Waymo의 확장 전략은 매우 신중하다. 새로운 도시로 진출하기 전에 철저한 준비가 이루어진다. 먼저 해당 도시의 HD map을 구축한다. 이는 수 센티미터 정확도의 3D 지도로, 모든 차선, 표지판, 신호등, 도로 경계를 포함한다. 맵핑 차량이 해당 지역을 여러 번 주행하며 데이터를 수집하고, 이를 처리하여 지도를 생성한다. 그 다음 안전 운전자가 동승한 상태로 수개월간 테스트 주행을 한다. 시스템의 성능을 평가하고, 지역 특유의 교통 패턴을 학습한다.

충분한 데이터와 경험이 축적되면, 제한된 구역에서 무인 주행을 시작한다. 초기에는 승객 없이 차량만 주행하며, 원격 모니터링 센터에서 감시한다. 문제가 없다고 판단되면, 소수의 선택된 사용자에게 서비스를 제공한다. 피드백을 수집하고 개선한 후, 점진적으로 서비스 지역과 시간대를 확장한다. 이러한 과정은 수년이 걸릴 수 있지만, Waymo는 서두르지 않는다. 안전성이 검증될 때까지 확장하지 않는 것이 원칙이다.

Waymo의 안전 기록은 인상적이다. 수천만 마일의 자율주행을 기록했으며, 인간 운전 평균 대비 훨씬 낮은 사고율을 보인다고 Waymo는 발표했다. 그러나 사고가 전혀 없는 것은 아니다. 경미한 접촉 사고들이 보고되었으며, 일부는 Waymo 차량의 판단 오류였다. 각 사고는 철저히 분석되며, 재발 방지를 위한 개선이 이루어진다.

Waymo의 과제는 경제성이다. 고가의 센서와 HD map 구축, 그리고 느린 확장 속도는 높은 비용을 의미한다. Robotaxi 서비스가 수익성을 달성하려면, 운영 비용을 크게 줄이거나 서비스 가격을 올려야 한다. Waymo는 센서 비용을 지속적으로 절감하고 있으며, 규모의 경제를 통해 단가를 낮추려 한다. 또한 차량 활용도를 높이고, 유지 보수를 자동화하여 운영 효율을 개선한다.

2025년 말까지 Waymo는 2500대 규모의 플릿을 목표로 하고 있다. 현재 주로 Jaguar I-PACE 전기차를 사용하지만, Geely의 Zeekr 차량으로도 확장하고 있다. 더 저렴하고 자율주행에 최적화된 차량으로 전환하면 경제성이 개선될 것으로 기대된다. 장기적으로 Waymo는 자체 설계한 전용 자율주행 차량을 도입할 계획도 가지고 있다. 운전석이 없고, 승객 공간을 최대화하며, 자율주행 하드웨어가 통합된 차량이다.

Waymo의 성공은 자율주행 산업에 중요한 의미를 가진다. Level 4 자율주행이 기술적으로 가능하며, 상업적으로 운영될 수 있음을 입증했다. 이는 규제 기관과 대중에게 자율주행의 실현 가능성을 확신시킨다. 동시에 Waymo의 보수적이고 비용이 많이 드는 접근은, 자율주행의 대중화가 얼마나 어려운지도 보여준다. Tesla의 빠르고 저렴한 접근과 Waymo의 느리고 확실한 접근 사이의 긴장은 자율주행 산업의 미래를 형성할 것이다.

#pagebreak()

= 결론

본 보고서는 End-to-End 자율주행 기술의 현황과 미래를 포괄적으로 분석하였다. 전통적인 모듈식 아키텍처에서 통합적인 End-to-End 학습으로의 패러다임 전환은 단순한 기술적 변화가 아니라, 자율주행 시스템을 설계하고 개발하는 근본적인 철학의 변화를 의미한다. 모방 학습, 강화학습, 인간 피드백 기반 학습 등 다양한 학습 방법론이 발전하고 있으며, 이들의 조합을 통해 인간 수준을 넘어서는 주행 성능을 달성할 가능성이 열리고 있다.

Foundation Models와 생성 AI의 융합은 자율주행 기술에 새로운 지평을 열었다. Vision Transformer는 대규모 사전 학습을 통해 강력한 시각 표현을 학습하며, 적은 데이터로도 새로운 상황에 빠르게 적응할 수 있게 한다. World Models는 미래를 예측하고 시뮬레이션하여 더 안전하고 지능적인 의사결정을 가능하게 한다. Diffusion Models는 현실적인 시나리오를 생성하여 드문 상황에 대한 대응 능력을 향상시킨다. Bird's Eye View 표현과 Query-based 아키텍처는 다중 센서 정보를 효과적으로 통합하고 3차원 공간을 이해하는 능력을 크게 개선했다.

글로벌 산업 생태계는 빠르게 진화하고 있다. Tesla는 vision-only 접근과 완전한 End-to-End 아키텍처로 독자적인 길을 걷고 있으며, 수백만 대의 플릿 데이터를 활용하여 지속적으로 시스템을 개선하고 있다. Waymo는 Level 4 Robotaxi 서비스를 여러 도시로 확장하며, 상업적 자율주행의 실현 가능성을 입증하고 있다. 중국 시장은 정부 지원과 대규모 투자, 빠른 상용화로 세계 자율주행 산업을 선도하고 있다. 한국은 후발 주자로서 인프라와 데이터 측면에서 도전에 직면해 있지만, 제조 역량과 반도체 기술을 바탕으로 기회를 모색하고 있다.

기술적으로는 여전히 해결해야 할 과제들이 남아 있다. Long-tail 이벤트 처리, 안전성의 통계적 검증, 새로운 환경에 대한 일반화, 실시간 처리를 위한 컴퓨팅 효율성 등은 상용화를 위해 극복해야 할 핵심 과제이다. 비기술적으로는 규제 프레임워크의 정립, 윤리적 의사결정 기준 마련, 대중의 신뢰 구축, 일자리 전환에 대한 사회적 대응 등이 필요하다.

향후 5년간 자율주행 기술은 급속히 발전할 것으로 전망된다. 2025-2026년에는 NOA 기능이 중급 차량에도 보급되고, Foundation Models가 표준 기술로 자리잡을 것이다. 2027-2028년에는 도시 전역에서 Level 3/Level 4 자율주행이 상용화되고, World Models 기반 planning이 핵심 기술로 부상할 것이다. 2029-2030년에는 제한된 영역에서 완전 무인 자율주행이 실현되고, Robotaxi의 경제성이 입증될 것이다. 이를 통해 교통사고가 감소하고, 모빌리티 접근성이 향상되며, 도시 구조와 생활 방식이 근본적으로 변화할 것이다.

자율주행의 미래는 단순히 운전의 자동화를 넘어, 더 안전하고 효율적이며 지속 가능한 교통 시스템을 실현하는 것이다. 기술 개발뿐만 아니라 사회적 합의, 규제 정비, 인프라 투자가 함께 이루어져야 진정한 자율주행 시대가 열릴 것이다. 한국은 이러한 글로벌 경쟁에서 기술력과 제조 역량을 바탕으로 독자적인 경쟁력을 확보할 수 있을 것으로 기대된다.
