#set page(
  paper: "a4",
  margin: (x: 2.2cm, y: 2.5cm),
  numbering: "1",
)

#set text(
  font: "Linux Libertine",
  size: 11pt,
  lang: "ko",
)

#set par(
  justify: true,
  leading: 0.75em,
  first-line-indent: 1em,
)

#set heading(numbering: "1.1")

#show heading.where(level: 1): it => {
  pagebreak(weak: true)
  v(1.5em)
  block(
    width: 100%,
    fill: rgb("#1e40af"),
    inset: 18pt,
    radius: 3pt,
    text(fill: white, size: 22pt, weight: "bold")[
      #counter(heading).display() #h(0.5em) #it.body
    ]
  )
  v(1.2em)
}

#show heading.where(level: 2): it => {
  v(1.2em)
  block(
    width: 100%,
    fill: rgb("#dbeafe"),
    inset: 12pt,
    radius: 2pt,
    text(fill: rgb("#1e40af"), size: 15pt, weight: "bold")[
      #counter(heading).display() #h(0.5em) #it.body
    ]
  )
  v(0.8em)
}

#show heading.where(level: 3): it => {
  v(1em)
  text(size: 13pt, fill: rgb("#1e40af"), weight: "bold")[
    #counter(heading).display() #h(0.5em) #it.body
  ]
  v(0.5em)
}

// 제목 페이지
#align(center)[
  #v(4cm)

  #block(
    width: 90%,
    fill: rgb("#1e40af"),
    inset: 25pt,
    radius: 5pt,
  )[
    #text(size: 32pt, weight: "bold", fill: white)[
      End-to-End 자율주행 기술의\
      현황과 미래 전망
    ]
  ]

  #v(1.5cm)

  #text(size: 18pt, fill: rgb("#334155"), weight: "semibold")[
    Foundation Models와 생성 AI 기반 자율주행 시스템의\
    기술적 진화와 산업 생태계 분석
  ]

  #v(3cm)

  #block(
    width: 70%,
    stroke: 1pt + rgb("#cbd5e1"),
    inset: 15pt,
    radius: 3pt,
  )[
    #text(size: 16pt, weight: "bold")[
      한국전자통신연구원 (KETI)
    ]

    #v(0.5em)

    #text(size: 13pt, fill: rgb("#64748b"))[
      지능형 모빌리티 연구센터
    ]
  ]

  #v(1cm)

  #text(size: 14pt, fill: rgb("#64748b"))[
    2025년 10월
  ]

  #v(2cm)

  #align(left)[
    #block(
      width: 100%,
      fill: rgb("#f8fafc"),
      inset: 15pt,
      radius: 3pt,
    )[
      #text(size: 10pt, fill: rgb("#475569"))[
        *요약*: 본 보고서는 자율주행 기술의 최신 패러다임인 End-to-End 학습 방식과 Foundation Models의 융합을 중심으로, 글로벌 자율주행 산업의 기술적 발전 현황과 주요 기업들의 상용화 전략을 종합적으로 분석한다. 특히 Tesla의 FSD, Waymo의 Robotaxi, 그리고 중국 시장의 급속한 발전을 심층 분석하고, 2025년부터 2030년까지의 기술 로드맵과 산업 전망을 제시한다.
      ]
    ]
  ]
]

#pagebreak()

// 목차
#outline(
  title: [목차],
  depth: 3,
  indent: auto,
)

#pagebreak()

= 서론

== 연구 배경 및 목적

자율주행 기술은 21세기 교통 혁명의 핵심으로 부상하고 있다. 지난 수십 년간 모듈식 아키텍처를 중심으로 발전해온 자율주행 시스템은, 최근 인공지능 기술의 비약적 발전과 함께 End-to-End 학습 방식이라는 새로운 패러다임으로 전환되고 있다. 전통적인 모듈식 접근은 인지, 예측, 계획, 제어를 독립된 모듈로 구현하여 개발과 검증이 체계적이라는 장점이 있었으나, 모듈 간 정보 손실과 부분 최적화라는 근본적 한계를 가지고 있었다.

End-to-End 학습은 센서 입력에서 제어 명령까지 전체 과정을 단일 신경망으로 학습하여 이러한 한계를 극복한다. 1980년대 ALVINN에서 처음 제안되었으나 실용화되지 못했던 이 접근은, 2010년대 중반 딥러닝의 발전, GPU 성능 향상, 대규모 데이터 축적으로 재조명받기 시작했다. 특히 2016년 NVIDIA의 연구와 Tesla의 FSD 시스템 도입은 이 패러다임의 실현 가능성을 입증했다.

최근에는 자연어 처리와 컴퓨터 비전에서 성공한 Foundation Models가 자율주행에 도입되고 있다. Vision Transformer, World Models, Diffusion Models 등이 인지, 예측, 시뮬레이션 능력을 혁신적으로 향상시키고 있으며, Bird's Eye View 표현과 결합하여 3차원 공간 이해 능력을 대폭 개선했다.

본 보고서는 이러한 기술적 진화의 맥락에서 End-to-End 자율주행 시스템의 현황을 종합적으로 분석한다. 주요 학습 방법론, Foundation Models와 생성 AI의 통합, 글로벌 기업들의 기술 개발 현황과 상용화 전략을 다룬다. 특히 Tesla의 vision-only 접근, Waymo의 Level 4 Robotaxi, 중국 시장의 급속한 발전을 중점 분석하며, 2025년부터 2030년까지의 기술 로드맵과 산업 전망을 제시한다.

== 연구 방법 및 범위

본 연구는 문헌 조사, 산업 동향 분석, 기술 벤치마킹을 종합하는 접근 방식을 채택하였다. 학술 문헌은 CVPR, ICCV, NeurIPS, ICRA 등 주요 학회의 최근 3년간 논문과 arXiv 사전공개 논문을 중심으로 조사하였다. 산업 동향은 Tesla, Waymo, Cruise, Baidu Apollo 등 선도 기업들의 공개 자료, 기술 블로그, 투자자 발표 자료를 분석하였으며, Gartner, McKinsey, ARK Invest 등의 산업 분석 보고서를 참고하였다. 기술 벤치마킹은 nuScenes, Waymo Open Dataset, Argoverse 등의 공개 데이터셋과 CARLA 시뮬레이터, Apollo 플랫폼 등 오픈소스 프로젝트를 활용하였다.

시간적 범위는 2020년부터 2025년 현재까지를 중심으로 하되 2030년까지의 미래 전망을 포함한다. 지리적 범위는 미국(기술 선도), 중국(빠른 상용화), 유럽(안전 규제), 한국(후발 주자)을 중점 다룬다. 기술적 범위는 End-to-End 학습, Foundation Models, 생성 AI, BEV 표현, Transformer 아키텍처 등 소프트웨어와 AI 알고리즘에 초점을 둔다.

#pagebreak()

= End-to-End 자율주행의 기술적 기반

== 패러다임의 전환: 모듈식에서 통합식으로

자율주행 기술의 발전 과정에서 가장 근본적인 변화는 시스템 아키텍처의 패러다임 전환이다. 초기 자율주행 연구는 인간 운전자의 의사결정 과정을 모방하여 인지, 예측, 계획, 제어라는 네 가지 단계로 분해하고 각각을 독립적인 모듈로 구현하는 방식을 택했다. 이러한 모듈식 아키텍처는 2007년 DARPA Urban Challenge에서 우승한 Carnegie Mellon University의 Boss 시스템에서 전형적으로 나타나며, 이후 십여 년간 자율주행 시스템 개발의 표준적인 접근 방식으로 자리잡았다.

모듈식 아키텍처의 첫 번째 단계인 인지 모듈은 카메라, LiDAR, 레이더 등의 센서로부터 원시 데이터를 수집하여 주변 환경을 이해한다. 구체적으로는 차량, 보행자, 자전거, 교통 표지판 등의 객체를 검출하고 분류하며, 차선과 도로 경계를 인식하고, 교통 신호등의 상태를 파악한다. 2010년대 초반에는 주로 수작업으로 설계된 특징 추출기와 지지 벡터 머신, 랜덤 포레스트 같은 전통적 기계학습 알고리즘이 사용되었으나, 2015년 이후부터는 깊은 합성곱 신경망이 인지 모듈의 핵심 기술로 자리잡았다. AlexNet, VGG, ResNet과 같은 ImageNet에서 사전 학습된 네트워크들이 전이 학습을 통해 자율주행에 적용되었으며, YOLO, SSD, Faster R-CNN 같은 실시간 객체 검출 알고리즘들이 개발되었다.

두 번째 단계인 예측 모듈은 인지 모듈이 출력한 객체 정보를 받아 각 객체의 미래 행동을 예측한다. 예를 들어, 앞 차량이 차선을 변경할 것인지, 보행자가 도로를 횡단할 것인지, 교차로에서 다른 차량이 어떤 경로로 진행할 것인지 등을 판단한다. 초기에는 일정 속도 모델이나 칼만 필터와 같은 단순한 물리 기반 모델이 사용되었으나, 점차 다항식 궤적 피팅, 가우시안 프로세스, 그리고 최근에는 순환 신경망과 Transformer 기반의 딥러닝 모델들이 활용되고 있다. 특히 사회적 상호작용을 고려한 예측 모델들이 개발되어, 여러 에이전트가 서로 영향을 주고받는 복잡한 교통 상황에서의 예측 정확도가 크게 향상되었다.

세 번째 단계인 계획 모듈은 예측 결과를 바탕으로 자차의 주행 궤적을 생성한다. 이는 다시 경로 계획과 행동 계획으로 나뉠 수 있다. 경로 계획은 출발지에서 목적지까지의 전체 경로를 결정하는 것으로, 주로 HD 맵과 네비게이션 정보를 활용하여 A-star, Dijkstra와 같은 그래프 탐색 알고리즘으로 해결된다. 행동 계획은 현재 교통 상황에서 어떤 기동을 할 것인지 결정하는 것으로, 유한 상태 기계나 의사결정 트리가 전통적으로 사용되었다. 궤적 생성 단계에서는 장애물 회피, 차선 준수, 승차감, 법규 준수 등의 다양한 제약 조건을 만족하는 최적 궤적을 찾아야 하며, 이를 위해 최적화 기법, 샘플링 기반 계획, 격자 기반 탐색 등이 활용된다.

마지막 단계인 제어 모듈은 계획된 궤적을 실제 차량 동작으로 변환한다. 조향, 가속, 제동 등의 제어 명령을 생성하여 차량이 원하는 궤적을 정확히 추종하도록 한다. PID 제어기, 모델 예측 제어, 선형 이차 제어 등의 고전 제어 이론이 주로 사용되며, 차량의 동역학 모델을 기반으로 현재 상태와 목표 상태의 차이를 최소화한다.

모듈식 아키텍처는 몇 가지 명확한 장점을 가진다. 첫째, 각 모듈을 독립적으로 개발하고 테스트할 수 있어 개발 프로세스가 체계적이고 효율적이다. 여러 팀이 동시에 다른 모듈을 개발할 수 있으며, 특정 모듈의 성능 문제를 격리하여 개선할 수 있다. 둘째, 시스템의 동작을 인간이 이해하고 해석할 수 있다. 각 모듈의 중간 출력을 검사하여 시스템이 환경을 어떻게 인식하고 어떤 추론을 거쳐 결정을 내렸는지 추적할 수 있다. 이는 디버깅과 안전성 검증에 매우 유용하다. 셋째, 도메인 지식을 효과적으로 통합할 수 있다. 물리 법칙, 교통 규칙, 인간 운전 습관 등의 선행 지식을 각 모듈에 명시적으로 인코딩할 수 있다.

그러나 모듈식 아키텍처는 근본적인 한계를 가지고 있으며, 이는 시스템 성능의 천장을 만든다. 가장 심각한 문제는 모듈 간 인터페이스에서 발생하는 정보 손실이다. 인지 모듈은 고차원의 센서 데이터를 객체 목록이라는 저차원 표현으로 압축한다. 예를 들어, 1920x1080 해상도의 이미지는 약 200만 픽셀의 정보를 담고 있지만, 이를 처리한 인지 모듈의 출력은 수십 개의 bounding box와 클래스 레이블로 요약된다. 이 과정에서 미세한 질감, 조명 조건, 깊이의 불확실성 등 많은 정보가 소실되며, 예측과 계획 모듈은 이 축약된 정보만으로 의사결정을 해야 한다.

두 번째 문제는 오류의 누적이다. 인지 모듈이 차량을 잘못 검출하거나 분류하면, 예측 모듈은 잘못된 정보를 바탕으로 미래를 예측하고, 계획 모듈은 잘못된 예측을 바탕으로 위험한 궤적을 생성할 수 있다. 각 모듈의 오류가 연쇄적으로 증폭되어 최종 성능에 치명적인 영향을 미칠 수 있다. 특히 드물게 발생하는 복잡한 상황에서 이러한 문제가 두드러진다.

세 번째 문제는 부분 최적화로 인한 전체 성능 저하이다. 각 모듈은 자신만의 목적 함수를 최적화하도록 학습되거나 설계된다. 인지 모듈은 검출 정확도를 최대화하고, 예측 모듈은 예측 오차를 최소화하며, 계획 모듈은 궤적의 부드러움과 안전성을 최적화한다. 그러나 전체 시스템의 목표는 안전하고 효율적이며 편안한 주행을 달성하는 것이며, 이는 개별 모듈의 목표와 다를 수 있다. 예를 들어, 인지 모듈이 모든 객체를 완벽하게 검출하는 것보다 주행에 중요한 객체만 정확히 검출하는 것이 더 유용할 수 있다.

네 번째 문제는 확장성의 한계이다. 새로운 센서를 추가하거나, 새로운 유형의 객체를 인식하거나, 새로운 주행 시나리오를 처리하려면 여러 모듈을 동시에 수정해야 한다. 인터페이스를 변경하면 연결된 모든 모듈에 영향을 미치므로, 시스템의 진화가 어렵다.

이러한 모듈식 아키텍처의 한계를 극복하기 위해 End-to-End 학습 방식이 제안되었다. End-to-End 접근은 센서 입력에서 제어 출력까지의 전체 매핑을 하나의 신경망으로 직접 학습한다. 중간 단계의 명시적인 표현 없이, 데이터로부터 직접 입출력 관계를 학습하는 것이다. 이는 1989년 Pomerleau가 제안한 ALVINN 시스템에서 처음 시도되었으나, 당시의 제한된 컴퓨팅 성능과 작은 데이터셋으로 인해 간단한 도로에서만 작동했다.

End-to-End 학습이 다시 주목받게 된 것은 2010년대 중반, 딥러닝의 부흥과 함께였다. 2016년 NVIDIA의 연구는 단 9층의 합성곱 신경망으로 카메라 이미지에서 조향각을 직접 예측할 수 있음을 보였다. 네트워크는 72시간의 인간 주행 데이터로 학습되었으며, 명시적인 차선 검출이나 경로 계획 없이도 다양한 도로에서 주행할 수 있었다. 더 놀라운 점은 네트워크가 스스로 유용한 중간 표현을 학습했다는 것이다. 네트워크 내부의 활성화를 시각화한 결과, 초기 층은 에지와 텍스처를 검출하고, 중간 층은 차선과 도로 경계를 감지하며, 후기 층은 주행 가능한 영역을 표현하는 것으로 나타났다. 이는 명시적으로 지도하지 않았음에도 네트워크가 주행에 유용한 특징들을 자동으로 학습했음을 의미한다.

End-to-End 아키텍처의 핵심 장점은 전체 시스템을 하나의 목적 함수로 최적화할 수 있다는 것이다. 주행 성능을 직접 측정하여 역전파 알고리즘으로 네트워크의 모든 파라미터를 업데이트한다. 이는 부분 최적화의 문제를 해결하며, 시스템의 모든 구성 요소가 최종 목표를 향해 협력하도록 만든다. 또한 모듈 간 인터페이스가 없으므로 정보 손실이 최소화된다. 센서 데이터의 모든 정보가 네트워크를 통해 흐르며, 각 결정에 필요한 정보를 직접 추출할 수 있다.

두 번째 장점은 데이터 기반 학습을 통한 암묵적 지식의 획득이다. 인간이 명시적으로 프로그래밍하기 어려운 복잡한 패턴과 규칙을 데이터로부터 자동으로 학습한다. 예를 들어, 다른 운전자의 의도를 미묘한 단서로부터 추론하거나, 복잡한 교차로에서의 우선순위를 암묵적으로 학습할 수 있다. 이는 수백만 마일의 주행 데이터에서 추출된 집단 지성을 활용하는 것이다.

세 번째 장점은 확장성이다. 더 많은 데이터와 더 큰 모델, 더 강력한 컴퓨팅 자원을 투입하면 성능이 지속적으로 향상되는 스케일링 법칙을 따른다. 이는 자연어 처리의 GPT 시리즈나 컴퓨터 비전의 Vision Transformer에서 입증되었으며, 자율주행에서도 유사한 현상이 관찰되고 있다. Tesla는 지속적으로 플릿 데이터를 수집하고 모델 크기를 증가시키며 성능을 개선하고 있으며, 이는 모듈식 시스템에서는 달성하기 어려운 지속적 진화이다.

물론 End-to-End 아키텍처에도 도전 과제가 있다. 가장 큰 문제는 해석 가능성의 부족이다. 네트워크가 왜 특정 결정을 내렸는지 이해하기 어렵다. 수백만 개의 파라미터로 구성된 블랙박스 시스템은 디버깅이 어렵고, 안전성을 검증하기 힘들다. 규제 기관과 보험 회사는 설명 가능한 시스템을 선호하므로, 이는 상용화의 장벽이 될 수 있다. 두 번째 문제는 대규모 고품질 데이터가 필요하다는 것이다. 모듈식 시스템은 각 모듈을 개별적으로 레이블된 데이터로 학습시킬 수 있지만, End-to-End 시스템은 입출력 쌍 전체가 필요하다. 드문 상황의 데이터를 충분히 수집하는 것은 어렵고 비용이 많이 든다.

현재의 추세는 순수한 End-to-End 시스템과 모듈식 시스템의 중간 지점을 탐색하는 것이다. 하이브리드 아키텍처는 End-to-End 학습의 장점을 취하면서도 일부 해석 가능성과 안전성 보장을 유지한다. 예를 들어, 인지와 예측은 End-to-End로 학습하되, 계획은 명시적인 안전 제약을 포함하는 최적화 문제로 정식화할 수 있다. 또는 전체 시스템을 End-to-End로 학습하되, 중간 표현을 명시적으로 감독하여 해석 가능성을 높일 수 있다. Tesla의 FSD는 이러한 하이브리드 접근의 대표적인 예이다. 시스템은 End-to-End 신경망을 핵심으로 하지만, 중간에 명시적인 3D 공간 표현과 객체 리스트를 생성하며, 최종 계획 단계에서는 물리적 제약과 안전 규칙을 명시적으로 확인한다.

향후 몇 년간 End-to-End 아키텍처는 더욱 정교해지고 널리 채택될 것으로 예상된다. Foundation Models의 발전과 함께, 대규모 사전 학습과 fine-tuning을 통해 데이터 효율성이 개선될 것이다. 설명 가능 AI 기법의 발전으로 블랙박스의 내부를 들여다보고 신뢰성을 높일 수 있을 것이다. 그리고 시뮬레이션 기술의 발전으로 드문 상황의 데이터를 합성하여 학습에 활용할 수 있을 것이다. 모듈식에서 End-to-End로의 패러다임 전환은 단순한 기술적 변화가 아니라, 자율주행 시스템을 설계하고 개발하는 근본적인 철학의 변화를 의미한다.

=== 모듈식 vs End-to-End 아키텍처 비교

아래 표는 전통적인 모듈식 아키텍처와 End-to-End 학습 아키텍처의 구조, 장단점, 적용 사례를 상세히 비교한 것이다.

#figure(
  table(
    columns: 3,
    stroke: 0.5pt + rgb("#cbd5e1"),
    fill: (x, y) => {
      if y == 0 { rgb("#1e40af") }
      else if calc.odd(y) { rgb("#f8fafc") }
      else { white }
    },
    align: (x, y) => {
      if x == 0 { left }
      else { left }
    },
    inset: 10pt,

    // Header
    table.cell(fill: rgb("#1e40af"))[*구분*],
    table.cell(fill: rgb("#1e40af"))[*모듈식 아키텍처*],
    table.cell(fill: rgb("#1e40af"))[*End-to-End 아키텍처*],

    // Architecture
    [*시스템 구조*],
    [센서 → 인지 → 예측 → 계획 → 제어\
    \
    각 모듈 독립적 개발\
    명시적 인터페이스로 연결],
    [센서 → 통합 신경망 → 제어\
    \
    전체를 하나의 네트워크로 학습\
    암묵적 중간 표현],

    // Perception
    [*인지 방식*],
    [명시적 객체 검출\
    • Bounding box\
    • 객체 분류 (차량, 보행자)\
    • 차선 검출\
    • 신호등 인식],
    [암묵적 특징 학습\
    • BEV 공간 표현\
    • Occupancy prediction\
    • Query-based detection\
    • 특징 융합],

    // Intermediate Representation
    [*중간 표현*],
    [구조화된 데이터\
    • 객체 리스트 (위치, 속도, 클래스)\
    • 차선 다항식\
    • 교통 신호 상태\
    → 저차원 압축, 정보 손실],
    [학습된 표현\
    • 고차원 특징 벡터\
    • BEV feature map\
    • Latent space encoding\
    → 원시 데이터 정보 보존],

    // Optimization
    [*최적화 방식*],
    [모듈별 독립 최적화\
    • 인지: 검출 정확도\
    • 예측: 궤적 오차\
    • 계획: 경로 최적성\
    → 부분 최적화 (sub-optimal)],
    [전체 통합 최적화\
    • 단일 손실 함수\
    • 주행 성능 직접 최적화\
    • End-to-end backpropagation\
    → 전역 최적화 가능],

    // Error Handling
    [*오류 전파*],
    [연쇄적 오류 누적\
    인지 오류 → 예측 오류 → 계획 오류\
    → 증폭 효과 발생],
    [통합 처리\
    오류가 전체 네트워크에 분산\
    → 개별 모듈 실패에 강건],

    // Interpretability
    [*해석 가능성*],
    [높음\
    • 각 모듈 출력 검사 가능\
    • 의사결정 과정 추적\
    • 디버깅 용이\
    • 규제 친화적],
    [낮음\
    • 블랙박스 특성\
    • 결정 이유 불명확\
    • 디버깅 어려움\
    • 설명 필요 시 추가 기법 요구],

    // Data Requirements
    [*데이터 요구사항*],
    [모듈별 레이블\
    • 객체 경계 상자\
    • 차선 주석\
    • 신호등 레이블\
    → 레이블링 비용 높음],
    [입출력 쌍\
    • 센서 데이터\
    • 제어 명령 (조향, 가속)\
    → 자동 수집 가능\
    → 대규모 데이터 필요],

    // Scalability
    [*확장성*],
    [제한적\
    • 새 센서/객체 추가 시\
      다수 모듈 수정 필요\
    • 인터페이스 의존성\
    • 성능 천장 존재],
    [우수함\
    • 데이터/모델 크기 증가 시\
      지속적 성능 향상\
    • Scaling law 적용\
    • 새 기능 학습 용이],

    // Development
    [*개발 방식*],
    [• 병렬 팀 작업 가능\
    • 모듈별 전문성 활용\
    • 점진적 개선\
    • 기존 기술 재활용],
    [• 통합 개발 팀 필요\
    • 대규모 GPU 인프라\
    • 전체 재학습 빈번\
    • 지속적 데이터 수집],

    // Safety Verification
    [*안전성 검증*],
    [상대적 용이\
    • 각 모듈 개별 검증\
    • 규칙 기반 안전 보장\
    • 형식 검증 가능\
    • 최악의 경우 분석],
    [어려움\
    • 통계적 검증만 가능\
    • Edge case 예측 곤란\
    • 대규모 테스트 필요\
    • 시뮬레이션 의존],

    // Representative Systems
    [*대표 시스템*],
    [• Waymo Driver\
    • Apollo (Baidu)\
    • 대부분의 전통 OEM\
    • DARPA Challenge 우승작들],
    [• Tesla FSD v12+\
    • Wayve (영국)\
    • NVIDIA DRIVE\
    • 최신 연구 시스템들],

    // Current Trend
    [*현재 추세*],
    [점진적 통합\
    일부 모듈을 End-to-End로\
    대체하는 하이브리드 접근],
    [확대 적용\
    Foundation Models 활용\
    더 많은 기업이 채택 중],
  ),
  caption: [모듈식 아키텍처와 End-to-End 아키텍처의 종합 비교. 두 접근은 각각 안전성/해석성과 성능/확장성이라는 서로 다른 가치를 우선시한다.]
)

이 비교에서 알 수 있듯이, 두 아키텍처는 근본적으로 다른 설계 철학을 반영한다. 모듈식 아키텍처는 엔지니어링의 전통적 원칙인 분할 정복, 명확한 인터페이스, 검증 가능성을 중시한다. 이는 안전이 최우선인 산업에서 자연스러운 선택이며, 개발 프로세스가 체계적이고 이해하기 쉽다. 반면 End-to-End 아키텍처는 데이터 중심 AI의 철학을 따른다. 명시적인 규칙 대신 패턴 학습, 수작업 엔지니어링 대신 자동 최적화, 부분 설계 대신 전체 최적화를 지향한다. 이는 더 높은 성능 잠재력을 가지지만, 이해와 검증이 어렵다는 트레이드오프를 수반한다.

실제로 대부분의 현대 자율주행 시스템은 순수한 모듈식도, 순수한 End-to-End도 아닌 하이브리드 접근을 취한다. 예를 들어, 인지와 예측 단계는 End-to-End 신경망으로 통합하되, 계획 단계는 명시적인 안전 제약을 포함하는 최적화 문제로 유지할 수 있다. 또는 전체를 End-to-End로 학습하되, 중간에 해석 가능한 표현(BEV 맵, 객체 리스트)을 명시적으로 생성하도록 보조 손실을 추가할 수 있다. 이러한 하이브리드 접근은 양쪽의 장점을 취하려는 시도이며, 향후 수년간 주류 접근 방식이 될 것으로 예상된다.

=== 아키텍처 플로우 다이어그램

아래 다이어그램은 모듈식 아키텍처와 End-to-End 아키텍처의 정보 흐름을 시각적으로 비교한 것이다.

#figure(
  block(
    width: 100%,
    fill: rgb("#f8fafc"),
    inset: 20pt,
    radius: 5pt,
  )[
    #align(center)[
      #text(size: 14pt, weight: "bold", fill: rgb("#1e40af"))[모듈식 아키텍처]
    ]

    #v(1em)

    #grid(
      columns: (1fr, auto, 1fr, auto, 1fr, auto, 1fr, auto, 1fr),
      column-gutter: 8pt,
      row-gutter: 12pt,
      align: center + horizon,

      // Row 1
      block(fill: rgb("#93c5fd"), inset: 10pt, radius: 3pt, width: 100%)[
        *센서*\
        #text(size: 9pt)[카메라\
        LiDAR\
        레이더]
      ],
      text(size: 20pt)[→],
      block(fill: rgb("#bfdbfe"), inset: 10pt, radius: 3pt, width: 100%)[
        *인지*\
        #text(size: 9pt)[객체 검출\
        차선 인식\
        신호등 인식]
      ],
      text(size: 20pt)[→],
      block(fill: rgb("#dbeafe"), inset: 10pt, radius: 3pt, width: 100%)[
        *예측*\
        #text(size: 9pt)[궤적 예측\
        의도 추론\
        위험 평가]
      ],
      text(size: 20pt)[→],
      block(fill: rgb("#eff6ff"), inset: 10pt, radius: 3pt, width: 100%)[
        *계획*\
        #text(size: 9pt)[경로 생성\
        기동 결정\
        최적화]
      ],
      text(size: 20pt)[→],
      block(fill: rgb("#f0f9ff"), inset: 10pt, radius: 3pt, width: 100%)[
        *제어*\
        #text(size: 9pt)[조향\
        가속\
        제동]
      ],
    )

    #v(0.5em)
    #align(center)[
      #text(size: 9pt, fill: rgb("#64748b"))[
        ⚠ 각 모듈 간 정보 손실 발생 | 독립적 최적화 | 오류 누적
      ]
    ]

    #v(2em)

    #align(center)[
      #text(size: 14pt, weight: "bold", fill: rgb("#059669"))[End-to-End 아키텍처]
    ]

    #v(1em)

    #grid(
      columns: (1fr, auto, 1fr, auto, 1fr),
      column-gutter: 12pt,
      row-gutter: 12pt,
      align: center + horizon,

      // Row 2
      block(fill: rgb("#6ee7b7"), inset: 10pt, radius: 3pt, width: 100%)[
        *센서*\
        #text(size: 9pt)[카메라\
        (멀티뷰)\
        레이더]
      ],
      text(size: 20pt)[→],
      block(fill: rgb("#a7f3d0"), inset: 15pt, radius: 3pt, width: 100%)[
        *통합 신경망*\
        #text(size: 9pt)[Vision Transformer\
        BEV Encoder\
        Temporal Fusion\
        Planning Network\
        \
        (암묵적 인지/예측/계획)]
      ],
      text(size: 20pt)[→],
      block(fill: rgb("#d1fae5"), inset: 10pt, radius: 3pt, width: 100%)[
        *제어*\
        #text(size: 9pt)[조향\
        가속\
        제동]
      ],
    )

    #v(0.5em)
    #align(center)[
      #text(size: 9pt, fill: rgb("#059669"))[
        ✓ 최소 정보 손실 | 전역 최적화 | 오류 분산
      ]
    ]
  ],
  caption: [모듈식 아키텍처와 End-to-End 아키텍처의 정보 흐름 비교. 모듈식은 명시적 단계로 구분되며 각 인터페이스에서 정보가 압축되는 반면, End-to-End는 단일 네트워크가 전체 매핑을 학습하여 정보를 보존한다.]
)

이 다이어그램에서 볼 수 있듯이, 모듈식 아키텍처는 5단계의 명확한 처리 파이프라인을 가지며 각 단계가 구조화된 데이터를 주고받는다. 반면 End-to-End 아키텍처는 센서 데이터가 단일한 통합 신경망을 거쳐 직접 제어 명령으로 변환된다. 중간 과정이 네트워크 내부의 암묵적 표현으로 존재하지만, 명시적인 인터페이스는 없다. 이는 정보 흐름의 효율성을 높이고 전체 시스템의 최적화를 가능하게 하지만, 해석과 디버깅은 더 어렵게 만든다.

== 학습 방법론의 진화

End-to-End 자율주행 시스템을 학습시키는 방법론은 크게 세 가지로 분류할 수 있다. 모방 학습, 강화학습, 그리고 인간 피드백 기반 강화학습이다. 각 방법론은 고유한 장단점을 가지고 있으며, 실제 시스템에서는 이들을 조합하여 사용하는 경우가 많다.

모방 학습은 가장 직관적이고 널리 사용되는 방법이다. 기본 아이디어는 전문가인 인간 운전자의 행동을 관찰하고 이를 모방하는 정책을 학습하는 것이다. 수학적으로는 감독 학습 문제로 정식화되며, 입력은 센서 데이터, 출력은 조향각과 가속 페달 값이다. 학습 데이터는 인간 운전자가 실제로 주행한 로그에서 수집되며, 신경망은 주어진 센서 입력에서 인간이 취한 행동을 예측하도록 학습된다.

모방 학습의 가장 단순한 형태는 행동 복제이다. 데이터셋의 각 샘플에 대해 신경망의 예측과 인간의 실제 행동 사이의 오차를 계산하고, 이를 최소화하도록 네트워크 파라미터를 업데이트한다. 이는 일반적인 감독 학습과 동일하며, 구현이 간단하고 학습이 안정적이다. 대규모 데이터셋이 있다면 상당히 좋은 성능을 달성할 수 있다. Waymo는 수천만 마일의 주행 데이터를 수집했으며, 이를 바탕으로 모방 학습으로 강력한 초기 정책을 획득했다.

그러나 행동 복제는 분포 이동 문제로 인해 한계를 가진다. 학습 데이터는 인간 운전자가 만든 상태 분포를 따른다. 즉, 인간이 주로 경험하는 상황들로 구성되어 있다. 그러나 학습된 정책이 실제로 주행할 때는 작은 오류로 인해 학습 데이터와 다른 상태에 놓일 수 있다. 예를 들어, 정책이 도로의 중앙에서 약간 벗어나면, 이는 인간 운전자가 거의 경험하지 않는 상태이므로 학습 데이터에 충분한 예시가 없다. 정책은 이 상황에서 어떻게 행동해야 할지 모르고 더 큰 오류를 만들 수 있으며, 이는 누적되어 궁극적으로 실패로 이어진다.

이 문제를 해결하기 위해 데이터셋 집계 알고리즘이 제안되었다. 핵심 아이디어는 학습된 정책이 만드는 상태 분포에서 추가 데이터를 수집하는 것이다. 구체적인 절차는 다음과 같다. 먼저 초기 정책을 인간 데이터로 학습한다. 이 정책을 실제 환경이나 시뮬레이션에서 실행하여 새로운 상태들을 경험하게 한다. 이 새로운 상태들에서 인간 전문가가 어떻게 행동했을지 레이블을 달거나, 실제로 인간이 제어권을 가져와 행동을 기록한다. 이 새로운 데이터를 기존 데이터셋에 추가하고, 확장된 데이터셋으로 정책을 재학습한다. 이 과정을 여러 번 반복하면, 정책은 자신이 실제로 경험할 상태 분포에서 학습하게 되므로 분포 이동 문제가 완화된다.

데이터셋 집계는 이론적으로 우수하지만 실용적인 어려움이 있다. 매 반복마다 인간 전문가가 새로운 상태에 레이블을 달아야 하므로 비용이 많이 든다. 또한 안전성 문제가 있다. 학습 중인 정책은 아직 완벽하지 않으므로, 실제 차량에서 실행하면 위험할 수 있다. 따라서 시뮬레이션 환경에서 데이터셋 집계를 수행하는 것이 일반적이지만, 시뮬레이션과 실제의 차이로 인해 전이 문제가 발생할 수 있다.

모방 학습의 또 다른 발전은 조건부 모방 학습이다. 기본 모방 학습은 동일한 상황에서도 인간이 다른 행동을 취할 수 있다는 점을 고려하지 못한다. 예를 들어, 교차로에서 직진, 좌회전, 우회전 모두 가능하며, 어떤 행동을 택할지는 목적지에 따라 달라진다. 조건부 모방 학습은 고수준 명령을 추가 입력으로 받아 이를 해결한다. 네트워크는 센서 데이터와 함께 네비게이션 명령을 받아, 해당 명령에 맞는 행동을 출력한다. 이는 정책의 다중 모드성을 명시적으로 모델링하며, 동일한 네트워크로 다양한 주행 기동을 수행할 수 있게 한다.

강화학습은 모방 학습의 대안적 접근이다. 인간의 시연을 직접 모방하는 대신, 보상 신호를 통해 좋은 행동과 나쁜 행동을 구별하고 시행착오를 통해 학습한다. 에이전트는 환경과 상호작용하며, 각 행동에 대한 보상을 받는다. 목표는 장기적인 누적 보상을 최대화하는 정책을 찾는 것이다. 강화학습은 이론적으로 최적 정책을 학습할 수 있으며, 인간보다 더 나은 성능을 달성할 가능성도 있다.

자율주행에서 강화학습을 적용하는 핵심 과제는 보상 함수를 설계하는 것이다. 보상 함수는 시스템이 무엇을 최적화해야 하는지 정의한다. 자율주행의 목표는 안전하고 효율적이며 편안한 주행을 달성하는 것이지만, 이를 수학적 함수로 정확히 표현하기는 어렵다. 보상 함수는 여러 요소의 가중합으로 구성되는 경우가 많다. 안전성을 위해서는 충돌 시 큰 음의 보상을 주고, 다른 차량이나 장애물과의 거리가 가까울수록 음의 보상을 준다. 효율성을 위해서는 목표 속도를 유지하거나 목적지에 빠르게 도달할수록 양의 보상을 준다. 승차감을 위해서는 급격한 조향이나 가감속에 음의 보상을 준다. 또한 교통 규칙을 준수하도록 차선 이탈, 신호 위반 등에 벌점을 부여한다.

보상 함수 설계의 어려움은 이들 요소 간의 상충 관계에 있다. 예를 들어, 안전을 극대화하려면 매우 느리게 주행하거나 아예 멈춰야 하지만, 이는 효율성과 배치된다. 승차감을 위해 부드럽게 주행하면 긴급 상황에서 충돌을 피하지 못할 수 있다. 각 요소의 가중치를 어떻게 설정할지는 주관적이며, 잘못된 가중치는 바람직하지 않은 행동을 유도할 수 있다. 보상 해킹 문제도 발생할 수 있다. 에이전트가 보상 함수의 허점을 찾아 의도와 다른 방식으로 보상을 최대화하는 것이다.

강화학습의 또 다른 과제는 샘플 효율성이다. 에이전트는 많은 시행착오를 통해 학습하며, 좋은 정책을 얻기 위해 수백만 번의 상호작용이 필요할 수 있다. 시뮬레이션에서는 빠르게 많은 샘플을 생성할 수 있지만, 실제 차량에서는 불가능하다. 또한 안전성 문제로 인해 실제 환경에서 무작위 탐험을 하기 어렵다. 학습 초기의 정책은 성능이 좋지 않으므로, 실제로 주행하면 사고를 일으킬 위험이 크다.

이러한 문제들을 완화하기 위해 여러 전략이 사용된다. 첫째, 시뮬레이션에서 먼저 학습하고 실제 환경으로 전이한다. CARLA, LGSVL과 같은 고품질 시뮬레이터에서 수백만 번의 주행을 시뮬레이션하여 초기 정책을 학습한 후, 실제 환경에서 fine-tuning한다. 시뮬레이션과 실제의 차이를 줄이기 위해 도메인 랜덤화 기법을 사용한다. 시뮬레이션의 시각적 외관, 물리 파라미터, 센서 노이즈 등을 무작위로 변화시켜 다양성을 늘리면, 학습된 정책이 실제 환경에 더 잘 전이된다.

둘째, 모델 기반 강화학습을 사용하여 샘플 효율성을 높인다. 환경의 동역학 모델을 학습하고, 이 모델 내에서 계획하거나 추가 학습을 수행한다. 실제 환경과의 상호작용으로 모델을 학습하고, 학습된 모델로 시뮬레이션된 경험을 생성하여 정책을 개선한다. 이는 제한된 실제 데이터를 효율적으로 활용할 수 있게 한다.

셋째, 오프라인 강화학습을 활용한다. 이는 온라인 상호작용 없이 과거에 수집된 데이터셋만으로 정책을 학습하는 방법이다. 인간이나 다른 정책이 수집한 주행 로그를 사용하여 강화학습 알고리즘을 적용한다. 이는 안전하며, 기존 데이터를 재활용할 수 있다는 장점이 있다. 그러나 데이터셋이 커버하지 않는 상태-행동 쌍에 대한 학습이 어렵고, 분포 밖 일반화 문제가 발생할 수 있다.

최근에는 모방 학습과 강화학습을 결합하는 방법이 주목받고 있다. 모방 학습으로 인간 수준의 초기 정책을 빠르게 획득한 후, 강화학습으로 이를 더욱 개선하는 것이다. 이는 두 방법의 장점을 취하며, 인간을 뛰어넘는 성능을 달성할 가능성을 제공한다.

인간 피드백 기반 강화학습은 보상 함수 설계의 어려움을 우회하는 방법이다. 명시적인 보상 함수를 엔지니어가 작성하는 대신, 인간의 선호도로부터 보상 모델을 학습한다. 절차는 두 단계로 구성된다. 첫 번째 단계에서는 여러 주행 궤적 쌍을 인간에게 보여주고, 어느 쪽이 더 나은지 평가하게 한다. 예를 들어, 두 개의 교차로 통과 영상을 보여주고 어느 쪽이 더 안전하고 부드러운지 선택하게 한다. 이러한 비교 데이터를 수집하여 보상 모델을 학습한다. 보상 모델은 주행 궤적을 입력으로 받아 점수를 출력하는 신경망이며, 인간이 선호하는 궤적에 더 높은 점수를 부여하도록 학습된다.

두 번째 단계에서는 학습된 보상 모델을 사용하여 강화학습을 수행한다. 에이전트는 환경과 상호작용하며, 각 행동에 대해 보상 모델로부터 보상을 받는다. 이 보상을 최대화하도록 정책을 학습한다. 학습이 진행됨에 따라 새로운 정책 행동에 대한 인간 피드백을 추가로 수집하여 보상 모델을 개선할 수 있다.

인간 피드백 기반 강화학습의 장점은 복잡하고 주관적인 목표를 다룰 수 있다는 것이다. 승차감, 자연스러움, 공손함 같은 개념은 수식으로 정의하기 어렵지만, 인간은 직관적으로 평가할 수 있다. 또한 인간의 암묵적 지식을 시스템에 전달할 수 있다. 교통 예절, 문화적 차이 등 명시적으로 표현되지 않은 규범들을 학습할 수 있다.

그러나 인간 피드백 수집에는 비용이 든다. 수천 또는 수만 쌍의 비교가 필요하며, 각각에 대해 인간이 신중하게 평가해야 한다. 또한 인간 평가자 간의 불일치, 평가자의 피로와 실수, 그리고 평가 기준의 모호함 등이 노이즈를 만든다. 능동 학습 기법을 사용하여 효율성을 높일 수 있다. 시스템은 가장 정보가 많은 비교 쌍, 즉 모델의 불확실성이 큰 경우를 우선적으로 인간에게 질의한다.

자율주행 분야에서 인간 피드백 기반 강화학습의 적용은 아직 초기 단계이지만, 유망한 결과들이 보고되고 있다. Wayve는 주행 스타일을 개선하기 위해 이 방법을 사용했다. 기술적으로 안전하지만 승객에게 불편함을 주는 행동들을 인간 피드백을 통해 식별하고 개선했다. 예를 들어, 너무 급격한 브레이크, 불필요하게 넓은 회전, 소심한 합류 등을 인간 평가자가 지적하면, 시스템은 이를 학습하여 더 자연스러운 주행 스타일을 획득했다.

게임 이론 기반 강화학습은 다중 에이전트 환경에서 상호작용을 고려하는 방법이다. 자율주행 차량은 혼자 주행하는 것이 아니라 다른 차량, 보행자, 자전거 등과 도로를 공유한다. 이들 각각은 자신의 목표를 가지고 행동하며, 서로 영향을 주고받는다. 단순히 다른 에이전트를 환경의 일부로 취급하면, 그들의 적응적 행동을 놓칠 수 있다.

게임 이론은 전략적 상호작용을 분석하는 수학적 틀을 제공한다. 내쉬 균형 개념은 각 에이전트가 다른 에이전트의 전략이 주어졌을 때 자신의 전략을 최적화한 상태를 나타낸다. 자율주행에서는 교차로 통과, 차선 변경, 합류 등의 상황이 게임으로 모델링될 수 있다. 각 차량은 안전하게 목표에 도달하려 하며, 다른 차량의 행동을 예측하고 이에 최적으로 대응한다.

레벨-k 추론은 인간의 제한된 합리성을 모델링하는 접근이다. 레벨-0 에이전트는 단순한 휴리스틱을 따르며, 다른 에이전트를 고려하지 않는다. 레벨-1 에이전트는 다른 에이전트가 레벨-0이라고 가정하고 최적 대응을 한다. 레벨-k 에이전트는 다른 에이전트가 레벨-(k-1)이라고 가정한다. 실제 인간 운전자는 보통 레벨-1이나 레벨-2 정도의 추론을 하는 것으로 알려져 있다. 자율주행 시스템이 이를 모델링하면, 인간 운전자와 더 자연스럽게 상호작용할 수 있다.

역 강화학습은 관찰된 행동으로부터 보상 함수를 추론한다. 다른 차량의 주행 로그를 보고, 그 차량이 암묵적으로 최적화하고 있는 목표가 무엇인지 역으로 계산한다. 예를 들어, 한 차량이 일관되게 빠른 속도를 유지하면서도 안전 거리를 확보한다면, 그 차량은 속도와 안전의 균형을 중시하는 보상 함수를 가진 것으로 추정할 수 있다. 이렇게 추론된 보상 함수를 사용하여 그 차량의 미래 행동을 예측할 수 있다.

다중 에이전트 강화학습 알고리즘들도 개발되고 있다. QMIX는 여러 에이전트의 Q-함수를 결합하여 중앙집중식으로 학습하되, 실행 시에는 각 에이전트가 독립적으로 행동한다. MADDPG는 중앙집중식 비평가와 분산 행위자를 사용하여 협력과 경쟁이 혼재된 환경에서 학습한다. 이러한 알고리즘들은 시뮬레이션 환경에서 흥미로운 행동 패턴을 보여주었으며, 실제 자율주행에 적용하는 연구가 진행 중이다.

학습 방법론의 선택은 데이터 가용성, 안전 요구사항, 성능 목표에 따라 달라진다. 대규모 인간 주행 데이터가 있다면 모방 학습이 빠르고 안전한 시작점을 제공한다. 더 나은 성능을 추구한다면 강화학습으로 정책을 fine-tuning할 수 있다. 주관적 품질을 중시한다면 인간 피드백을 활용한다. 복잡한 상호작용이 중요하다면 게임 이론적 접근을 고려한다. 실제로는 이들을 조합한 하이브리드 방법이 가장 효과적인 경우가 많으며, 자율주행 기업들은 자신의 상황에 맞는 최적의 조합을 찾기 위해 지속적으로 실험하고 있다.

#pagebreak()

= End-to-End 자율주행 기술 발전 타임라인

자율주행 기술은 수십 년에 걸쳐 발전해왔다. 아래 타임라인은 End-to-End 학습과 관련된 주요 이정표를 시각화한 것이다.

#figure(
  block(
    width: 100%,
    fill: rgb("#f8fafc"),
    inset: 20pt,
    radius: 5pt,
  )[
    // Timeline entries
    #let timeline-entry(year, color, title, desc) = {
      grid(
        columns: (80pt, auto),
        column-gutter: 15pt,
        align: (right, left),

        // Year box
        block(
          fill: color,
          inset: 8pt,
          radius: 3pt,
          width: 100%,
        )[
          #align(center)[
            #text(size: 13pt, weight: "bold", fill: white)[#year]
          ]
        ],

        // Content
        [
          #text(size: 11pt, weight: "bold")[#title]\
          #text(size: 9pt, fill: rgb("#64748b"))[#desc]
          #v(1.5em)
        ],
      )
    }

    timeline-entry("1989", rgb(99, 102, 241), "ALVINN 시스템",
      "Carnegie Mellon의 Pomerleau, 최초의 End-to-End 주행 시스템 개발. 단순 신경망으로 도로 주행 성공.")

    timeline-entry("2007", rgb(139, 92, 246), "DARPA Urban Challenge",
      "모듈식 아키텍처의 정점. Carnegie Mellon Boss 시스템 우승. 10년간 표준 접근법으로 자리잡음.")

    timeline-entry("2012", rgb(168, 85, 247), "Deep Learning 혁명",
      "AlexNet이 ImageNet에서 압도적 성능. 딥러닝이 컴퓨터 비전의 주류로 부상. 자율주행 인지 시스템에 CNN 적용 시작.")

    timeline-entry("2016", rgb(192, 132, 252), "NVIDIA E2E 주행",
      "단 9층 CNN으로 카메라→조향각 직접 예측 성공. End-to-End 학습의 실현 가능성 재입증. 72시간 데이터로 학습.")

    timeline-entry("2017-2019", rgb(216, 180, 254), "Transformer 등장",
      "2017: Attention is All You Need 발표. 2019: BERT, GPT-2 성공. 자연어 처리에서 Foundation Models 개념 확립.")

    timeline-entry("2020", rgb(233, 213, 255), "Vision Transformer",
      "Google Research, ViT 발표. Transformer가 CNN 능가. 자율주행 인지 시스템의 새로운 백본으로 부상.")

    timeline-entry("2021", rgb(252, 165, 165), "Tesla Vision-Only",
      "Tesla, FSD Beta에서 LiDAR 제거, 순수 카메라 기반 시스템 채택. 대규모 플릿 데이터로 지속적 학습.")

    timeline-entry("2022", rgb(251, 146, 60), "FSD Beta v11-v12",
      "Tesla, End-to-End 신경망을 planning 단계까지 확장. Waymo, Level 4 Robotaxi 서비스 확대 (SF, LA).")

    timeline-entry("2023", rgb(253, 186, 116), "생성 AI 통합",
      "World Models (GAIA-1, DriveGAN) 등장. Diffusion Models로 주행 시나리오 생성. Occupancy Networks 주류화.")

    timeline-entry("2024", rgb(254, 215, 170), "Foundation Models 적용",
      "대규모 사전학습 모델이 자율주행 표준으로. BEV Transformer, Query-based 아키텍처 발전. 중국 시장 급부상.")

    timeline-entry("2025", rgb(34, 197, 94), "FSD v13-v14",
      "Occupancy Network 전면 적용. 도심 성능 대폭 향상. Level 3 규제 승인 논의 본격화. Robotaxi 경쟁 심화.")

    timeline-entry("2026-2030", rgb(22, 163, 74), "상용화 가속",
      "2026-2027: Level 3 확대. 2028: Level 4 제한적 상용화. 2029-2030: Robotaxi 대중화. MaaS 생태계 형성.")
  ],
  caption: [End-to-End 자율주행 기술의 역사적 발전. 1989년 ALVINN의 초기 시도부터 2030년 완전 상용화 전망까지, 기술은 하드웨어 성능, 데이터 규모, 알고리즘 혁신이 맞물려 발전해왔다.]
)

이 타임라인에서 볼 수 있듯이, End-to-End 자율주행은 30년 이상의 긴 역사를 가지고 있다. 초기에는 컴퓨팅 성능과 데이터 부족으로 실용화되지 못했으나, 2010년대 딥러닝 혁명, 2020년대 Transformer와 Foundation Models의 등장으로 비약적으로 발전하고 있다. 특히 2016년 이후 가속화된 발전은 GPU 성능 향상, 대규모 데이터 축적, 알고리즘 혁신이 동시에 이루어진 결과이다. 향후 5년간은 기술적 성숙과 함께 규제 승인, 대중 신뢰 구축, 비즈니스 모델 확립이 핵심 과제가 될 것이다.

#pagebreak()

= Foundation Models와 생성 AI의 융합

== Vision Transformer와 대규모 사전학습

컴퓨터 비전 분야는 오랫동안 합성곱 신경망의 지배를 받아왔다. 합성곱 연산은 이미지의 지역적 패턴을 효과적으로 포착하며, 평행 이동 불변성이라는 유용한 귀납적 편향을 제공한다. AlexNet이 2012년 ImageNet 대회에서 압도적 성능을 보인 이후, ResNet, EfficientNet 등 다양한 합성곱 기반 아키텍처들이 발전해왔다. 그러나 2020년 Google Research에서 발표한 Vision Transformer는 이러한 패러다임에 근본적인 도전을 제기했다.

Vision Transformer는 자연어 처리에서 성공을 거둔 Transformer 아키텍처를 이미지 인식에 적용한 것이다. Transformer의 핵심은 self-attention 메커니즘으로, 입력 시퀀스의 모든 위치 간의 관계를 직접 모델링한다. 이미지에 Transformer를 적용하기 위해, 이미지를 작은 패치들로 나누고 이를 시퀀스로 취급한다. 예를 들어, 224×224 이미지를 16×16 패치로 나누면 196개의 패치가 생성되며, 각 패치는 하나의 토큰으로 간주된다. 각 패치는 선형 변환을 통해 고차원 벡터로 임베딩되고, 위치 정보를 인코딩하는 벡터가 추가된다. 이렇게 만들어진 토큰 시퀀스가 Transformer encoder에 입력된다.

Transformer encoder는 여러 층의 self-attention과 feed-forward 네트워크로 구성된다. Self-attention은 각 토큰이 다른 모든 토큰과 상호작용하도록 하여, 이미지 전체의 문맥을 고려할 수 있게 한다. 이는 합성곱의 제한된 수용 영역과 대조적이다. 합성곱 신경망은 여러 층을 쌓아야 전역적 정보를 통합할 수 있지만, Vision Transformer는 단일 attention 층에서도 이미지의 먼 부분들 간의 관계를 직접 포착한다.

Vision Transformer의 초기 결과는 놀라웠다. 충분히 큰 데이터셋에서 학습하면 최고 성능의 합성곱 신경망과 동등하거나 더 나은 성능을 보였다. 특히 JFT-300M이라는 3억 개 이미지의 대규모 데이터셋에서 사전 학습한 Vision Transformer는 ImageNet에서 당시 최고 성능을 기록했다. 더 흥미로운 점은 모델 크기를 키울수록 성능이 지속적으로 향상되는 스케일링 법칙을 보인다는 것이다. 합성곱 신경망은 일정 크기 이상에서 성능 향상이 정체되는 경향이 있지만, Transformer는 더 많은 파라미터와 데이터를 투입할수록 성능이 계속 개선된다.

자율주행 분야에서 Vision Transformer의 도입은 인지 시스템의 성능을 크게 향상시켰다. 먼저 사전 학습의 이점이 크다. ImageNet이나 더 큰 데이터셋에서 사전 학습된 Vision Transformer는 일반적인 시각 표현을 학습하며, 이를 자율주행 작업에 전이할 수 있다. 적은 양의 자율주행 데이터로도 fine-tuning을 통해 좋은 성능을 달성할 수 있다. 이는 특히 드물게 발생하는 상황에서 유용하다. 예를 들어, 눈 오는 날의 주행 데이터는 제한적이지만, 사전 학습된 모델은 일반적인 눈 패턴을 이미 알고 있으므로 빠르게 적응할 수 있다.

둘째, Vision Transformer의 장거리 의존성 모델링 능력은 복잡한 장면 이해에 도움이 된다. 도심 주행에서는 여러 객체들이 서로 상호작용하며, 먼 곳의 교통 신호나 표지판이 현재 행동에 영향을 줄 수 있다. Vision Transformer는 이미지의 서로 다른 부분들 간의 관계를 효과적으로 포착하여, 이러한 복잡한 문맥을 이해할 수 있다.

셋째, Vision Transformer는 다양한 해상도와 종횡비의 이미지를 다루기 용이하다. 자율주행에서는 여러 카메라가 다른 해상도와 시야각을 가질 수 있으며, Vision Transformer는 패치 단위로 처리하므로 이러한 변화에 유연하게 대응한다.

Tesla는 FSD 시스템에서 Vision Transformer를 적극 활용하고 있다. 다중 카메라 이미지를 처리하는 백본 네트워크로 Transformer를 사용하며, 이를 통해 8개 카메라의 정보를 효과적으로 통합한다. 각 카메라 이미지에서 추출된 특징들이 Transformer의 attention 메커니즘을 통해 상호작용하며, 전체 차량 주변의 일관된 표현을 생성한다.

Vision Transformer의 성공은 대규모 사전 학습의 중요성을 재확인시켰다. Foundation Models라는 개념은 범용적인 대규모 모델을 먼저 학습하고, 이를 다양한 downstream 작업에 적용한다는 아이디어이다. GPT-3, BERT와 같은 언어 모델들이 이 접근의 효과를 입증했으며, 이제 비전과 자율주행에서도 동일한 패러다임이 자리잡고 있다.

CLIP은 Vision과 Language를 결합한 Foundation Model의 대표적 예이다. OpenAI가 개발한 CLIP은 4억 개의 이미지-텍스트 쌍으로 학습되었으며, 이미지와 텍스트를 공동 임베딩 공간에 매핑한다. 학습은 contrastive learning 방식으로 이루어진다. 매칭되는 이미지-텍스트 쌍은 임베딩 공간에서 가깝게, 그렇지 않은 쌍은 멀게 배치되도록 학습한다. 이를 통해 CLIP은 시각적 개념과 언어적 설명 간의 연결을 학습한다.

CLIP의 흥미로운 특성은 zero-shot 일반화 능력이다. 학습 중에 보지 못한 새로운 객체 클래스도 텍스트 설명만 제공하면 인식할 수 있다. 예를 들어, CLIP이 "공사 중인 도로"라는 개념을 명시적으로 학습하지 않았더라도, 이 텍스트 설명과 유사한 이미지를 찾을 수 있다. 자율주행에서는 다양하고 예측 불가능한 상황을 마주치므로, 이러한 zero-shot 능력이 매우 유용하다.

자율주행에서 CLIP을 활용하는 방법은 여러 가지이다. 첫째, 이상 상황 감지에 사용할 수 있다. 정상적인 도로 장면과 다른 이상한 상황을 텍스트 쿼리로 찾을 수 있다. "도로를 막고 있는 쓰레기", "뒤집힌 차량" 같은 드문 상황을 학습 데이터 없이도 인식할 수 있다. 둘째, 사람의 자연어 명령을 이해하는 데 활용할 수 있다. "빨간 건물 앞에 세워주세요"와 같은 지시를 시각 정보와 연결하여 해석한다. 셋째, 플릿 데이터의 자동 레이블링에 사용할 수 있다. 텍스트 쿼리로 특정 유형의 장면을 검색하여, 레이블링 비용을 크게 줄일 수 있다.

최근에는 자율주행 전용 Foundation Models를 개발하려는 시도들이 나타나고 있다. 일반적인 이미지가 아닌 대규모 주행 영상으로 사전 학습하는 것이다. Waymo는 자사의 방대한 주행 데이터를 활용하여 자율주행 특화 사전 학습을 수행하고 있다. 수천만 마일의 주행 로그에서 추출한 수십억 프레임의 이미지와 LiDAR 포인트 클라우드로 대규모 모델을 학습한다. 이러한 모델은 자율주행에 특화된 표현을 학습하며, 일반 이미지에서 사전 학습한 모델보다 더 나은 성능을 보인다.

Self-supervised learning은 레이블 없이 데이터로부터 표현을 학습하는 방법이다. 주행 데이터는 풍부하지만, 모든 프레임에 정확한 레이블을 다는 것은 비용이 많이 든다. Self-supervised learning은 데이터 자체로부터 감독 신호를 만들어낸다. Masked image modeling은 이미지의 일부를 가리고 나머지로부터 가려진 부분을 복원하도록 학습한다. 이는 BERT의 masked language modeling과 유사한 아이디어이다. 모델은 문맥을 이용하여 빠진 정보를 추론하는 능력을 획득하며, 이는 downstream 작업에 유용한 표현을 제공한다.

Contrastive learning은 유사한 샘플들을 가깝게, 다른 샘플들을 멀게 배치하도록 학습한다. 자율주행에서는 시간적으로 가까운 프레임들이 유사한 장면을 담고 있으므로, 이들을 positive pair로 사용할 수 있다. 또한 데이터 증강 기법을 사용하여 동일한 장면의 다른 view를 생성하고, 이들을 유사하게 만들도록 학습할 수 있다.

Temporal consistency는 연속된 프레임 간의 일관성을 활용한다. 차량이 부드럽게 움직이므로, 연속 프레임의 특징 표현도 부드럽게 변해야 한다. 이 제약을 학습 목적함수에 추가하여 시간적으로 안정적인 표현을 학습한다. 또한 광학 흐름이나 차량의 자세 변화 정보를 사용하여, 연속 프레임 간의 대응 관계를 학습할 수 있다.

Multi-modal pre-training은 여러 센서의 데이터를 함께 활용하여 학습한다. 카메라, LiDAR, 레이더, GPS, IMU 등의 센서는 서로 보완적인 정보를 제공한다. 카메라는 시각적 세부사항과 색상 정보를 제공하고, LiDAR는 정확한 3차원 거리 정보를 제공하며, 레이더는 속도 정보와 악천후 강건성을 제공한다. 이들을 통합하여 학습하면 각 센서의 장점을 결합한 강력한 표현을 얻을 수 있다.

BEVFormer는 이러한 다중 모드 사전 학습의 예이다. 여러 카메라 이미지를 Bird's Eye View 공간으로 변환하며, 이 과정에서 공간적 관계와 3차원 구조를 학습한다. 사전 학습 단계에서는 대규모 주행 데이터로 BEV 표현을 학습하고, fine-tuning 단계에서는 특정 작업에 맞게 조정한다. 이는 3D 객체 검출, occupancy 예측, 경로 계획 등 다양한 downstream 작업에서 우수한 성능을 보인다.

Foundation Models의 또 다른 장점은 few-shot learning 능력이다. 새로운 환경이나 새로운 유형의 객체에 빠르게 적응할 수 있다. 예를 들어, 특정 국가에서 사용되는 독특한 교통 표지판이나, 특정 지역의 건축 양식 등을 몇 가지 예시만으로 학습할 수 있다. 이는 자율주행 시스템을 새로운 지역으로 확장할 때 매우 유용하다.

Prompt learning은 Foundation Models를 활용하는 새로운 패러다임이다. 모델의 파라미터를 수정하지 않고, 적절한 입력 프롬프트를 설계하여 원하는 작업을 수행하게 한다. 예를 들어, "이 장면에서 주행 가능한 영역은 어디인가?"라는 텍스트 프롬프트와 함께 이미지를 입력하면, 모델이 해당 영역을 출력한다. 이는 작업별로 별도의 모델을 학습할 필요 없이, 하나의 Foundation Model로 다양한 작업을 수행할 수 있게 한다.

Meta-learning 또는 learning to learn은 여러 작업에서 학습한 경험을 바탕으로, 새로운 작업에 빠르게 적응하는 능력을 학습한다. Foundation Model을 meta-learning 프레임워크로 학습하면, 적은 데이터로도 새로운 시나리오에 빠르게 fine-tuning될 수 있다. 이는 장기적으로 자율주행 시스템의 지속적인 진화와 개선을 가능하게 한다.

Vision Transformer와 Foundation Models의 발전은 자율주행을 데이터 중심, 스케일 중심의 패러다임으로 이끌고 있다. 더 많은 데이터, 더 큰 모델, 더 강력한 컴퓨팅이 성능 향상의 핵심 동력이 되고 있다. 이는 자원이 풍부한 대기업에게 유리하며, 실제로 Tesla, Waymo, Baidu와 같은 기업들이 막대한 투자를 통해 데이터와 컴퓨팅 인프라를 구축하고 있다. 동시에, 오픈소스 Foundation Models의 발전은 작은 기업과 연구자들에게도 최신 기술에 접근할 기회를 제공하고 있다. 향후 몇 년간 이러한 Foundation Models는 더욱 크고 강력해질 것이며, 자율주행의 핵심 기술로 자리잡을 것으로 전망된다.

== World Models: 미래를 예측하는 생성 모델

World Models는 자율주행 시스템이 환경의 미래 상태를 예측하고 시뮬레이션할 수 있게 하는 생성 모델이다. 전통적인 자율주행 시스템은 현재 관측된 정보만을 바탕으로 결정을 내리지만, World Model을 갖춘 시스템은 가능한 행동들의 미래 결과를 상상하고 비교할 수 있다. 이는 인간 운전자가 "만약 지금 추월하면 어떻게 될까?"와 같은 가상 시나리오를 머릿속에서 시뮬레이션하는 것과 유사하다.

World Model의 핵심 아이디어는 환경의 동역학을 학습하는 것이다. 주어진 현재 상태와 행동에 대해, 다음 상태가 어떻게 될지 예측하는 모델을 학습한다. 수학적으로는 조건부 확률 분포로 표현된다. 이 모델이 정확하다면, 실제 환경과 상호작용하지 않고도 머릿속에서 또는 시뮬레이션에서 미래를 탐색할 수 있다. 이는 여러 측면에서 유용하다.

첫째, 더 안전한 의사결정이 가능하다. 위험한 행동을 실제로 취하기 전에 시뮬레이션에서 그 결과를 예측하여, 사고를 예방할 수 있다. 예를 들어, 급차선 변경이 인접 차량과의 충돌로 이어질지 미리 예측할 수 있다. 둘째, 모델 기반 계획이 가능하다. 여러 행동 시퀀스를 시뮬레이션하여 최적의 계획을 찾을 수 있다. 이는 체스에서 여러 수를 미리 읽어보는 것과 유사하다. 셋째, 샘플 효율적인 강화학습이 가능하다. 실제 환경과의 상호작용 없이 World Model 내에서 정책을 학습하고 개선할 수 있다.

Wayve가 개발한 GAIA-1은 자율주행을 위한 대규모 World Model의 선구적인 예이다. GAIA-1은 생성 AI의 최신 기술을 활용하여, 주행 영상의 미래 프레임을 예측한다. 모델은 과거 몇 프레임의 이미지와 차량의 행동 정보를 입력으로 받아, 다음 몇 프레임의 이미지를 생성한다. 학습 데이터는 수천 시간의 실제 주행 영상이며, 모델은 이로부터 도로의 물리적 법칙, 다른 차량의 행동 패턴, 날씨와 조명의 영향 등을 암묵적으로 학습한다.

GAIA-1의 흥미로운 특징은 다양한 조건을 제어할 수 있다는 것이다. 단순히 현재 행동만이 아니라, 고수준의 텍스트 설명도 조건으로 받을 수 있다. 예를 들어, "비 오는 날 좌회전" 또는 "야간 고속도로 주행"과 같은 텍스트를 입력하면, 해당 조건에 맞는 주행 영상을 생성한다. 이는 특정 시나리오의 데이터가 부족할 때 합성 데이터를 생성하는 데 유용하다. 또한 행동의 결과를 시각화하여 시스템의 의사결정을 이해하고 검증하는 데도 도움이 된다.

NVIDIA의 Neural World Model은 또 다른 접근 방식을 제시한다. 고화질 비디오 픽셀 수준에서 예측하는 대신, 저차원의 잠재 공간에서 동역학을 학습한다. 먼저 Encoder 네트워크가 고차원 센서 데이터를 저차원 잠재 벡터로 압축한다. 이 잠재 공간에서 동역학 모델이 미래 잠재 상태를 예측한다. 필요시 Decoder 네트워크가 잠재 벡터를 다시 이미지로 복원한다. 이러한 접근은 계산 효율성을 크게 향상시키며, 잠재 공간이 더 구조화되고 해석 가능한 표현을 제공할 수 있다.

물리 법칙의 통합은 World Model의 신뢰성을 높이는 중요한 요소이다. 순수 데이터 기반 모델은 물리적으로 불가능한 미래를 예측할 수 있다. 예를 들어, 차량이 갑자기 사라지거나, 중력을 무시하고 날아오를 수 있다. 물리 기반 모델링을 통합하면 이러한 비현실적인 예측을 방지할 수 있다. 차량의 동역학 모델, 충돌 물리, 마찰 계수 등을 명시적으로 인코딩하여, 물리 법칙을 만족하는 예측만 생성하도록 제약할 수 있다.

World Model의 학습은 대규모 주행 데이터로부터 이루어진다. 수백만 시간의 주행 로그에서 연속된 프레임과 행동 정보를 추출하여 학습 샘플을 만든다. 학습 목표는 모델이 예측한 미래 프레임과 실제 관측된 프레임 사이의 차이를 최소화하는 것이다. 비디오 예측은 어려운 문제인데, 미래는 본질적으로 불확실하고 다양한 가능성이 존재하기 때문이다. 예를 들어, 앞 차량이 직진할 수도, 좌회전할 수도, 우회전할 수도 있다. 단일 예측만 출력한다면 이러한 불확실성을 표현할 수 없다.

확률적 World Model은 이 문제를 해결한다. 미래의 여러 가능한 시나리오를 확률 분포로 표현한다. Variational Autoencoder나 Diffusion Models와 같은 생성 모델 기법을 사용하여, 조건부 분포로부터 샘플링할 수 있다. 이를 통해 하나의 현재 상태에서 출발하여 여러 가능한 미래 궤적을 생성하고, 각각의 가능성을 평가할 수 있다. 계획 단계에서는 가장 안전하거나 효율적인 궤적을 선택한다.

World Model의 응용은 다양하다. 안전성 검증에서는 위험한 시나리오를 시뮬레이션하여 시스템의 대응을 테스트한다. 실제로 위험한 상황을 만들 필요 없이, World Model 내에서 타이어 펑크, 급제동, 끼어들기 등을 시뮬레이션하고 시스템이 적절히 반응하는지 확인한다. 데이터 증강에서는 드문 상황의 합성 데이터를 생성한다. 눈 오는 날, 안개 낀 날, 복잡한 교차로 등 실제 데이터가 부족한 상황을 World Model로 생성하여 학습 데이터를 풍부하게 만든다.

모델 기반 계획에서는 여러 행동 시퀀스의 미래를 시뮬레이션하여 최적의 행동을 선택한다. 몬테카를로 트리 탐색이나 모델 예측 제어와 결합하여, 장기적 결과를 고려한 지능적인 결정을 내린다. 설명 가능성 향상에서는 시스템이 왜 특정 행동을 선택했는지 시각화한다. "이 행동을 취하면 이런 미래가 펼쳐질 것"이라는 예측을 보여줌으로써, 시스템의 추론 과정을 투명하게 만든다.

World Model 연구는 아직 초기 단계이지만, 빠르게 발전하고 있다. 생성 AI의 발전, 특히 Diffusion Models와 대규모 Transformer의 성공은 World Model의 품질을 크게 향상시켰다. 컴퓨팅 성능의 증가로 실시간 또는 준실시간으로 미래를 시뮬레이션하는 것이 가능해지고 있다. 향후 몇 년 내에 World Model은 자율주행 시스템의 핵심 구성 요소로 자리잡을 것으로 전망된다. 단순히 반응하는 시스템에서 미래를 예측하고 계획하는 진정으로 지능적인 시스템으로의 진화를 가능하게 할 것이다.

== Diffusion Models와 생성 AI의 응용

Diffusion Models는 최근 몇 년간 생성 AI 분야에서 가장 주목받는 기술로 부상했다. 이미지 생성 분야에서 DALL-E, Midjourney, Stable Diffusion과 같은 시스템들이 놀라운 품질의 이미지를 생성하면서 대중적 관심을 받았다. 이러한 기술이 자율주행 분야에도 적용되기 시작하면서, 데이터 증강, 시나리오 생성, 계획 등 다양한 측면에서 혁신을 가져오고 있다.

Diffusion Models의 기본 원리는 점진적인 노이즈 추가와 제거이다. Forward process에서는 깨끗한 데이터에 단계적으로 가우시안 노이즈를 추가하여, 최종적으로 순수한 노이즈로 만든다. Reverse process에서는 이를 역전시켜, 노이즈로부터 점진적으로 노이즈를 제거하여 깨끗한 데이터를 복원한다. 학습은 reverse process를 수행하는 신경망을 훈련시키는 것이며, 이 네트워크는 각 단계에서 추가된 노이즈를 예측하도록 학습된다.

자율주행에서 Diffusion Models의 첫 번째 응용은 궤적 생성이다. 안전한 주행 궤적을 생성하는 것은 복잡한 제약 최적화 문제이다. 충돌 회피, 차선 준수, 속도 제한, 승차감 등 다양한 제약을 동시에 만족해야 한다. 전통적 방법은 샘플링 기반 계획이나 최적화 알고리즘을 사용하지만, 복잡한 환경에서는 계산 비용이 크고 실시간 처리가 어렵다. Diffusion Models는 조건부 생성을 통해 이 문제를 우아하게 해결한다.

궤적 생성을 위한 Diffusion Model은 현재 상태, 목표 지점, 주변 장애물 정보를 조건으로 받아, 이를 만족하는 궤적을 생성한다. 학습 데이터는 인간 운전자나 기존 계획 알고리즘이 생성한 고품질 궤적들이다. 모델은 이러한 궤적의 분포를 학습하며, 추론 시에는 랜덤 노이즈에서 시작하여 점진적으로 정제된 궤적을 생성한다. Diffusion process의 각 단계에서 조건 정보를 주입하여, 생성되는 궤적이 제약을 만족하도록 유도한다.

이 접근의 장점은 다양성과 품질의 균형이다. 하나의 상황에서 여러 가능한 궤적을 생성할 수 있으며, 각각은 제약을 만족하면서도 다른 특성을 가진다. 예를 들어, 장애물을 왼쪽으로 피할지 오른쪽으로 피할지, 보수적으로 느리게 진행할지 적극적으로 빠르게 진행할지 등 다양한 옵션을 제공한다. 최종 선택은 안전성, 효율성, 승차감 등을 종합적으로 평가하여 이루어진다. 또한 Diffusion Models는 부드러운 궤적을 자연스럽게 생성한다. 점진적 정제 과정이 고주파 노이즈를 제거하여, 급격한 조향이나 가감속 없는 부드러운 궤적을 만든다.

두 번째 응용은 시나리오 생성이다. 자율주행 시스템을 철저히 테스트하려면 다양한 교통 상황을 경험해야 한다. 그러나 실제 주행으로는 드문 상황을 충분히 수집하기 어렵다. Diffusion Models는 현실적이면서도 다양한 테스트 시나리오를 합성적으로 생성할 수 있다. 예를 들어, 복잡한 교차로에서 여러 차량이 동시에 진입하는 상황, 보행자가 갑자기 뛰어드는 상황, 공사로 차선이 축소되는 상황 등을 생성한다.

시나리오 생성에서 중요한 것은 현실성과 다양성이다. 생성된 시나리오가 물리 법칙을 위반하거나 비현실적이면 테스트의 의미가 없다. Diffusion Models는 대규모 실제 데이터로부터 학습하므로, 현실적인 교통 패턴과 에이전트 행동을 재현한다. 동시에, 생성 과정의 랜덤성 덕분에 학습 데이터에 없던 새로운 조합과 변형을 만들어낸다. 이는 edge case와 corner case를 발견하는 데 유용하다.

적대적 시나리오 생성은 시스템의 약점을 찾는 데 특화된 접근이다. Diffusion Model을 강화학습과 결합하여, 자율주행 시스템을 실패하게 만드는 시나리오를 적극적으로 탐색한다. 생성 모델은 시나리오를 생성하고, 자율주행 시스템이 이를 처리하며, 결과에 따라 보상 신호가 생성 모델에 피드백된다. 생성 모델은 시스템을 더 어렵게 만드는 시나리오를 생성하도록 학습된다. 이를 통해 발견된 취약점은 시스템 개선에 활용된다.

세 번째 응용은 센서 데이터 합성이다. 고품질 센서 데이터, 특히 정확한 레이블이 달린 데이터는 수집하기 어렵고 비용이 많이 든다. Diffusion Models는 다양한 센서 데이터를 합성할 수 있다. 카메라 이미지 생성은 가장 직접적인 응용이다. 특정 날씨, 조명, 도로 조건에서의 이미지를 생성하여 학습 데이터를 증강한다. 예를 들어, 맑은 날 수집한 데이터를 비 오는 날이나 눈 오는 날로 변환할 수 있다. 이는 style transfer나 domain adaptation 기법과 결합되어 더욱 현실적인 결과를 만든다.

LiDAR 포인트 클라우드 생성도 가능하다. 3차원 장면의 기하학적 구조를 학습하여, 새로운 시점이나 객체 배치에 대한 LiDAR 데이터를 생성한다. 이는 카메라-LiDAR 융합 알고리즘을 학습시킬 때 유용하다. 레이더 신호 합성은 더 어렵지만 연구가 진행 중이다. 레이더는 도플러 효과를 통해 속도 정보를 제공하므로, 이를 정확히 모델링하려면 물리 기반 시뮬레이션이 필요하다.

네 번째 응용은 occupancy prediction이다. Occupancy grid는 주변 공간의 각 셀이 점유되어 있는지, 주행 가능한지를 나타내는 표현이다. 미래 occupancy를 예측하면, 안전한 경로를 계획하는 데 직접 활용할 수 있다. Diffusion Models는 현재 관측과 행동 계획을 조건으로, 미래 시점의 occupancy grid를 생성한다. 이는 명시적으로 객체를 검출하고 추적하지 않아도, 주행 가능한 공간을 예측할 수 있게 한다.

Diffusion Models는 불확실성 정량화에도 유리하다. 여러 번 샘플링하여 생성된 결과들의 분산을 통해, 예측의 불확실성을 추정할 수 있다. 예를 들어, 궤적 생성에서 여러 샘플이 비슷한 경로를 보인다면 높은 확신을, 샘플들이 크게 다르다면 높은 불확실성을 의미한다. 이러한 불확실성 정보는 의사결정에 활용될 수 있다. 불확실성이 높은 상황에서는 더 보수적으로 행동하거나, 추가 정보를 수집하려 시도할 수 있다.

최근 연구들은 Diffusion Models를 자율주행의 전체 파이프라인에 통합하려 시도하고 있다. UniAD는 인지, 예측, 계획을 하나의 Diffusion 기반 프레임워크로 통합한다. 각 단계가 조건부 생성으로 정식화되며, 전체 시스템이 end-to-end로 학습된다. MotionDiffuser는 다중 에이전트의 미래 모션을 공동으로 예측하는 Diffusion Model이다. 여러 차량과 보행자의 궤적을 동시에 생성하며, 이들 간의 상호작용과 사회적 규범을 학습한다.

Diffusion Models의 계산 비용은 여전히 과제이다. 수십 또는 수백 단계의 반복적 정제가 필요하므로, 단일 forward pass에 비해 느리다. 실시간 자율주행에서는 밀리초 단위의 지연도 중요하므로, 이는 실용성의 장벽이 될 수 있다. 그러나 여러 기법들이 이를 완화하고 있다. DDIM과 같은 빠른 샘플링 방법은 단계 수를 10배 이상 줄일 수 있다. Knowledge distillation은 느린 Diffusion Model의 출력을 빠른 단일 단계 모델로 증류한다. 전용 하드웨어 가속과 최적화 기법도 추론 속도를 크게 향상시킨다.

Diffusion Models와 생성 AI는 자율주행을 데이터 생성과 합성의 시대로 이끌고 있다. 실제 데이터 수집만으로는 불가능했던 다양성과 규모를 달성할 수 있다. 드문 상황, 위험한 상황, 비현실적이지만 테스트에 필요한 상황들을 안전하게 생성하고 활용할 수 있다. 이는 자율주행 시스템의 강건성과 안전성을 크게 향상시킬 것이다. 앞으로 생성 AI 기술의 지속적인 발전과 함께, 자율주행에서의 응용도 더욱 확대되고 정교해질 것으로 기대된다.

== Bird's Eye View 표현과 공간 이해

Bird's Eye View는 자율주행에서 다중 카메라 정보를 통합하고 3차원 공간을 이해하는 강력한 표현 방법이다. 전통적으로 각 카메라 이미지는 독립적으로 처리되었으며, 이들의 정보를 통합하는 것은 복잡한 기하학적 변환과 센서 융합 알고리즘을 필요로 했다. BEV 표현은 이를 단일하고 일관된 좌표계로 통합하여, downstream 작업을 크게 단순화한다.

BEV의 핵심 아이디어는 모든 센서 정보를 위에서 내려다본 조감도 관점으로 변환하는 것이다. 자율주행 차량은 주변 360도를 커버하는 여러 카메라를 가지며, 각 카메라는 다른 방향과 시야각을 가진다. BEV 변환은 이러한 다양한 관점의 이미지들을 단일한 2차원 격자로 투영한다. 이 격자의 각 셀은 실제 세계의 특정 위치에 대응하며, 그 위치에서의 특징이나 점유 상태를 나타낸다.

BEV 표현의 장점은 여러 가지이다. 첫째, 공간적 일관성이다. 원근 투영된 카메라 이미지에서는 거리에 따라 객체의 크기가 변하고, 멀리 있는 객체는 작게 보인다. BEV에서는 모든 위치가 동일한 metric 스케일을 가지므로, 거리와 크기가 직접적으로 해석된다. 이는 경로 계획과 제어에 매우 유용하다. 둘째, 다중 카메라 융합이 자연스럽다. 각 카메라의 정보가 공통의 BEV 공간에 투영되므로, 중복되는 영역에서는 정보가 자동으로 융합되고, 가려진 영역은 다른 카메라로 보완된다.

셋째, 시간적 통합이 용이하다. 차량이 움직이면서 관측되는 BEV는 차량의 자세 변화에 따라 변환될 수 있다. 과거 프레임의 BEV를 현재 좌표계로 변환하여 누적하면, 단일 프레임에서는 보이지 않던 영역도 시간에 따라 채워질 수 있다. 이는 부분 관측 문제를 완화한다. 넷째, downstream 작업이 단순해진다. 객체 검출, 차선 인식, 주행 가능 영역 분할 등이 모두 BEV 공간에서 수행되며, 이는 2차원 이미지에서보다 기하학적으로 직관적이다.

BEV 변환을 구현하는 방법은 여러 가지이다. 초기 접근은 명시적인 기하학적 투영이었다. 카메라의 내재 파라미터와 외재 파라미터를 사용하여, 각 이미지 픽셀이 BEV 그리드의 어느 위치에 대응하는지 계산한다. 그러나 이는 깊이 정보를 필요로 하며, 카메라만으로는 정확한 깊이를 알기 어렵다. 따라서 깊이 추정 네트워크를 먼저 학습하고, 이를 바탕으로 투영을 수행한다.

LSS는 이러한 접근을 체계화한 방법이다. 세 단계로 구성된다. Lift 단계에서는 각 이미지 픽셀에 대해 가능한 깊이 범위를 고려하여, 2차원 특징을 3차원 공간으로 확장한다. 각 픽셀이 여러 깊이 가설을 가지며, 깊이 확률 분포와 함께 3차원 특징 볼륨을 만든다. Splat 단계에서는 이 3차원 특징들을 BEV 그리드로 투영한다. 동일한 BEV 셀에 투영되는 여러 3차원 특징들을 합산하거나 평균내어 통합한다. Shoot 단계에서는 생성된 BEV 특징으로 객체 검출이나 분할 등의 작업을 수행한다.

BEVFormer는 Transformer 아키텍처를 활용한 더 발전된 방법이다. 명시적인 기하학적 투영 대신, attention 메커니즘을 통해 이미지 특징과 BEV 특징을 연결한다. BEV grid의 각 위치를 query로 표현하고, 이미지 특징들을 key와 value로 사용한다. Spatial cross-attention은 각 BEV query가 해당 위치와 관련된 이미지 영역에 attention을 주도록 학습된다. 예를 들어, BEV의 특정 위치는 그 위치를 바라보는 카메라들의 픽셀에 높은 attention weight를 가진다.

BEVFormer의 또 다른 혁신은 temporal attention이다. 현재 프레임의 BEV뿐만 아니라, 과거 프레임의 BEV도 활용한다. 차량의 움직임 정보를 사용하여 과거 BEV를 현재 좌표계로 정렬하고, temporal self-attention을 통해 시간적 정보를 통합한다. 이는 가려졌다가 나타나는 객체를 추적하거나, 부분 관측으로부터 완전한 장면을 재구성하는 데 도움이 된다.

BEVDet과 BEVDepth는 깊이 추정을 명시적으로 학습하는 방법이다. 카메라 이미지로부터 각 픽셀의 깊이 분포를 예측하는 네트워크를 학습한다. 감독 신호로는 LiDAR로부터 얻은 ground truth 깊이나, stereo 카메라로부터 계산된 깊이가 사용된다. 정확한 깊이 추정은 BEV 변환의 품질을 크게 향상시킨다. 특히 먼 거리의 객체나 작은 객체를 정확히 위치시키는 데 중요하다.

BEV 표현의 응용은 다양하다. 3D 객체 검출은 가장 직접적인 응용이다. BEV 공간에서 차량, 보행자, 자전거 등의 3D bounding box를 예측한다. 이는 이미지 기반 검출보다 거리와 크기를 정확히 추정할 수 있다. 지도 세그멘테이션은 차선, 도로 경계, 횡단보도, 주차 공간 등을 BEV에서 분할한다. 이는 HD map이 없는 환경에서도 로컬 지도를 생성할 수 있게 한다.

Occupancy prediction은 BEV의 각 셀이 점유되어 있는지, 어떤 유형의 객체인지 예측한다. 명시적인 bounding box 없이도 주행 가능 공간과 장애물을 이해할 수 있다. 이는 명시적으로 정의되지 않은 객체나 복잡한 형태의 장애물을 다루는 데 유리하다. Motion forecasting은 다른 에이전트의 미래 궤적을 BEV에서 예측한다. 공간적 맥락과 에이전트 간 상호작용을 고려하여, 사회적으로 타당한 예측을 생성한다.

경로 계획은 BEV occupancy와 motion forecast를 바탕으로 안전한 궤적을 생성한다. BEV 공간에서 직접 계획하므로, 이미지 좌표와 실제 좌표 간 변환이 불필요하다. End-to-end 학습에서는 BEV 특징이 계획 네트워크의 입력으로 직접 사용된다. Tesla의 FSD는 이러한 접근을 채택하여, BEV 표현을 중심으로 전체 파이프라인을 구성한다.

BEV 표현의 해상도와 범위는 trade-off 관계에 있다. 높은 해상도는 세밀한 정보를 제공하지만, 메모리와 계산 비용이 크다. 넓은 범위는 먼 거리까지 인식할 수 있지만, 동일한 해상도에서는 더 많은 셀을 필요로 한다. 일반적으로 차량 주변 50-100미터, 해상도 0.2-0.5미터 정도가 사용된다. 적응적 해상도 기법도 연구되고 있는데, 차량 근처는 높은 해상도로, 먼 곳은 낮은 해상도로 표현하여 효율성을 높인다.

BEV와 LiDAR의 통합은 센서 융합의 강력한 방법이다. LiDAR 포인트 클라우드는 본질적으로 BEV와 유사한 표현이므로, 카메라 기반 BEV와 자연스럽게 융합된다. 두 modality의 특징을 BEV 공간에서 concatenate하거나 attention으로 융합하여, 각각의 장점을 결합한다. 카메라는 시각적 세부사항과 semantic 정보를, LiDAR는 정확한 기하학과 거리 정보를 제공한다.

BEV 표현은 자율주행의 공간 이해를 혁신적으로 개선했다. 다중 센서와 시간 정보를 통합하는 자연스러운 프레임워크를 제공하며, 이는 인지, 예측, 계획의 모든 단계에서 활용된다. Transformer 아키텍처와의 결합은 end-to-end 학습을 가능하게 하며, 전체 시스템의 성능을 향상시킨다. Tesla, Waymo를 비롯한 주요 자율주행 기업들이 BEV를 핵심 기술로 채택하고 있으며, 이는 앞으로도 자율주행 시스템의 표준적인 표현 방법으로 자리잡을 것이다.

#pagebreak()

= 글로벌 자율주행 산업의 현황

== Tesla의 Vision-Only 접근과 FSD 시스템

Tesla는 자율주행 분야에서 가장 독특하고 논쟁적인 접근 방식을 취하고 있다. 대부분의 경쟁사들이 LiDAR, 레이더, 카메라를 조합한 다중 센서 시스템을 구축하는 동안, Tesla는 2021년부터 순수 카메라 기반, 즉 vision-only 접근을 채택했다. 이러한 선택은 기술적, 경제적, 철학적 이유가 복합적으로 작용한 결과이다.

Elon Musk와 Tesla의 엔지니어들은 인간이 주로 시각만으로 운전한다는 점을 강조한다. 인간 운전자는 LiDAR 없이도 안전하게 주행하며, 충분히 발전된 컴퓨터 비전 시스템이라면 동일한 성능을 달성할 수 있다는 것이 그들의 주장이다. 더 나아가, 도로 인프라와 교통 규칙은 인간의 시각 인지를 전제로 설계되었으므로, 카메라 기반 시스템이 가장 자연스럽다고 본다. 경제적 측면에서도 카메라는 LiDAR에 비해 훨씬 저렴하다. LiDAR 시스템은 차량당 수천에서 수만 달러의 비용이 들지만, 고품질 카메라는 수백 달러 수준이다. Tesla가 목표로 하는 대중 시장 자율주행을 실현하려면 센서 비용의 절감이 필수적이다.

또한 Tesla는 이미 생산되어 도로를 주행 중인 수백만 대의 차량을 가지고 있다. 이들 차량은 모두 카메라 기반 센서 구성을 가지며, 소프트웨어 업데이트만으로 자율주행 기능을 추가하거나 개선할 수 있다. 만약 LiDAR가 필수적이라면, 기존 차량들은 자율주행의 혜택을 받을 수 없게 된다. Vision-only 접근은 이러한 기존 플릿을 활용할 수 있게 한다.

Tesla의 센서 구성은 8개의 카메라로 이루어진다. 전방을 향한 3개의 카메라는 각각 다른 초점 거리를 가져 넓은 시야각과 먼 거리를 동시에 커버한다. 측면과 후방을 향한 5개의 카메라는 차량 주변 360도를 완전히 감지한다. 이들 카메라는 초당 36프레임으로 촬영하며, 약 250미터까지의 거리를 인식할 수 있다. Hardware 3.0 컴퓨터는 144 TOPS의 성능을 제공하며, 2023년부터 도입된 Hardware 4.0는 더 향상된 성능과 센서를 가진다.

FSD의 소프트웨어 아키텍처는 완전한 end-to-end 학습을 지향한다. 초기 버전에서는 여전히 모듈식 구성 요소들이 남아 있었지만, 버전이 올라감에 따라 점점 더 통합되었다. FSD Beta v12부터는 계획 단계도 신경망으로 대체되어, 카메라 입력에서 차량 제어까지 거의 전체가 신경망으로 처리된다. 이는 자율주행 분야에서 가장 극단적인 end-to-end 시스템 중 하나이다.

FSD의 핵심은 강력한 비전 시스템이다. Vision Transformer 기반의 백본 네트워크가 각 카메라 이미지로부터 특징을 추출한다. 이들 특징은 BEV 변환을 거쳐 단일한 조감도 표현으로 통합된다. BEV 공간에서 occupancy network가 각 위치의 점유 상태를 예측한다. 이는 명시적인 객체 검출 없이도 주행 가능 공간과 장애물을 이해할 수 있게 한다. 물론 차량, 보행자 등의 객체 검출과 추적도 수행되지만, 이는 보조적인 역할이다.

Planning 네트워크는 BEV 표현과 고수준 목표를 입력으로 받아, 미래 몇 초간의 주행 궤적을 출력한다. 이 네트워크는 수백만 마일의 인간 주행 데이터로 학습되었으며, 안전성, 효율성, 승차감을 동시에 고려하도록 훈련되었다. 흥미롭게도, Tesla는 명시적인 교통 규칙을 프로그래밍하지 않는다. 대신, 인간 운전자가 따르는 규칙을 데이터로부터 암묵적으로 학습한다. 예를 들어, 빨간 신호에서 멈추는 것, 교차로에서 양보하는 것, 제한 속도를 지키는 것 등이 모두 학습된 행동이다.

Tesla의 가장 큰 자산은 데이터이다. 2025년 기준으로 전 세계에 수백만 대의 Tesla 차량이 주행하고 있으며, 대부분이 데이터 수집에 동의했다. 이들 차량은 끊임없이 주행 영상과 차량 데이터를 수집하며, 특정 조건에서는 Tesla 서버로 전송한다. 모든 데이터를 전송하는 것은 불가능하므로, 차량 내에서 흥미로운 이벤트를 필터링한다. Shadow mode는 FSD 시스템이 백그라운드에서 작동하여 자신이 어떻게 주행했을지 기록하고, 실제 인간 운전자의 행동과 비교한다. 차이가 크거나 특이한 상황에서는 해당 데이터가 업로드 대상으로 표시된다.

이렇게 수집된 데이터는 Tesla의 데이터 엔진을 구동한다. 엔지니어들은 시스템이 실패하거나 어려움을 겪는 상황을 식별한다. 해당 상황의 데이터를 수집하고 레이블링하여 재학습에 사용한다. 이러한 순환은 지속적으로 반복되며, 시스템은 점점 더 다양한 상황을 처리할 수 있게 된다. Tesla는 이를 "flywheel" 효과라고 부른다. 더 많은 차량이 더 많은 데이터를 생성하고, 더 많은 데이터가 더 나은 시스템을 만들며, 더 나은 시스템이 더 많은 차량 판매로 이어진다.

자동 레이블링 파이프라인도 중요한 역할을 한다. 모든 데이터에 사람이 레이블을 다는 것은 불가능하므로, 이미 학습된 모델이 새로운 데이터에 자동으로 레이블을 달고, 사람은 불확실한 경우만 검토한다. 또한 여러 프레임과 다중 카메라의 정보를 통합하여 레이블의 일관성과 정확성을 높인다. 예를 들어, 한 프레임에서 가려진 객체도 다른 프레임이나 다른 카메라에서는 보일 수 있으며, 이를 종합하여 더 정확한 3D 위치를 추정한다.

FSD의 성능은 버전이 올라감에 따라 꾸준히 개선되고 있다. 초기 버전은 주로 고속도로에서만 작동했지만, 최근 버전은 복잡한 도심 환경에서도 주행할 수 있다. 개입 빈도, 즉 인간이 제어권을 가져가야 하는 빈도는 지속적으로 감소하고 있다. Tesla는 내부적으로 miles per intervention 지표를 추적하며, 이는 수백 마일에서 수천 마일로 향상되었다고 알려져 있다. 그러나 아직 완전 무인 자율주행 수준에는 이르지 못했다.

Vision-only 접근의 한계도 명확하다. 카메라는 악천후에 취약하다. 비, 눈, 안개는 시야를 가리며, 야간이나 역광 상황에서는 인식 성능이 저하된다. 깊이 추정은 본질적으로 어려운 문제이며, 특히 질감이 없는 표면이나 먼 거리에서는 부정확할 수 있다. LiDAR는 이러한 상황에서 더 강건하고 정확한 거리 정보를 제공한다. 또한 카메라는 redundancy가 부족하다. 카메라가 고장나거나 가려지면 해당 방향의 인식이 불가능해진다. 다중 센서 시스템은 하나의 센서가 실패해도 다른 센서로 보완할 수 있다.

비평가들은 Tesla의 접근이 안전성을 희생한다고 주장한다. 규제 기관과 안전 단체들은 FSD의 사고 사례를 지적하며, 더 엄격한 검증이 필요하다고 강조한다. Tesla는 통계적으로 FSD가 인간 운전자보다 안전하다고 주장하지만, 독립적인 검증 데이터는 제한적이다. 또한 FSD는 여전히 감독이 필요한 Level 2+ 시스템이며, 완전 자율주행인 Level 4/5와는 거리가 있다.

그럼에도 Tesla의 접근은 자율주행 분야에 큰 영향을 미치고 있다. 다른 기업들도 카메라 중심 시스템을 재고하고 있으며, 일부는 LiDAR를 선택 사항으로 만들거나 장기적으로 제거할 계획을 가지고 있다. Vision-only가 충분히 발전한다면, 이는 자율주행의 대중화를 크게 앞당길 수 있다. Tesla는 이러한 미래에 가장 가까이 있는 기업이며, 그들의 성공 여부는 자율주행 산업 전체의 방향을 결정할 것이다.

== Waymo의 Robotaxi 서비스와 Level 4 달성

Waymo는 자율주행 분야의 선구자이자 가장 보수적이고 체계적인 접근을 취하는 기업이다. Google의 자율주행 프로젝트로 시작하여 2016년 독립 회사가 된 Waymo는, 완전 무인 자율주행, 즉 SAE Level 4를 달성한 몇 안 되는 기업 중 하나이다. Tesla가 대중 시장과 빠른 반복을 추구한다면, Waymo는 안전성과 확실성을 최우선으로 하며 점진적으로 확장한다.

Waymo의 기술 철학은 "safety by design"이다. 시스템의 모든 구성 요소는 안전을 최우선으로 설계되며, 다중 redundancy가 핵심 원칙이다. Waymo Driver라고 불리는 자율주행 시스템은 29개의 카메라, 5개의 LiDAR, 6개의 레이더를 탑재한다. 이는 Tesla의 8개 카메라와 극명한 대조를 이룬다. 각 센서는 다른 센서의 약점을 보완하며, 하나가 실패해도 시스템은 계속 작동할 수 있다.

LiDAR는 Waymo 시스템의 중추이다. LiDAR는 레이저 펄스를 발사하고 반사되는 시간을 측정하여 거리를 정확히 계산한다. 이는 날씨와 조명에 관계없이 일관된 성능을 제공하며, 카메라가 어려움을 겪는 야간이나 역광 상황에서도 강건하다. Waymo는 자체 LiDAR 하드웨어를 개발하여 성능을 최적화하고 비용을 절감했다. 최신 세대 LiDAR는 300미터 이상의 거리를 감지하며, 작은 객체도 정확히 인식한다.

카메라는 시각적 세부사항과 색상 정보를 제공한다. 교통 신호등의 색상, 표지판의 텍스트, 차선 표시의 유형 등은 카메라로만 인식할 수 있다. Waymo의 카메라는 고동적 범위를 가지며, 밝은 하늘과 어두운 그림자를 동시에 처리할 수 있다. 레이더는 속도 정보를 제공하며, 도플러 효과를 통해 다가오는 차량의 상대 속도를 직접 측정한다. 이는 충돌 예측과 긴급 제동에 중요하다.

Waymo의 소프트웨어는 모듈식 아키텍처를 유지하면서도, 각 모듈을 높은 수준으로 발전시켰다. 인지 시스템은 다중 센서 융합을 통해 주변 환경의 3D 모델을 구축한다. 객체의 위치, 크기, 방향, 속도를 정확히 추정하며, 불확실성도 정량화한다. 예측 시스템은 다른 에이전트의 미래 행동을 예측한다. 단순한 궤적 예측을 넘어, 의도와 주의를 추론한다. 예를 들어, 보행자가 도로를 횡단하려는지, 단지 도로변에 서 있는 것인지 판단한다.

계획 시스템은 안전성을 최우선으로 하는 계층적 구조를 가진다. 상위 계획은 목적지까지의 전체 경로를 결정하며, HD map과 실시간 교통 정보를 활용한다. 중간 계획은 현재 도로에서의 전략을 결정한다. 차선 변경, 추월, 합류 등의 기동을 계획한다. 하위 계획은 구체적인 궤적을 생성하며, 장애물 회피와 승차감을 고려한다. 각 단계에서 여러 옵션이 평가되며, 안전성 검증을 통과한 것만 실행된다.

Waymo의 안전성 검증은 매우 엄격하다. 계획된 모든 궤적은 충돌 가능성을 확인하며, 조금이라도 위험이 있다면 거부된다. 최악의 경우 분석을 통해, 다른 차량이 예상과 다르게 행동해도 안전을 보장할 수 있는지 평가한다. 비상 제동 시스템은 항상 대기 상태이며, 밀리초 단위로 위험을 감지하고 대응한다. 하드웨어 redundancy도 중요하다. 중요한 구성 요소들은 이중화되어 있으며, 하나가 실패해도 시스템은 안전하게 정지하거나 제한된 기능으로 계속 작동할 수 있다.

Waymo One은 Waymo의 상용 Robotaxi 서비스이다. 2018년 피닉스에서 시작하여, 현재는 샌프란시스코, 로스앤젤레스, 오스틴 등으로 확장되었다. 2025년 기준으로 주당 15만 회 이상의 유료 승차를 제공하며, 24시간 운영된다. 완전 무인으로 운영되며, 안전 운전자가 동승하지 않는다. 사용자는 Waymo One 앱으로 차량을 호출하며, 경험은 Uber나 Lyft와 유사하다.

Waymo의 확장 전략은 매우 신중하다. 새로운 도시로 진출하기 전에 철저한 준비가 이루어진다. 먼저 해당 도시의 HD map을 구축한다. 이는 수 센티미터 정확도의 3D 지도로, 모든 차선, 표지판, 신호등, 도로 경계를 포함한다. 맵핑 차량이 해당 지역을 여러 번 주행하며 데이터를 수집하고, 이를 처리하여 지도를 생성한다. 그 다음 안전 운전자가 동승한 상태로 수개월간 테스트 주행을 한다. 시스템의 성능을 평가하고, 지역 특유의 교통 패턴을 학습한다.

충분한 데이터와 경험이 축적되면, 제한된 구역에서 무인 주행을 시작한다. 초기에는 승객 없이 차량만 주행하며, 원격 모니터링 센터에서 감시한다. 문제가 없다고 판단되면, 소수의 선택된 사용자에게 서비스를 제공한다. 피드백을 수집하고 개선한 후, 점진적으로 서비스 지역과 시간대를 확장한다. 이러한 과정은 수년이 걸릴 수 있지만, Waymo는 서두르지 않는다. 안전성이 검증될 때까지 확장하지 않는 것이 원칙이다.

Waymo의 안전 기록은 인상적이다. 수천만 마일의 자율주행을 기록했으며, 인간 운전 평균 대비 훨씬 낮은 사고율을 보인다고 Waymo는 발표했다. 그러나 사고가 전혀 없는 것은 아니다. 경미한 접촉 사고들이 보고되었으며, 일부는 Waymo 차량의 판단 오류였다. 각 사고는 철저히 분석되며, 재발 방지를 위한 개선이 이루어진다.

Waymo의 과제는 경제성이다. 고가의 센서와 HD map 구축, 그리고 느린 확장 속도는 높은 비용을 의미한다. Robotaxi 서비스가 수익성을 달성하려면, 운영 비용을 크게 줄이거나 서비스 가격을 올려야 한다. Waymo는 센서 비용을 지속적으로 절감하고 있으며, 규모의 경제를 통해 단가를 낮추려 한다. 또한 차량 활용도를 높이고, 유지 보수를 자동화하여 운영 효율을 개선한다.

2025년 말까지 Waymo는 2500대 규모의 플릿을 목표로 하고 있다. 현재 주로 Jaguar I-PACE 전기차를 사용하지만, Geely의 Zeekr 차량으로도 확장하고 있다. 더 저렴하고 자율주행에 최적화된 차량으로 전환하면 경제성이 개선될 것으로 기대된다. 장기적으로 Waymo는 자체 설계한 전용 자율주행 차량을 도입할 계획도 가지고 있다. 운전석이 없고, 승객 공간을 최대화하며, 자율주행 하드웨어가 통합된 차량이다.

Waymo의 성공은 자율주행 산업에 중요한 의미를 가진다. Level 4 자율주행이 기술적으로 가능하며, 상업적으로 운영될 수 있음을 입증했다. 이는 규제 기관과 대중에게 자율주행의 실현 가능성을 확신시킨다. 동시에 Waymo의 보수적이고 비용이 많이 드는 접근은, 자율주행의 대중화가 얼마나 어려운지도 보여준다. Tesla의 빠르고 저렴한 접근과 Waymo의 느리고 확실한 접근 사이의 긴장은 자율주행 산업의 미래를 형성할 것이다.

=== Tesla와 Waymo의 기술 전략 비교

아래 표는 자율주행 분야의 두 선도 기업인 Tesla와 Waymo의 접근 방식을 다각도로 비교한 것이다. 이들은 기술 철학, 센서 구성, 시장 전략, 확장 속도 등에서 극명한 대조를 보이며, 각각의 장단점을 가지고 있다.

#figure(
  table(
    columns: 3,
    stroke: 0.5pt + rgb("#cbd5e1"),
    fill: (x, y) => {
      if y == 0 { rgb("#1e40af") }
      else if calc.odd(y) { rgb("#f8fafc") }
      else { white }
    },
    align: (x, y) => {
      if x == 0 { left }
      else { left }
    },
    inset: 10pt,

    // Header
    table.cell(fill: rgb("#1e40af"))[*구분*],
    table.cell(fill: rgb("#1e40af"))[*Tesla FSD*],
    table.cell(fill: rgb("#1e40af"))[*Waymo Driver*],

    // Rows
    [*기술 철학*],
    [Vision-only, End-to-End 학습\
    "인간처럼 카메라만으로"],
    [Multi-sensor fusion, Safety by design\
    "다중 센서로 안전성 최대화"],

    [*센서 구성*],
    [카메라 8대\
    (360도 커버, 최대 250m)],
    [카메라 29대, LiDAR 5대, 레이더 6대\
    (다중 redundancy)],

    [*센서 비용*],
    [약 \$1,000 미만\
    (대중화 가능)],
    [약 \$100,000 이상\
    (고가, 점진적 비용 절감 중)],

    [*아키텍처*],
    [완전 End-to-End 신경망\
    (v12부터 planning까지 신경망화)],
    [모듈식 아키텍처\
    (인지-예측-계획 분리)],

    [*자율주행 레벨*],
    [Level 2+ (감독 필요)\
    Level 3/4 목표 (2025-2026)],
    [Level 4 (완전 무인)\
    (Phoenix, SF, LA, Austin 등)],

    [*데이터 규모*],
    [수백만 대 플릿\
    수십억 마일 주행 데이터],
    [수천 대 플릿\
    수천만 마일 주행 데이터],

    [*학습 방법*],
    [Imitation Learning (주)\
    대규모 플릿 데이터 활용],
    [Simulation + Real-world\
    철저한 검증 후 배포],

    [*HD Map 의존성*],
    [없음\
    (실시간 센서 데이터만 사용)],
    [매우 높음\
    (cm급 정밀 지도 필수)],

    [*확장 전략*],
    [빠른 배포, 지속적 업데이트\
    (모든 Tesla 차량 대상)],
    [점진적 확장, 철저한 검증\
    (도시별 수년간 준비)],

    [*비즈니스 모델*],
    [차량 판매 + FSD 구독\
    향후 Robotaxi 서비스 계획],
    [Robotaxi 서비스 (Waymo One)\
    주당 15만+ 유료 승차],

    [*장점*],
    [• 낮은 센서 비용\
    • 빠른 반복 개발\
    • 대규모 플릿 데이터\
    • 대중 시장 적용 가능],
    [• 높은 안전성\
    • Level 4 달성\
    • 다중 센서 redundancy\
    • 입증된 상용 서비스],

    [*한계*],
    [• 악천후 취약성\
    • 감독 여전히 필요\
    • 규제 승인 미확보\
    • 깊이 추정 한계],
    [• 매우 높은 비용\
    • 느린 확장 속도\
    • HD map 구축 부담\
    • 제한된 서비스 지역],
  ),
  caption: [Tesla FSD와 Waymo Driver의 종합 비교. 두 기업은 자율주행 실현을 위한 상반된 전략을 추구하며, 각각 대중화와 안전성이라는 서로 다른 우선순위를 가진다.]
)

이러한 비교에서 알 수 있듯이, Tesla와 Waymo는 자율주행이라는 동일한 목표를 향해 정반대의 경로를 걷고 있다. Tesla는 저비용 센서와 대규모 데이터를 활용하여 빠르게 반복 개발하며 대중 시장을 겨냥한다. 반면 Waymo는 고가의 센서와 철저한 검증을 통해 안전성을 최우선시하며, 제한된 지역에서 완전 무인 서비스를 실현했다. 두 접근 방식 모두 장단점이 있으며, 어느 것이 궁극적으로 승리할지는 기술 발전, 규제 환경, 시장 수용성 등 여러 요인에 달려 있다. 일부 전문가들은 두 접근이 수렴하여, 초기에는 고가 센서로 시작하되 점차 카메라 중심으로 전환할 것이라고 전망하기도 한다.

#pagebreak()

= 2025년 기술 동향과 산업 전망

== FSD v13과 v14의 진화

2025년은 Tesla FSD의 중요한 전환점이 되고 있다. FSD v13은 2025년 초 광범위하게 배포되기 시작했으며, 이전 버전들과 비교하여 몇 가지 중요한 개선사항을 보였다. 가장 눈에 띄는 변화는 End-to-End 신경망 아키텍처의 비중이 더욱 확대되었다는 점이다. v12에서 시작된 계획 단계의 신경망화가 v13에서 더욱 정교해졌으며, 시스템은 더 자연스럽고 인간다운 주행 스타일을 보인다.

도시 환경에서의 성능 향상이 특히 두드러진다. 복잡한 교차로 통과, 비보호 좌회전, 로터리 주행 등 어려운 시나리오에서의 처리 능력이 개선되었다. 사용자들의 보고에 따르면, 인간 개입이 필요한 빈도가 이전 버전 대비 30-40% 감소했다. 주차장 자율 주행 기능도 추가되어, 사용자가 차에서 내린 후 차량이 스스로 주차 공간을 찾아 주차하고, 호출 시 픽업 장소로 이동하는 기능이 실현되었다.

FSD v14는 2025년 중반 출시를 목표로 개발 중이다. v14의 핵심 혁신은 Occupancy Network의 전면적 적용이다. 기존의 객체 검출 중심 접근에서 벗어나, BEV 공간의 각 셀이 점유되어 있는지, 어떤 속성을 가지는지 직접 예측하는 방식으로 전환된다. 이는 명시적으로 정의되지 않은 객체나 복잡한 형태의 장애물에 대한 강건성을 크게 향상시킬 것으로 기대된다. 또한 시간적 예측 범위가 확장되어, 현재 2-3초인 계획 지평을 5-7초까지 늘리는 것이 목표이다.

Hardware 4.0 차량에 대한 최적화도 v14의 주요 특징이다. 개선된 카메라 센서와 증가된 컴퓨팅 파워를 활용하여, 더 높은 해상도의 BEV 표현과 더 정밀한 occupancy 예측이 가능해진다. Tesla는 Hardware 3.0 차량도 v14를 지원할 것이라고 발표했지만, 일부 기능은 Hardware 4.0 전용이 될 가능성이 있다.

Elon Musk는 2025년 말까지 "unsupervised" FSD, 즉 운전자 감독 없이 작동하는 자율주행을 목표로 한다고 밝혔다. 이는 SAE Level 3 또는 4에 해당하는 수준이다. 기술적으로는 제한된 영역이나 조건에서 달성 가능할 수 있지만, 규제 승인이 가장 큰 변수이다. 미국 각 주마다 다른 규제 환경, 안전성 입증의 어려움, 그리고 책임 소재 문제 등이 해결되어야 한다. 전문가들은 일부 주에서 제한적인 승인이 나올 수 있지만, 전국적인 무인 자율주행은 2026년 이후가 될 것으로 전망한다.

Robotaxi는 Tesla의 장기 전략에서 핵심적인 위치를 차지한다. Tesla는 2025년 Robotaxi 전용 차량을 공개할 계획이며, 이는 운전석이 없고 승객 공간을 최대화한 디자인이 될 것으로 예상된다. 제조는 2026년 이후 시작될 전망이며, 초기에는 Tesla 소유의 플릿으로 운영되다가 점차 개인 차량 소유자들이 자신의 차량을 Robotaxi로 운영할 수 있는 플랫폼으로 확장될 계획이다. 이는 Uber나 Lyft와 같은 기존 승차 공유 서비스에 직접적인 경쟁 요소가 될 것이다.

== 중국 시장의 급속한 발전

중국은 2025년 현재 자율주행 기술의 상용화에서 세계를 선도하고 있다. 정부의 적극적인 지원, 거대한 시장 규모, 그리고 빠른 기술 발전이 결합하여 놀라운 속도로 진화하고 있다. 중국 정부는 스마트 모빌리티를 국가 핵심 전략 산업으로 지정하고, 규제 샌드박스를 통해 실험적 서비스를 허용하며, 인프라 투자를 아끼지 않고 있다.

중국 OEM들의 NOA 기능 배포는 매우 공격적이다. 2025년 상반기 기준으로 주요 전기차 브랜드들이 도심 NOA 기능을 출시했거나 베타 테스트 중이다. 특히 주목할 만한 것은 이들 시스템이 HD map 없이도 작동하는 경우가 많다는 점이다. 실시간 센서 데이터와 BEV 표현만으로 주행하는 "mapless" 접근이 확산되고 있으며, 이는 새로운 지역으로의 확장을 크게 가속화한다.

중국 시장의 독특한 특징은 가격 경쟁력이다. 국산 자율주행 칩, 센서, 그리고 소프트웨어 플랫폼의 발전으로 시스템 비용이 크게 절감되었다. 일부 중급 모델에서도 고급 자율주행 기능이 표준 또는 저렴한 옵션으로 제공되며, 이는 대중 시장 확산을 가속화한다. 또한 월 구독 모델이 널리 채택되어, 초기 구매 비용 부담 없이 자율주행 기능을 사용할 수 있다.

Robotaxi 서비스도 빠르게 확대되고 있다. Baidu의 Apollo Go는 중국 최대 규모로, 2025년 여러 주요 도시에서 완전 무인 운영을 하고 있다. 베이징, 상하이, 선전, 우한 등에서 일반 대중이 앱을 통해 무인 택시를 호출할 수 있다. 정부와의 긴밀한 협력 덕분에 규제 승인이 비교적 신속하게 이루어지며, 테스트 구역이 지속적으로 확대되고 있다.

중국 시장의 또 다른 강점은 로컬 최적화이다. 중국의 독특한 교통 환경과 운전 문화에 특화된 시스템 개발이 이루어지고 있다. 높은 교통 밀도, 다양한 도로 참여자(전동 스쿠터, 삼륜차 등), 그리고 덜 엄격한 차선 준수 등 중국 특유의 상황을 처리하는 능력이 필수적이다. 중국 기업들은 방대한 중국 주행 데이터로 학습하여, 이러한 환경에서 우수한 성능을 보인다.

그러나 중국 시스템의 글로벌 확장은 과제를 안고 있다. 중국 외 지역에서의 데이터 수집과 학습에는 제약이 있으며, 서로 다른 교통 규칙과 문화에 적응해야 한다. 또한 지정학적 긴장으로 인한 기술 이전 제한, 데이터 프라이버시 규제 등이 장벽이 될 수 있다. 그럼에도 중국 기업들은 동남아시아와 중동 등으로 진출을 시도하고 있으며, 가격 경쟁력을 무기로 시장을 개척하고 있다.

== 한국의 도전과 기회

한국은 자율주행 분야에서 후발 주자의 위치에 있다. 현대자동차그룹은 글로벌 자동차 제조사로서의 역량을 가지고 있지만, 자율주행 소프트웨어에서는 Tesla, Waymo, 중국 기업들에 비해 뒤처져 있다. 2025년 현재 현대자동차는 고속도로 자율주행 기능을 개발 중이며, 도심 자율주행은 그 이후 단계로 계획하고 있다. 보수적인 접근을 취하고 있으며, 안전성을 최우선으로 하는 전략이다.

한국이 직면한 가장 큰 과제는 AI 인프라의 부족이다. Tesla는 Dojo라는 슈퍼컴퓨터를 포함하여 약 13만 5천 개의 GPU를 보유하고 있다고 알려져 있다. 이는 대규모 신경망 학습에 필수적이다. 반면 한국 기업들은 수십에서 수백 개 수준의 GPU를 가지고 있는 것으로 추정된다. 이러한 인프라 격차는 모델 개발 속도와 성능에 직접적인 영향을 미친다. Foundation Models의 시대에서 컴퓨팅 파워는 곧 경쟁력이며, 이 부분에서의 투자 확대가 시급하다.

두 번째 과제는 데이터이다. Tesla는 전 세계 수백만 대의 차량으로부터 주행 데이터를 수집한다. Waymo도 수천만 마일의 실제 주행 데이터를 가지고 있다. 중국 기업들은 거대한 내수 시장에서 풍부한 데이터를 확보한다. 한국은 상대적으로 작은 시장 규모와 제한된 플릿으로 인해 데이터 수집에 한계가 있다. 데이터 다양성도 문제이다. 다양한 기후, 도로 조건, 교통 상황을 경험하려면 글로벌 데이터가 필요하지만, 이는 현지 규제와 프라이버시 문제로 어려움이 있다.

세 번째는 인재 확보이다. 자율주행 분야의 최고 전문가들은 Tesla, Waymo, Google, 중국 대기업들로 몰리고 있다. 높은 연봉, 최첨단 프로젝트, 풍부한 자원이 인재를 끌어당긴다. 한국 기업들은 이들과의 인재 경쟁에서 불리한 위치에 있다. 국내 대학과 연구기관에서 우수한 연구자들이 배출되지만, 많은 경우 해외로 유출된다. 인재 유치와 육성을 위한 생태계 조성이 필요하다.

그러나 한국에게도 기회는 있다. 첫째, 제조 역량이다. 한국은 세계적 수준의 자동차 제조 기술과 품질 관리 시스템을 가지고 있다. 자율주행 차량의 대량 생산과 품질 보증에서 경쟁력을 가질 수 있다. 둘째, 반도체와 전자 기술이다. 삼성과 SK는 메모리 반도체에서 세계를 선도하며, AI 칩 개발에도 투자하고 있다. 자율주행 컴퓨팅 플랫폼의 핵심 부품을 국산화하고 최적화할 수 있는 기반이 있다.

셋째, 5G와 V2X 기술이다. 한국은 5G 상용화에서 선도적 위치에 있으며, 차량-인프라 통신 기술 개발에 투자하고 있다. 자율주행과 통신 기술의 융합을 통해 독자적인 강점을 만들 수 있다. 넷째, 정부 지원이다. 한국 정부는 자율주행을 미래 성장 동력으로 인식하고, 규제 샌드박스, R&D 지원, 테스트베드 구축 등을 추진하고 있다. 이러한 지원이 효과적으로 집행된다면 산업 발전을 가속화할 수 있다.

한국의 전략은 선택과 집중이 될 수밖에 없다. 모든 분야에서 Tesla나 Waymo를 따라잡기는 어렵다. 대신 특정 분야에서 차별화된 강점을 구축하는 것이 현실적이다. 예를 들어, 상용차 자율주행, 특정 유형의 도로나 환경에 특화된 시스템, 또는 자율주행과 다른 기술의 융합 영역에서 틈새 시장을 공략할 수 있다. 또한 글로벌 협력을 통해 기술과 데이터를 확보하는 것도 중요한 전략이다. Mobileye, NVIDIA 등 글로벌 파트너와의 협력을 강화하고, 오픈소스 생태계에 적극 참여하여 최신 기술에 접근해야 한다.

#pagebreak()

= 기술적 과제와 미래 전망

== 극복해야 할 핵심 과제

=== Long-tail 문제: 드문 상황의 처리

자율주행 시스템이 직면한 가장 어려운 과제 중 하나는 long-tail 문제이다. 대부분의 주행 상황은 상대적으로 단순하고 예측 가능하다. 맑은 날 고속도로 주행이나 정상적인 도심 교통에서는 현재의 시스템들이 이미 우수한 성능을 보인다. 그러나 드물게 발생하는 복잡하고 예외적인 상황들, 즉 long tail에 속하는 상황들이 진정한 도전이다.

Long-tail 상황의 예는 무수히 많다. 도로 공사로 차선이 복잡하게 변경되고 임시 표지판이 설치된 경우, 사고 현장에서 경찰이 수신호로 교통을 통제하는 경우, 강풍으로 나무가 도로를 막은 경우, 동물이 도로를 횡단하는 경우, 다른 차량이 역주행하는 경우 등이다. 이러한 상황들은 각각 매우 드물게 발생하지만, 전체적으로는 상당한 비중을 차지한다. 문제는 이런 상황들이 너무 다양하고 예측 불가능하여, 모두를 명시적으로 프로그래밍하거나 충분한 학습 데이터를 수집하기 어렵다는 것이다.

Long-tail 문제를 해결하기 위한 여러 접근이 시도되고 있다. 첫째, 데이터 수집의 규모를 극대화한다. Tesla의 접근이 대표적인데, 수백만 대의 플릿으로 수십억 마일을 주행하면서 드문 상황도 충분히 경험하고 데이터를 수집한다. 규모가 충분히 크면 long tail도 결국 데이터로 포착할 수 있다는 전략이다. 둘째, 생성 AI를 활용한 데이터 증강이다. Diffusion Models와 World Models를 사용하여 실제로는 경험하지 못한 드문 상황을 합성적으로 생성하고, 이를 학습과 테스트에 활용한다.

셋째, Foundation Models의 일반화 능력을 활용한다. 대규모 사전 학습을 통해 다양한 시각적 개념과 패턴을 학습한 모델은, 학습 중에 직접 보지 못한 새로운 조합도 어느 정도 이해할 수 있다. Zero-shot 또는 few-shot learning을 통해 적은 예시만으로도 새로운 상황에 적응할 수 있다. 넷째, 인간과의 협력이다. 시스템이 불확실하거나 이해할 수 없는 상황에 직면하면, 원격 운영자에게 도움을 요청하거나, 안전하게 정지하고 인간 운전자에게 제어권을 넘긴다. 완전 무인을 목표로 하지만, 과도기에는 이러한 fall-back 전략이 필수적이다.

=== 안전성의 통계적 검증

자율주행 시스템이 상용화되려면, 인간 운전자보다 안전하다는 것을 통계적으로 입증해야 한다. 이는 생각보다 훨씬 어려운 문제이다. 미국 기준으로 인간 운전자는 평균적으로 약 100만 마일당 1건의 사망 사고를 일으킨다. 자율주행 시스템이 이보다 훨씬 안전하다는 것을 95% 신뢰도로 통계적으로 입증하려면, 수억 마일의 무사고 주행이 필요하다는 연구 결과가 있다.

실제 도로에서 수억 마일을 주행하는 것은 시간과 비용이 막대하다. Waymo가 2009년부터 시작하여 2025년까지 누적한 자율주행 마일리지가 수천만 마일 수준이다. 더 많은 플릿으로 가속화할 수 있지만, 여전히 엄청난 시간과 자원이 필요하다. 게다가 시스템은 지속적으로 업데이트되므로, 각 버전마다 새로 검증해야 한다는 문제도 있다.

이 문제를 해결하기 위해 시뮬레이션 기반 검증이 핵심 역할을 한다. CARLA와 같은 고품질 시뮬레이터에서 수십억 마일의 가상 주행을 수행하여, 다양한 시나리오에서의 시스템 행동을 평가한다. 그러나 시뮬레이션과 실제의 차이, 즉 sim-to-real gap은 여전히 과제이다. 시뮬레이션에서 완벽해도 실제에서 실패할 수 있다. 따라서 시뮬레이션 결과를 실제 주행 데이터로 보정하고 검증하는 하이브리드 접근이 필요하다.

시나리오 기반 테스트도 중요한 방법론이다. 모든 가능한 상황을 테스트하는 대신, 안전에 중요한 핵심 시나리오들을 식별하고 이들을 집중적으로 평가한다. ISO나 Euro NCAP 같은 표준 기관들이 자율주행을 위한 표준 테스트 시나리오를 개발하고 있다. 각 시나리오에서의 성능을 정량적으로 측정하고, 종합하여 전체 안전성을 평가한다.

분해된 안전성 분석도 유용하다. 전체 시스템의 안전성을 한 번에 입증하는 대신, 각 구성 요소의 안전성을 독립적으로 검증하고 조합한다. 인지 시스템의 검출률과 오검출률, 예측 시스템의 정확도, 계획 시스템의 안전 마진 등을 각각 평가한다. 각 모듈이 충분한 성능을 가지고 적절히 통합되었다면, 전체 시스템도 안전하다는 논리이다. 이는 완벽하지는 않지만, 실용적인 접근이다.

=== 일반화: 새로운 환경으로의 적응

자율주행 시스템은 학습된 환경뿐만 아니라 새로운 환경에서도 작동해야 한다. 그러나 환경 간의 차이는 생각보다 크다. 도시마다 도로 구조, 표지판 스타일, 교통 패턴이 다르다. 국가마다 운전 문화, 우선 순위 규칙, 법규가 다르다. 계절과 날씨에 따라 가시성과 도로 조건이 변한다. 시스템이 모든 변화에 강건하게 대응하는 것은 어렵다.

Domain adaptation 기술이 이 문제를 완화한다. 소스 도메인(예: 캘리포니아)에서 학습된 모델을 타겟 도메인(예: 뉴욕)으로 전이할 때, 도메인 간 차이를 명시적으로 모델링하고 보정한다. Adversarial learning을 사용하여 도메인 불변 표현을 학습하거나, 소량의 타겟 도메인 데이터로 fine-tuning을 수행한다. Style transfer 기술로 소스 도메인 데이터를 타겟 도메인 스타일로 변환하여 데이터 증강에 활용하기도 한다.

Foundation Models의 일반화 능력도 중요하다. 대규모 다양한 데이터로 사전 학습된 모델은 특정 환경에 과적합되지 않고, 일반적인 시각 이해 능력을 가진다. 새로운 환경에서도 기본적인 인지 능력은 유지되며, 적은 양의 fine-tuning으로 적응할 수 있다. Meta-learning 기법은 "학습하는 법을 학습"하여, 새로운 작업이나 환경에 빠르게 적응하는 능력을 학습한다.

로컬 파트너십과 현지화도 실용적인 전략이다. 새로운 지역으로 진출할 때, 현지 기업이나 기관과 협력하여 지역 특유의 지식과 데이터를 확보한다. 현지 엔지니어를 고용하여 시스템을 현지 환경에 맞게 조정한다. 이는 순수 기술적 일반화보다는 느릴 수 있지만, 안전성과 신뢰성을 보장한다.

=== 컴퓨팅 효율성: 실시간 처리와 전력 제약

자율주행 시스템은 실시간으로 작동해야 한다. 센서 데이터를 수집하고, 처리하고, 결정을 내리고, 차량을 제어하는 전체 과정이 밀리초 단위로 이루어져야 한다. 일반적으로 10Hz, 즉 100ms 주기로 제어 명령이 업데이트되며, 이는 각 처리 단계가 수십 밀리초 내에 완료되어야 함을 의미한다. 동시에, 차량 내 전력과 열 관리 제약도 고려해야 한다. 수백 와트의 전력을 지속적으로 소비하면 배터리 수명에 영향을 미치고, 냉각 시스템의 부담도 커진다.

Foundation Models와 생성 모델의 발전은 성능을 크게 향상시켰지만, 동시에 계산 비용도 증가시켰다. 수억 개 파라미터의 Transformer 모델, 수십 단계의 Diffusion 샘플링은 강력한 GPU에서도 수백 밀리초가 걸릴 수 있다. 이를 실시간으로 임베디드 시스템에서 실행하는 것은 도전이다.

모델 압축 기술이 중요한 역할을 한다. Pruning은 중요하지 않은 파라미터나 연결을 제거하여 모델 크기를 줄인다. Quantization은 32비트 부동소수점 대신 8비트 또는 더 낮은 정밀도를 사용하여 메모리와 계산량을 줄인다. Knowledge distillation은 큰 교사 모델의 지식을 작은 학생 모델로 전이하여, 성능을 유지하면서도 효율적인 모델을 만든다.

전용 하드웨어 가속도 핵심이다. NVIDIA의 Orin과 같은 자율주행 전용 SoC는 AI 추론에 최적화된 아키텍처를 가진다. Tensor cores, sparse matrix acceleration, low-precision arithmetic 등의 기능으로 효율성을 극대화한다. Tesla의 FSD 칩은 자사 워크로드에 특화된 ASIC으로, 범용 GPU보다 전력 대비 성능이 우수하다. 향후 더 많은 기업들이 자체 AI 칩을 개발할 것으로 예상된다.

Efficient architecture 설계도 중요하다. MobileNet, EfficientNet과 같이 모바일과 임베디드 환경을 위해 설계된 효율적인 신경망 아키텍처들이 자율주행에도 적용된다. Sparse attention, grouped convolution, depthwise separable convolution 등의 기법으로 계산량을 줄인다. 또한 전체 파이프라인을 최적화하여, 중복 계산을 제거하고 메모리 접근을 최소화한다.

계층적 처리와 early exit도 전략이 될 수 있다. 간단한 상황에서는 가벼운 모델로 빠르게 처리하고, 복잡하거나 불확실한 상황에서만 무거운 모델을 동원한다. 이는 평균적인 계산량을 줄이면서도, 필요한 경우 충분한 성능을 확보한다. 시간적 일관성도 활용한다. 모든 프레임에서 처음부터 계산하는 대신, 이전 프레임의 결과를 재사용하고 변화된 부분만 업데이트한다.

== 2030년까지의 기술 로드맵

=== 2025-2026: NOA의 대중화와 Foundation Models의 표준화

2025년과 2026년은 자율주행이 프리미엄 차량에서 중급 차량으로 확산되는 시기가 될 것이다. 고속도로 NOA 기능은 이미 많은 차량에 보급되었으며, 도심 NOA도 빠르게 확대되고 있다. 가격이 내려가면서 중급 모델에서도 옵션으로 제공되기 시작할 것이다. 특히 중국 시장에서 이러한 경향이 두드러질 것이며, 이는 글로벌 시장에도 압력을 가할 것이다.

Vision-centric 접근이 주류가 될 것이다. Tesla의 성공과 센서 비용 절감 압력으로 인해, 더 많은 기업들이 카메라 중심 시스템을 채택할 것이다. LiDAR는 완전히 사라지지는 않겠지만, 프리미엄 또는 Robotaxi 전용으로 남을 가능성이 크다. 양산 차량에서는 카메라와 레이더의 조합이 표준이 될 것이다.

Foundation Models가 자율주행의 표준 기술로 자리잡을 것이다. Vision Transformer, CLIP과 같은 대규모 사전 학습 모델이 인지 시스템의 백본이 되며, 기업들은 자사의 주행 데이터로 fine-tuning하여 차별화를 꾀할 것이다. 오픈소스 Foundation Models의 발전으로 작은 기업도 최신 기술에 접근할 수 있게 될 것이다.

규제 프레임워크가 점차 명확해질 것이다. 미국, 유럽, 중국 등 주요 시장에서 자율주행에 대한 법규와 표준이 정립되기 시작할 것이다. Level 3 인증이 더 많은 지역과 조건으로 확대되며, Level 4에 대한 논의도 본격화될 것이다. 안전성 평가 방법론, 책임 소재, 보험 체계 등에 대한 사회적 합의가 형성될 것이다.

=== 2027-2028: Level 3/4의 상용화와 World Models의 통합

2027년과 2028년은 자율주행이 질적으로 도약하는 시기가 될 것이다. 제한된 조건이지만 Level 3, 즉 조건부 자율주행이 선진 시장에서 상용화될 것이다. 고속도로와 일부 도심 지역에서 운전자가 시스템에 운전을 맡기고 다른 활동을 할 수 있게 될 것이다. 일부 지역에서는 Level 4, 즉 특정 구역 내 완전 무인 자율주행도 실현될 것이다.

World Models가 계획 시스템의 핵심으로 부상할 것이다. 단순히 현재 상태에서 반응하는 대신, 여러 행동의 미래 결과를 시뮬레이션하고 비교하여 최적의 결정을 내리는 시스템이 등장할 것이다. 이는 복잡한 상호작용과 장기적 계획이 필요한 도심 환경에서 특히 유용할 것이다.

자율주행 트럭이 본격적으로 상용화될 것이다. 물류 산업은 자율주행의 초기 수혜자가 될 가능성이 크다. 장거리 고속도로 운송은 비교적 예측 가능하고, 경제적 동기가 명확하다. 운전자 부족 문제도 자율주행 도입을 가속화할 것이다. 초기에는 특정 구간이나 convoy 형태로 시작하여, 점차 확대될 것이다.

BEV 표현과 Occupancy Networks가 표준 아키텍처로 자리잡을 것이다. 명시적 객체 검출 대신 공간 점유를 직접 예측하는 방식이 주류가 되며, 이는 long-tail 문제에 더 강건한 것으로 입증될 것이다. 시간적 BEV 통합도 발전하여, 과거 관측을 누적하고 미래를 예측하는 4D BEV 표현이 등장할 것이다.

=== 2029-2030: 완전 무인 자율주행의 실현

2029년과 2030년은 완전 무인 자율주행이 제한된 영역에서 실현되는 시기가 될 것이다. Robotaxi 서비스가 주요 도시의 상당 구역에서 24시간 운영되며, 일반 대중이 일상적으로 이용할 것이다. 안전 기록이 축적되고 신뢰가 구축되면서, 서비스 지역이 빠르게 확대될 것이다.

Robotaxi의 경제성이 입증될 것이다. 초기의 높은 비용이 규모의 경제, 기술 발전, 센서 가격 하락으로 인해 점차 낮아질 것이다. 일부 지역에서는 Robotaxi가 인간 운전 택시보다 저렴해지며, 이는 시장 구조를 근본적으로 변화시킬 것이다. 개인 차량 소유의 필요성도 감소하기 시작할 것이다.

자율주행이 도시 교통 시스템의 상당 비중을 차지하게 될 것이다. 일부 도시에서는 자율주행 차량 전용 차선이나 구역이 설치될 것이다. 교통 흐름 최적화, 주차 수요 감소, 도시 공간 재구성 등의 변화가 시작될 것이다. 스마트 시티 개념과 통합되어, V2X 통신으로 차량과 인프라가 협력하는 시스템이 구축될 것이다.

새로운 모빌리티 서비스 생태계가 형성될 것이다. Robotaxi뿐만 아니라, 자율주행 셔틀, 배달 로봇, 마지막 마일 솔루션 등 다양한 서비스가 등장할 것이다. MaaS 플랫폼이 이들을 통합하여, 사용자는 목적지만 입력하면 최적의 교통 수단 조합을 제안받고 이용할 수 있을 것이다. 모빌리티가 소유에서 서비스로 전환되는 패러다임 변화가 가속화될 것이다.

그러나 2030년에도 모든 곳에서 완전 자율주행이 가능한 것은 아닐 것이다. 복잡한 도심, 악천후, 비포장 도로, 규제가 엄격한 지역 등에서는 여전히 인간 운전이 필요하거나 선호될 것이다. 자율주행의 완전한 대중화는 2030년대 중반 이후가 될 것으로 예상된다.

=== 기술 로드맵 시각화 (2025-2030)

아래는 향후 5년간 자율주행 기술의 발전 경로를 시각화한 것이다. 각 시기별로 핵심 기술, 자율주행 레벨, 주요 이정표, 그리고 예상되는 시장 변화를 정리하였다.

#figure(
  table(
    columns: 5,
    stroke: 0.5pt + rgb("#cbd5e1"),
    fill: (x, y) => {
      if y == 0 { rgb("#1e40af") }
      else if x == 0 { rgb("#3b82f6") }
      else if calc.odd(y) { rgb("#f8fafc") }
      else { white }
    },
    align: center,
    inset: 8pt,

    // Header
    table.cell(fill: rgb("#1e40af"))[*항목*],
    table.cell(fill: rgb("#1e40af"))[*2025-2026*],
    table.cell(fill: rgb("#1e40af"))[*2027-2028*],
    table.cell(fill: rgb("#1e40af"))[*2029-2030*],
    table.cell(fill: rgb("#1e40af"))[*2030+*],

    // Autonomy Level
    table.cell(fill: rgb("#3b82f6"))[*자율주행\n레벨*],
    [Level 2+\n(감독 필요)\n\nLevel 3 시작\n(일부 조건)],
    [Level 3 확대\n(고속도로+도심)\n\nLevel 4 제한적\n(특정 구역)],
    [Level 4 상용화\n(주요 도시)\n\n무인 Robotaxi\n운영],
    [Level 4 확대\n\nLevel 5 연구\n(완전 자율)],

    // Key Technologies
    table.cell(fill: rgb("#3b82f6"))[*핵심\n기술*],
    [• Vision Transformer\n• BEV 표현\n• Imitation Learning\n• Foundation Models],
    [• World Models\n• Occupancy Networks\n• 시간적 예측\n• Multi-task Learning],
    [• 통합 E2E 시스템\n• 온라인 학습\n• V2X 통신\n• Edge AI],
    [• AGI 융합\n• Swarm Intelligence\n• 완전 자동화\n학습 파이프라인],

    // Sensor Configuration
    table.cell(fill: rgb("#3b82f6"))[*센서\n구성*],
    [Vision-centric\n주류화\n\nLiDAR:\n프리미엄 전용],
    [카메라 + 레이더\n표준\n\nLiDAR 비용\n50% 절감],
    [고해상도 카메라\n+ 저가 LiDAR\n\n센서 융합\n최적화],
    [초저가 센서\n\n차량당 비용\n\$500 이하],

    // Infrastructure
    table.cell(fill: rgb("#3b82f6"))[*인프라*],
    [일부 V2X 시범\n\nHD map 확대\n\n5G 네트워크],
    [자율주행 전용\n차선 설치\n\nSmart City 통합],
    [V2X 표준화\n\n도시 인프라\n재설계],
    [완전 통합\n교통 시스템\n\nAI 교통관제],

    // Major Players
    table.cell(fill: rgb("#3b82f6"))[*선도\n기업*],
    [Tesla FSD 확대\nWaymo 도시 추가\n중국 기업 부상],
    [Tesla Robotaxi\n중국 Level 3 양산\nWaymo 수익화],
    [Robotaxi 경쟁\n\n전통 OEM\nLevel 3 출시],
    [통합 MaaS\n플랫폼\n\n모빌리티 혁명],

    // Market Size
    table.cell(fill: rgb("#3b82f6"))[*시장\n규모*],
    [ADAS: \$30B\n\nAV 소프트웨어:\n\$5B\n\nRobotaxi: 시범],
    [ADAS: \$50B\n\nAV 소프트웨어:\n\$15B\n\nRobotaxi: \$2B],
    [ADAS: \$70B\n\nAV 소프트웨어:\n\$30B\n\nRobotaxi: \$10B],
    [통합 자율주행\n시장:\n\$200B+\n\n교통 패러다임\n전환],

    // Challenges
    table.cell(fill: rgb("#3b82f6"))[*주요\n과제*],
    [• 규제 승인\n• 안전성 입증\n• 악천후 대응\n• 비용 절감],
    [• Long-tail 처리\n• 일반화 능력\n• 도시별 적응\n• 윤리적 기준],
    [• 경제성 확보\n• 대중 신뢰\n• 일자리 전환\n• 사이버 보안],
    [• 완전 자율화\n• 글로벌 표준\n• 사회적 통합\n• 지속가능성],

    // Penetration
    table.cell(fill: rgb("#3b82f6"))[*시장\n침투율*],
    [프리미엄 차량:\n30-40%\n\n중급 차량:\n5-10%],
    [프리미엄 차량:\n60-70%\n\n중급 차량:\n25-35%],
    [프리미엄 차량:\n80-90%\n\n중급 차량:\n50-60%],
    [대부분 차량\nADAS 표준 탑재\n\n신차 40%\nLevel 3+],
  ),
  caption: [2025년부터 2030년 이후까지의 자율주행 기술 로드맵. 기술, 시장, 인프라, 규제가 단계적으로 진화하며 자율주행의 완전한 상용화로 나아간다.]
)

이 로드맵에서 볼 수 있듯이, 자율주행 기술은 점진적이면서도 가속화되는 발전 경로를 따를 것으로 예상된다. 2025-2026년은 기술 기반의 확립과 표준화 시기이며, 2027-2028년은 상용화의 돌파구를 마련하는 시기이다. 2029-2030년에는 Robotaxi가 실질적인 교통 수단으로 자리잡으며, 2030년 이후에는 모빌리티 생태계 전체가 근본적으로 재편될 것이다. 각 단계마다 기술적 과제와 비기술적 과제가 혼재하며, 이들을 균형있게 해결하는 것이 성공의 열쇠가 될 것이다.

#pagebreak()

= 결론

본 보고서는 End-to-End 자율주행 기술의 현황과 미래를 포괄적으로 분석하였다. 전통적인 모듈식 아키텍처에서 통합적인 End-to-End 학습으로의 패러다임 전환은 단순한 기술적 변화가 아니라, 자율주행 시스템을 설계하고 개발하는 근본적인 철학의 변화를 의미한다. 모방 학습, 강화학습, 인간 피드백 기반 학습 등 다양한 학습 방법론이 발전하고 있으며, 이들의 조합을 통해 인간 수준을 넘어서는 주행 성능을 달성할 가능성이 열리고 있다.

Foundation Models와 생성 AI의 융합은 자율주행 기술에 새로운 지평을 열었다. Vision Transformer는 대규모 사전 학습을 통해 강력한 시각 표현을 학습하며, 적은 데이터로도 새로운 상황에 빠르게 적응할 수 있게 한다. World Models는 미래를 예측하고 시뮬레이션하여 더 안전하고 지능적인 의사결정을 가능하게 한다. Diffusion Models는 현실적인 시나리오를 생성하여 드문 상황에 대한 대응 능력을 향상시킨다. Bird's Eye View 표현과 Query-based 아키텍처는 다중 센서 정보를 효과적으로 통합하고 3차원 공간을 이해하는 능력을 크게 개선했다.

글로벌 산업 생태계는 빠르게 진화하고 있다. Tesla는 vision-only 접근과 완전한 End-to-End 아키텍처로 독자적인 길을 걷고 있으며, 수백만 대의 플릿 데이터를 활용하여 지속적으로 시스템을 개선하고 있다. Waymo는 Level 4 Robotaxi 서비스를 여러 도시로 확장하며, 상업적 자율주행의 실현 가능성을 입증하고 있다. 중국 시장은 정부 지원과 대규모 투자, 빠른 상용화로 세계 자율주행 산업을 선도하고 있다. 한국은 후발 주자로서 인프라와 데이터 측면에서 도전에 직면해 있지만, 제조 역량과 반도체 기술을 바탕으로 기회를 모색하고 있다.

기술적으로는 여전히 해결해야 할 과제들이 남아 있다. Long-tail 이벤트 처리, 안전성의 통계적 검증, 새로운 환경에 대한 일반화, 실시간 처리를 위한 컴퓨팅 효율성 등은 상용화를 위해 극복해야 할 핵심 과제이다. 비기술적으로는 규제 프레임워크의 정립, 윤리적 의사결정 기준 마련, 대중의 신뢰 구축, 일자리 전환에 대한 사회적 대응 등이 필요하다.

향후 5년간 자율주행 기술은 급속히 발전할 것으로 전망된다. 2025-2026년에는 NOA 기능이 중급 차량에도 보급되고, Foundation Models가 표준 기술로 자리잡을 것이다. 2027-2028년에는 도시 전역에서 Level 3/Level 4 자율주행이 상용화되고, World Models 기반 planning이 핵심 기술로 부상할 것이다. 2029-2030년에는 제한된 영역에서 완전 무인 자율주행이 실현되고, Robotaxi의 경제성이 입증될 것이다. 이를 통해 교통사고가 감소하고, 모빌리티 접근성이 향상되며, 도시 구조와 생활 방식이 근본적으로 변화할 것이다.

자율주행의 미래는 단순히 운전의 자동화를 넘어, 더 안전하고 효율적이며 지속 가능한 교통 시스템을 실현하는 것이다. 기술 개발뿐만 아니라 사회적 합의, 규제 정비, 인프라 투자가 함께 이루어져야 진정한 자율주행 시대가 열릴 것이다. 한국은 이러한 글로벌 경쟁에서 기술력과 제조 역량을 바탕으로 독자적인 경쟁력을 확보할 수 있을 것으로 기대된다.
