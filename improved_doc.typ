#set page(
  paper: "a4",
  margin: (x: 2.2cm, y: 2.5cm),
  numbering: "1",
)

#set text(
  font: "Linux Libertine",
  size: 11pt,
  lang: "ko",
)

#set par(
  justify: true,
  leading: 0.75em,
  first-line-indent: 1em,
)

#set heading(numbering: "1.1")

#show heading.where(level: 1): it => {
  pagebreak(weak: true)
  v(1.5em)
  block(
    width: 100%,
    fill: rgb("#1e40af"),
    inset: 18pt,
    radius: 3pt,
    text(fill: white, size: 22pt, weight: "bold")[
      #counter(heading).display() #h(0.5em) #it.body
    ]
  )
  v(1.2em)
}

#show heading.where(level: 2): it => {
  v(1.2em)
  block(
    width: 100%,
    fill: rgb("#dbeafe"),
    inset: 12pt,
    radius: 2pt,
    text(fill: rgb("#1e40af"), size: 15pt, weight: "bold")[
      #counter(heading).display() #h(0.5em) #it.body
    ]
  )
  v(0.8em)
}

#show heading.where(level: 3): it => {
  v(1em)
  text(size: 13pt, fill: rgb("#1e40af"), weight: "bold")[
    #counter(heading).display() #h(0.5em) #it.body
  ]
  v(0.5em)
}

// 제목 페이지
#align(center)[
  #v(4cm)

  #block(
    width: 90%,
    fill: rgb("#1e40af"),
    inset: 25pt,
    radius: 5pt,
  )[
    #text(size: 32pt, weight: "bold", fill: white)[
      End-to-End 자율주행 기술의\
      현황과 미래 전망
    ]
  ]

  #v(1.5cm)

  #text(size: 18pt, fill: rgb("#334155"), weight: "semibold")[
    Foundation Models와 생성 AI 기반 자율주행 시스템의\
    기술적 진화와 산업 생태계 분석
  ]

  #v(3cm)

  #block(
    width: 70%,
    stroke: 1pt + rgb("#cbd5e1"),
    inset: 15pt,
    radius: 3pt,
  )[
    #text(size: 16pt, weight: "bold")[
      한국전자통신연구원 (KETI)
    ]

    #v(0.5em)

    #text(size: 13pt, fill: rgb("#64748b"))[
      지능형 모빌리티 연구센터
    ]
  ]

  #v(1cm)

  #text(size: 14pt, fill: rgb("#64748b"))[
    2025년 10월
  ]

  #v(2cm)

  #align(left)[
    #block(
      width: 100%,
      fill: rgb("#f8fafc"),
      inset: 15pt,
      radius: 3pt,
    )[
      #text(size: 10pt, fill: rgb("#475569"))[
        *요약*: 본 보고서는 자율주행 기술의 최신 패러다임인 End-to-End 학습 방식과 Foundation Models의 융합을 중심으로, 글로벌 자율주행 산업의 기술적 발전 현황과 주요 기업들의 상용화 전략을 종합적으로 분석한다. 특히 Tesla의 FSD, Waymo의 Robotaxi, 그리고 중국 시장의 급속한 발전을 심층 분석하고, 2025년부터 2030년까지의 기술 로드맵과 산업 전망을 제시한다.
      ]
    ]
  ]
]

#pagebreak()

// 목차
#outline(
  title: [목차],
  depth: 3,
  indent: auto,
)

#pagebreak()

= 서론

== 연구 배경 및 목적

자율주행 기술은 21세기 교통 혁명의 핵심으로 부상하고 있다. 지난 수십 년간 자율주행 시스템은 규칙 기반 알고리즘과 모듈식 아키텍처를 중심으로 발전해왔으나, 최근 인공지능 기술의 비약적 발전과 함께 End-to-End 학습 방식이라는 새로운 패러다임으로 전환되고 있다. 이러한 패러다임 전환은 단순히 기술적 접근 방법의 변화를 넘어, 자율주행 시스템의 근본적인 설계 철학과 개발 방법론의 혁신을 의미한다.

전통적인 모듈식 자율주행 시스템은 인지, 예측, 계획, 제어라는 네 가지 독립적인 모듈로 구성되어 있다. 각 모듈은 명확하게 정의된 입출력 인터페이스를 가지며, 순차적으로 정보를 처리하여 최종적으로 차량 제어 명령을 생성한다. 이러한 접근 방식은 시스템의 각 구성 요소를 독립적으로 개발하고 테스트할 수 있다는 장점이 있으며, 문제 발생 시 디버깅이 상대적으로 용이하다는 특징을 가진다. 또한 각 모듈의 동작을 사람이 이해하고 해석할 수 있어, 안전성 검증과 규제 준수 측면에서도 유리한 점이 많았다.

그러나 모듈식 접근 방식은 근본적인 한계를 내포하고 있다. 첫째, 각 모듈 간의 인터페이스에서 정보 손실이 불가피하게 발생한다. 예를 들어, 인지 모듈이 3차원 환경 정보를 객체 목록으로 축약하여 예측 모듈에 전달할 때, 원본 센서 데이터에 포함되어 있던 미세한 정보들이 소실된다. 둘째, 각 모듈이 독립적으로 최적화되기 때문에 전체 시스템의 관점에서는 차선의 성능을 보일 수 있다. 셋째, 모듈 간 오류가 누적되는 현상이 발생할 수 있으며, 특히 복잡한 도심 환경에서 이러한 문제가 두드러진다.

이와 대조적으로, End-to-End 학습 방식은 센서 입력에서 차량 제어 명령까지의 전체 과정을 하나의 신경망으로 직접 학습한다. 이는 1980년대 말 ALVINN 프로젝트에서 처음 제안된 개념이지만, 당시의 컴퓨팅 성능과 데이터 규모의 한계로 인해 실용화되지 못했다. 그러나 2010년대 중반 이후 딥러닝 기술의 발전, 특히 합성곱 신경망과 Transformer 아키텍처의 등장, 그리고 GPU 컴퓨팅 성능의 급격한 향상과 대규모 주행 데이터의 축적으로 인해 End-to-End 학습이 다시 주목받기 시작했다.

2016년 NVIDIA가 발표한 연구는 End-to-End 학습의 가능성을 보여주는 중요한 전환점이 되었다. 연구팀은 단순한 합성곱 신경망을 사용하여 카메라 이미지로부터 직접 조향각을 예측하는 시스템을 개발했으며, 명시적인 차선 검출이나 경로 계획 없이도 도로 주행이 가능함을 입증했다. 이후 Tesla는 Autopilot 시스템에 End-to-End 학습 요소를 점진적으로 도입하기 시작했으며, 2022년부터 본격적으로 Full Self-Driving 베타 버전에서 순수 비전 기반의 End-to-End 아키텍처를 채택했다.

최근 몇 년간 자율주행 분야에서 가장 주목할 만한 발전은 자연어 처리와 컴퓨터 비전 분야에서 성공을 거둔 Foundation Models의 도입이다. GPT, BERT, CLIP과 같은 대규모 사전 학습 모델들이 다양한 downstream 작업에서 탁월한 성능을 보이면서, 자율주행 분야에서도 유사한 접근 방식이 시도되고 있다. Vision Transformer는 이미지 인식 분야에서 기존 합성곱 신경망을 능가하는 성능을 보였으며, 자율주행의 인지 시스템에도 적용되기 시작했다. 특히 Bird's Eye View 표현 학습과 결합된 Transformer 기반 아키텍처는 다중 카메라의 정보를 효과적으로 통합하고 3차원 공간 이해 능력을 크게 향상시켰다.

생성 AI 기술 또한 자율주행 분야에 혁신을 가져오고 있다. World Models는 자율주행 시스템이 미래의 환경 변화를 예측하고 시뮬레이션할 수 있게 해주며, 이는 더 안전하고 지능적인 의사결정을 가능하게 한다. Diffusion Models는 현실적인 주행 시나리오를 생성하여 테스트 데이터를 증강하고, 드물게 발생하는 위험 상황에 대한 대응 능력을 향상시킨다. Wayve의 GAIA-1, NVIDIA의 Neural World Model과 같은 연구들은 생성 AI가 자율주행에서 단순한 보조 도구를 넘어 핵심 구성 요소가 될 수 있음을 보여주고 있다.

본 보고서는 이러한 기술적 진화의 맥락에서 End-to-End 자율주행 시스템의 현황을 종합적으로 분석하고자 한다. 구체적으로, 주요 학습 방법론인 Imitation Learning, Reinforcement Learning, Reinforcement Learning from Human Feedback의 원리와 적용 사례를 살펴보고, Foundation Models와 생성 AI가 자율주행 시스템에 어떻게 통합되고 있는지 분석한다. 또한 글로벌 주요 기업들의 기술 개발 현황과 상용화 전략을 조사하며, 특히 Tesla의 vision-only 접근 방식, Waymo의 Level 4 Robotaxi 서비스, 그리고 중국 시장의 급속한 발전을 중점적으로 다룬다.

더 나아가, 본 보고서는 자율주행 기술이 직면한 핵심 과제들을 식별하고 해결 방향을 제시한다. Long-tail 이벤트 처리, 안전성의 통계적 검증, 새로운 환경에 대한 일반화, 실시간 처리를 위한 컴퓨팅 효율성 등 기술적 과제뿐만 아니라, 규제 프레임워크, 윤리적 의사결정, 사회적 수용성과 같은 비기술적 과제들도 함께 논의한다. 마지막으로, 2025년부터 2030년까지의 기술 로드맵을 제시하고, 자율주행이 교통 시스템과 도시 구조, 나아가 우리의 생활 방식에 미칠 영향을 전망한다.

== 연구 방법 및 범위

본 연구는 문헌 조사, 산업 동향 분석, 기술 벤치마킹을 종합하는 다층적 접근 방식을 채택하였다. 먼저 학술 문헌 조사를 통해 End-to-End 학습의 이론적 기반과 최신 연구 동향을 파악하였다. 주요 학회인 CVPR, ICCV, NeurIPS, ICRA의 최근 3년간 논문들을 중심으로 분석하였으며, 특히 자율주행 관련 워크숍과 튜토리얼 자료를 참고하였다. arXiv 사전공개 서버를 통해 아직 출판되지 않은 최신 연구 동향도 파악하였다.

산업 동향 분석을 위해서는 주요 자율주행 기업들의 공개 자료, 기술 블로그, 투자자 발표 자료, 특허 출원 정보 등을 종합적으로 검토하였다. Tesla, Waymo, Cruise, Baidu Apollo 등 선도 기업들의 기술 개발 방향과 상용화 전략을 분석하였으며, 각 기업의 센서 구성, 컴퓨팅 플랫폼, 소프트웨어 아키텍처를 비교하였다. 또한 Gartner, McKinsey, ARK Invest 등의 산업 분석 보고서를 참고하여 시장 전망과 비즈니스 모델을 평가하였다.

기술 벤치마킹은 공개된 데이터셋과 리더보드를 기반으로 수행하였다. nuScenes, Waymo Open Dataset, Argoverse 등의 벤치마크에서 상위권을 차지한 알고리즘들의 특징을 분석하였으며, 이들의 성능 향상 추이를 통해 기술 발전 속도를 가늠하였다. 오픈소스 프로젝트인 CARLA 시뮬레이터, Apollo 플랫폼, Autoware 등도 참고하여 실제 구현 수준의 기술 세부사항을 파악하였다.

연구의 시간적 범위는 2020년부터 2025년 현재까지의 기술 발전을 중심으로 하되, 필요한 경우 2010년대 초기 연구까지 소급하여 기술 진화의 맥락을 제공하였다. 미래 전망은 2030년까지를 대상으로 하였으며, 이는 대부분의 산업 분석 기관들이 완전 자율주행의 상용화 시점으로 예측하는 시기와 일치한다.

지리적 범위는 전 세계를 대상으로 하되, 특히 미국, 중국, 유럽, 한국을 중점적으로 다루었다. 미국은 Waymo와 Tesla를 중심으로 한 기술 선도 지역으로, 중국은 빠른 상용화와 대규모 시장을 가진 지역으로, 유럽은 엄격한 안전 규제와 전통 자동차 산업의 전환 지역으로, 한국은 후발 주자로서의 도전과 기회를 가진 지역으로 각각 분석하였다.

기술적 범위는 End-to-End 학습을 핵심으로 하되, 이와 밀접하게 연관된 Foundation Models, 생성 AI, Bird's Eye View 표현, Transformer 아키텍처 등을 포괄하였다. 센서 기술, 차량 제어, 통신 기술 등 주변 기술들도 필요한 범위 내에서 다루었으나, 본 보고서의 주된 초점은 소프트웨어와 AI 알고리즘에 있다.

산업 분석은 OEM 자동차 제조사, Tier 1 부품 공급사, 자율주행 전문 기업, Robotaxi 서비스 제공자로 구분하여 각 주체의 역할과 전략을 분석하였다. 또한 자율주행 개발 조직의 구조와 필요 인력, 직무별 요구 역량 등 인적 자원 측면도 조사하였다.

본 보고서는 기술적 정확성과 객관성을 유지하기 위해 노력하였으나, 많은 최신 기술들이 아직 연구 개발 단계에 있고 상용화되지 않았음을 밝혀둔다. 또한 기업들의 내부 기술 정보는 공개되지 않는 경우가 많아, 공개 자료와 간접적 정보를 바탕으로 추론한 부분도 포함되어 있다. 기술 전망과 미래 예측은 현재의 기술 발전 추세와 산업 동향을 바탕으로 한 합리적 추정이지만, 실제 발전 경로는 예상과 다를 수 있다.

#pagebreak()

= End-to-End 자율주행의 기술적 기반

== 패러다임의 전환: 모듈식에서 통합식으로

자율주행 기술의 발전 과정에서 가장 근본적인 변화는 시스템 아키텍처의 패러다임 전환이다. 초기 자율주행 연구는 인간 운전자의 의사결정 과정을 모방하여 인지, 예측, 계획, 제어라는 네 가지 단계로 분해하고 각각을 독립적인 모듈로 구현하는 방식을 택했다. 이러한 모듈식 아키텍처는 2007년 DARPA Urban Challenge에서 우승한 Carnegie Mellon University의 Boss 시스템에서 전형적으로 나타나며, 이후 십여 년간 자율주행 시스템 개발의 표준적인 접근 방식으로 자리잡았다.

모듈식 아키텍처의 첫 번째 단계인 인지 모듈은 카메라, LiDAR, 레이더 등의 센서로부터 원시 데이터를 수집하여 주변 환경을 이해한다. 구체적으로는 차량, 보행자, 자전거, 교통 표지판 등의 객체를 검출하고 분류하며, 차선과 도로 경계를 인식하고, 교통 신호등의 상태를 파악한다. 2010년대 초반에는 주로 수작업으로 설계된 특징 추출기와 지지 벡터 머신, 랜덤 포레스트 같은 전통적 기계학습 알고리즘이 사용되었으나, 2015년 이후부터는 깊은 합성곱 신경망이 인지 모듈의 핵심 기술로 자리잡았다. AlexNet, VGG, ResNet과 같은 ImageNet에서 사전 학습된 네트워크들이 전이 학습을 통해 자율주행에 적용되었으며, YOLO, SSD, Faster R-CNN 같은 실시간 객체 검출 알고리즘들이 개발되었다.

두 번째 단계인 예측 모듈은 인지 모듈이 출력한 객체 정보를 받아 각 객체의 미래 행동을 예측한다. 예를 들어, 앞 차량이 차선을 변경할 것인지, 보행자가 도로를 횡단할 것인지, 교차로에서 다른 차량이 어떤 경로로 진행할 것인지 등을 판단한다. 초기에는 일정 속도 모델이나 칼만 필터와 같은 단순한 물리 기반 모델이 사용되었으나, 점차 다항식 궤적 피팅, 가우시안 프로세스, 그리고 최근에는 순환 신경망과 Transformer 기반의 딥러닝 모델들이 활용되고 있다. 특히 사회적 상호작용을 고려한 예측 모델들이 개발되어, 여러 에이전트가 서로 영향을 주고받는 복잡한 교통 상황에서의 예측 정확도가 크게 향상되었다.

세 번째 단계인 계획 모듈은 예측 결과를 바탕으로 자차의 주행 궤적을 생성한다. 이는 다시 경로 계획과 행동 계획으로 나뉠 수 있다. 경로 계획은 출발지에서 목적지까지의 전체 경로를 결정하는 것으로, 주로 HD 맵과 네비게이션 정보를 활용하여 A-star, Dijkstra와 같은 그래프 탐색 알고리즘으로 해결된다. 행동 계획은 현재 교통 상황에서 어떤 기동을 할 것인지 결정하는 것으로, 유한 상태 기계나 의사결정 트리가 전통적으로 사용되었다. 궤적 생성 단계에서는 장애물 회피, 차선 준수, 승차감, 법규 준수 등의 다양한 제약 조건을 만족하는 최적 궤적을 찾아야 하며, 이를 위해 최적화 기법, 샘플링 기반 계획, 격자 기반 탐색 등이 활용된다.

마지막 단계인 제어 모듈은 계획된 궤적을 실제 차량 동작으로 변환한다. 조향, 가속, 제동 등의 제어 명령을 생성하여 차량이 원하는 궤적을 정확히 추종하도록 한다. PID 제어기, 모델 예측 제어, 선형 이차 제어 등의 고전 제어 이론이 주로 사용되며, 차량의 동역학 모델을 기반으로 현재 상태와 목표 상태의 차이를 최소화한다.

모듈식 아키텍처는 몇 가지 명확한 장점을 가진다. 첫째, 각 모듈을 독립적으로 개발하고 테스트할 수 있어 개발 프로세스가 체계적이고 효율적이다. 여러 팀이 동시에 다른 모듈을 개발할 수 있으며, 특정 모듈의 성능 문제를 격리하여 개선할 수 있다. 둘째, 시스템의 동작을 인간이 이해하고 해석할 수 있다. 각 모듈의 중간 출력을 검사하여 시스템이 환경을 어떻게 인식하고 어떤 추론을 거쳐 결정을 내렸는지 추적할 수 있다. 이는 디버깅과 안전성 검증에 매우 유용하다. 셋째, 도메인 지식을 효과적으로 통합할 수 있다. 물리 법칙, 교통 규칙, 인간 운전 습관 등의 선행 지식을 각 모듈에 명시적으로 인코딩할 수 있다.

그러나 모듈식 아키텍처는 근본적인 한계를 가지고 있으며, 이는 시스템 성능의 천장을 만든다. 가장 심각한 문제는 모듈 간 인터페이스에서 발생하는 정보 손실이다. 인지 모듈은 고차원의 센서 데이터를 객체 목록이라는 저차원 표현으로 압축한다. 예를 들어, 1920x1080 해상도의 이미지는 약 200만 픽셀의 정보를 담고 있지만, 이를 처리한 인지 모듈의 출력은 수십 개의 bounding box와 클래스 레이블로 요약된다. 이 과정에서 미세한 질감, 조명 조건, 깊이의 불확실성 등 많은 정보가 소실되며, 예측과 계획 모듈은 이 축약된 정보만으로 의사결정을 해야 한다.

두 번째 문제는 오류의 누적이다. 인지 모듈이 차량을 잘못 검출하거나 분류하면, 예측 모듈은 잘못된 정보를 바탕으로 미래를 예측하고, 계획 모듈은 잘못된 예측을 바탕으로 위험한 궤적을 생성할 수 있다. 각 모듈의 오류가 연쇄적으로 증폭되어 최종 성능에 치명적인 영향을 미칠 수 있다. 특히 드물게 발생하는 복잡한 상황에서 이러한 문제가 두드러진다.

세 번째 문제는 부분 최적화로 인한 전체 성능 저하이다. 각 모듈은 자신만의 목적 함수를 최적화하도록 학습되거나 설계된다. 인지 모듈은 검출 정확도를 최대화하고, 예측 모듈은 예측 오차를 최소화하며, 계획 모듈은 궤적의 부드러움과 안전성을 최적화한다. 그러나 전체 시스템의 목표는 안전하고 효율적이며 편안한 주행을 달성하는 것이며, 이는 개별 모듈의 목표와 다를 수 있다. 예를 들어, 인지 모듈이 모든 객체를 완벽하게 검출하는 것보다 주행에 중요한 객체만 정확히 검출하는 것이 더 유용할 수 있다.

네 번째 문제는 확장성의 한계이다. 새로운 센서를 추가하거나, 새로운 유형의 객체를 인식하거나, 새로운 주행 시나리오를 처리하려면 여러 모듈을 동시에 수정해야 한다. 인터페이스를 변경하면 연결된 모든 모듈에 영향을 미치므로, 시스템의 진화가 어렵다.

이러한 모듈식 아키텍처의 한계를 극복하기 위해 End-to-End 학습 방식이 제안되었다. End-to-End 접근은 센서 입력에서 제어 출력까지의 전체 매핑을 하나의 신경망으로 직접 학습한다. 중간 단계의 명시적인 표현 없이, 데이터로부터 직접 입출력 관계를 학습하는 것이다. 이는 1989년 Pomerleau가 제안한 ALVINN 시스템에서 처음 시도되었으나, 당시의 제한된 컴퓨팅 성능과 작은 데이터셋으로 인해 간단한 도로에서만 작동했다.

End-to-End 학습이 다시 주목받게 된 것은 2010년대 중반, 딥러닝의 부흥과 함께였다. 2016년 NVIDIA의 연구는 단 9층의 합성곱 신경망으로 카메라 이미지에서 조향각을 직접 예측할 수 있음을 보였다. 네트워크는 72시간의 인간 주행 데이터로 학습되었으며, 명시적인 차선 검출이나 경로 계획 없이도 다양한 도로에서 주행할 수 있었다. 더 놀라운 점은 네트워크가 스스로 유용한 중간 표현을 학습했다는 것이다. 네트워크 내부의 활성화를 시각화한 결과, 초기 층은 에지와 텍스처를 검출하고, 중간 층은 차선과 도로 경계를 감지하며, 후기 층은 주행 가능한 영역을 표현하는 것으로 나타났다. 이는 명시적으로 지도하지 않았음에도 네트워크가 주행에 유용한 특징들을 자동으로 학습했음을 의미한다.

End-to-End 아키텍처의 핵심 장점은 전체 시스템을 하나의 목적 함수로 최적화할 수 있다는 것이다. 주행 성능을 직접 측정하여 역전파 알고리즘으로 네트워크의 모든 파라미터를 업데이트한다. 이는 부분 최적화의 문제를 해결하며, 시스템의 모든 구성 요소가 최종 목표를 향해 협력하도록 만든다. 또한 모듈 간 인터페이스가 없으므로 정보 손실이 최소화된다. 센서 데이터의 모든 정보가 네트워크를 통해 흐르며, 각 결정에 필요한 정보를 직접 추출할 수 있다.

두 번째 장점은 데이터 기반 학습을 통한 암묵적 지식의 획득이다. 인간이 명시적으로 프로그래밍하기 어려운 복잡한 패턴과 규칙을 데이터로부터 자동으로 학습한다. 예를 들어, 다른 운전자의 의도를 미묘한 단서로부터 추론하거나, 복잡한 교차로에서의 우선순위를 암묵적으로 학습할 수 있다. 이는 수백만 마일의 주행 데이터에서 추출된 집단 지성을 활용하는 것이다.

세 번째 장점은 확장성이다. 더 많은 데이터와 더 큰 모델, 더 강력한 컴퓨팅 자원을 투입하면 성능이 지속적으로 향상되는 스케일링 법칙을 따른다. 이는 자연어 처리의 GPT 시리즈나 컴퓨터 비전의 Vision Transformer에서 입증되었으며, 자율주행에서도 유사한 현상이 관찰되고 있다. Tesla는 지속적으로 플릿 데이터를 수집하고 모델 크기를 증가시키며 성능을 개선하고 있으며, 이는 모듈식 시스템에서는 달성하기 어려운 지속적 진화이다.

물론 End-to-End 아키텍처에도 도전 과제가 있다. 가장 큰 문제는 해석 가능성의 부족이다. 네트워크가 왜 특정 결정을 내렸는지 이해하기 어렵다. 수백만 개의 파라미터로 구성된 블랙박스 시스템은 디버깅이 어렵고, 안전성을 검증하기 힘들다. 규제 기관과 보험 회사는 설명 가능한 시스템을 선호하므로, 이는 상용화의 장벽이 될 수 있다. 두 번째 문제는 대규모 고품질 데이터가 필요하다는 것이다. 모듈식 시스템은 각 모듈을 개별적으로 레이블된 데이터로 학습시킬 수 있지만, End-to-End 시스템은 입출력 쌍 전체가 필요하다. 드문 상황의 데이터를 충분히 수집하는 것은 어렵고 비용이 많이 든다.

현재의 추세는 순수한 End-to-End 시스템과 모듈식 시스템의 중간 지점을 탐색하는 것이다. 하이브리드 아키텍처는 End-to-End 학습의 장점을 취하면서도 일부 해석 가능성과 안전성 보장을 유지한다. 예를 들어, 인지와 예측은 End-to-End로 학습하되, 계획은 명시적인 안전 제약을 포함하는 최적화 문제로 정식화할 수 있다. 또는 전체 시스템을 End-to-End로 학습하되, 중간 표현을 명시적으로 감독하여 해석 가능성을 높일 수 있다. Tesla의 FSD는 이러한 하이브리드 접근의 대표적인 예이다. 시스템은 End-to-End 신경망을 핵심으로 하지만, 중간에 명시적인 3D 공간 표현과 객체 리스트를 생성하며, 최종 계획 단계에서는 물리적 제약과 안전 규칙을 명시적으로 확인한다.

향후 몇 년간 End-to-End 아키텍처는 더욱 정교해지고 널리 채택될 것으로 예상된다. Foundation Models의 발전과 함께, 대규모 사전 학습과 fine-tuning을 통해 데이터 효율성이 개선될 것이다. 설명 가능 AI 기법의 발전으로 블랙박스의 내부를 들여다보고 신뢰성을 높일 수 있을 것이다. 그리고 시뮬레이션 기술의 발전으로 드문 상황의 데이터를 합성하여 학습에 활용할 수 있을 것이다. 모듈식에서 End-to-End로의 패러다임 전환은 단순한 기술적 변화가 아니라, 자율주행 시스템을 설계하고 개발하는 근본적인 철학의 변화를 의미한다.

== 학습 방법론의 진화

End-to-End 자율주행 시스템을 학습시키는 방법론은 크게 세 가지로 분류할 수 있다. 모방 학습, 강화학습, 그리고 인간 피드백 기반 강화학습이다. 각 방법론은 고유한 장단점을 가지고 있으며, 실제 시스템에서는 이들을 조합하여 사용하는 경우가 많다.

모방 학습은 가장 직관적이고 널리 사용되는 방법이다. 기본 아이디어는 전문가인 인간 운전자의 행동을 관찰하고 이를 모방하는 정책을 학습하는 것이다. 수학적으로는 감독 학습 문제로 정식화되며, 입력은 센서 데이터, 출력은 조향각과 가속 페달 값이다. 학습 데이터는 인간 운전자가 실제로 주행한 로그에서 수집되며, 신경망은 주어진 센서 입력에서 인간이 취한 행동을 예측하도록 학습된다.

모방 학습의 가장 단순한 형태는 행동 복제이다. 데이터셋의 각 샘플에 대해 신경망의 예측과 인간의 실제 행동 사이의 오차를 계산하고, 이를 최소화하도록 네트워크 파라미터를 업데이트한다. 이는 일반적인 감독 학습과 동일하며, 구현이 간단하고 학습이 안정적이다. 대규모 데이터셋이 있다면 상당히 좋은 성능을 달성할 수 있다. Waymo는 수천만 마일의 주행 데이터를 수집했으며, 이를 바탕으로 모방 학습으로 강력한 초기 정책을 획득했다.

그러나 행동 복제는 분포 이동 문제로 인해 한계를 가진다. 학습 데이터는 인간 운전자가 만든 상태 분포를 따른다. 즉, 인간이 주로 경험하는 상황들로 구성되어 있다. 그러나 학습된 정책이 실제로 주행할 때는 작은 오류로 인해 학습 데이터와 다른 상태에 놓일 수 있다. 예를 들어, 정책이 도로의 중앙에서 약간 벗어나면, 이는 인간 운전자가 거의 경험하지 않는 상태이므로 학습 데이터에 충분한 예시가 없다. 정책은 이 상황에서 어떻게 행동해야 할지 모르고 더 큰 오류를 만들 수 있으며, 이는 누적되어 궁극적으로 실패로 이어진다.

이 문제를 해결하기 위해 데이터셋 집계 알고리즘이 제안되었다. 핵심 아이디어는 학습된 정책이 만드는 상태 분포에서 추가 데이터를 수집하는 것이다. 구체적인 절차는 다음과 같다. 먼저 초기 정책을 인간 데이터로 학습한다. 이 정책을 실제 환경이나 시뮬레이션에서 실행하여 새로운 상태들을 경험하게 한다. 이 새로운 상태들에서 인간 전문가가 어떻게 행동했을지 레이블을 달거나, 실제로 인간이 제어권을 가져와 행동을 기록한다. 이 새로운 데이터를 기존 데이터셋에 추가하고, 확장된 데이터셋으로 정책을 재학습한다. 이 과정을 여러 번 반복하면, 정책은 자신이 실제로 경험할 상태 분포에서 학습하게 되므로 분포 이동 문제가 완화된다.

데이터셋 집계는 이론적으로 우수하지만 실용적인 어려움이 있다. 매 반복마다 인간 전문가가 새로운 상태에 레이블을 달아야 하므로 비용이 많이 든다. 또한 안전성 문제가 있다. 학습 중인 정책은 아직 완벽하지 않으므로, 실제 차량에서 실행하면 위험할 수 있다. 따라서 시뮬레이션 환경에서 데이터셋 집계를 수행하는 것이 일반적이지만, 시뮬레이션과 실제의 차이로 인해 전이 문제가 발생할 수 있다.

모방 학습의 또 다른 발전은 조건부 모방 학습이다. 기본 모방 학습은 동일한 상황에서도 인간이 다른 행동을 취할 수 있다는 점을 고려하지 못한다. 예를 들어, 교차로에서 직진, 좌회전, 우회전 모두 가능하며, 어떤 행동을 택할지는 목적지에 따라 달라진다. 조건부 모방 학습은 고수준 명령을 추가 입력으로 받아 이를 해결한다. 네트워크는 센서 데이터와 함께 네비게이션 명령을 받아, 해당 명령에 맞는 행동을 출력한다. 이는 정책의 다중 모드성을 명시적으로 모델링하며, 동일한 네트워크로 다양한 주행 기동을 수행할 수 있게 한다.

강화학습은 모방 학습의 대안적 접근이다. 인간의 시연을 직접 모방하는 대신, 보상 신호를 통해 좋은 행동과 나쁜 행동을 구별하고 시행착오를 통해 학습한다. 에이전트는 환경과 상호작용하며, 각 행동에 대한 보상을 받는다. 목표는 장기적인 누적 보상을 최대화하는 정책을 찾는 것이다. 강화학습은 이론적으로 최적 정책을 학습할 수 있으며, 인간보다 더 나은 성능을 달성할 가능성도 있다.

자율주행에서 강화학습을 적용하는 핵심 과제는 보상 함수를 설계하는 것이다. 보상 함수는 시스템이 무엇을 최적화해야 하는지 정의한다. 자율주행의 목표는 안전하고 효율적이며 편안한 주행을 달성하는 것이지만, 이를 수학적 함수로 정확히 표현하기는 어렵다. 보상 함수는 여러 요소의 가중합으로 구성되는 경우가 많다. 안전성을 위해서는 충돌 시 큰 음의 보상을 주고, 다른 차량이나 장애물과의 거리가 가까울수록 음의 보상을 준다. 효율성을 위해서는 목표 속도를 유지하거나 목적지에 빠르게 도달할수록 양의 보상을 준다. 승차감을 위해서는 급격한 조향이나 가감속에 음의 보상을 준다. 또한 교통 규칙을 준수하도록 차선 이탈, 신호 위반 등에 벌점을 부여한다.

보상 함수 설계의 어려움은 이들 요소 간의 상충 관계에 있다. 예를 들어, 안전을 극대화하려면 매우 느리게 주행하거나 아예 멈춰야 하지만, 이는 효율성과 배치된다. 승차감을 위해 부드럽게 주행하면 긴급 상황에서 충돌을 피하지 못할 수 있다. 각 요소의 가중치를 어떻게 설정할지는 주관적이며, 잘못된 가중치는 바람직하지 않은 행동을 유도할 수 있다. 보상 해킹 문제도 발생할 수 있다. 에이전트가 보상 함수의 허점을 찾아 의도와 다른 방식으로 보상을 최대화하는 것이다.

강화학습의 또 다른 과제는 샘플 효율성이다. 에이전트는 많은 시행착오를 통해 학습하며, 좋은 정책을 얻기 위해 수백만 번의 상호작용이 필요할 수 있다. 시뮬레이션에서는 빠르게 많은 샘플을 생성할 수 있지만, 실제 차량에서는 불가능하다. 또한 안전성 문제로 인해 실제 환경에서 무작위 탐험을 하기 어렵다. 학습 초기의 정책은 성능이 좋지 않으므로, 실제로 주행하면 사고를 일으킬 위험이 크다.

이러한 문제들을 완화하기 위해 여러 전략이 사용된다. 첫째, 시뮬레이션에서 먼저 학습하고 실제 환경으로 전이한다. CARLA, LGSVL과 같은 고품질 시뮬레이터에서 수백만 번의 주행을 시뮬레이션하여 초기 정책을 학습한 후, 실제 환경에서 fine-tuning한다. 시뮬레이션과 실제의 차이를 줄이기 위해 도메인 랜덤화 기법을 사용한다. 시뮬레이션의 시각적 외관, 물리 파라미터, 센서 노이즈 등을 무작위로 변화시켜 다양성을 늘리면, 학습된 정책이 실제 환경에 더 잘 전이된다.

둘째, 모델 기반 강화학습을 사용하여 샘플 효율성을 높인다. 환경의 동역학 모델을 학습하고, 이 모델 내에서 계획하거나 추가 학습을 수행한다. 실제 환경과의 상호작용으로 모델을 학습하고, 학습된 모델로 시뮬레이션된 경험을 생성하여 정책을 개선한다. 이는 제한된 실제 데이터를 효율적으로 활용할 수 있게 한다.

셋째, 오프라인 강화학습을 활용한다. 이는 온라인 상호작용 없이 과거에 수집된 데이터셋만으로 정책을 학습하는 방법이다. 인간이나 다른 정책이 수집한 주행 로그를 사용하여 강화학습 알고리즘을 적용한다. 이는 안전하며, 기존 데이터를 재활용할 수 있다는 장점이 있다. 그러나 데이터셋이 커버하지 않는 상태-행동 쌍에 대한 학습이 어렵고, 분포 밖 일반화 문제가 발생할 수 있다.

최근에는 모방 학습과 강화학습을 결합하는 방법이 주목받고 있다. 모방 학습으로 인간 수준의 초기 정책을 빠르게 획득한 후, 강화학습으로 이를 더욱 개선하는 것이다. 이는 두 방법의 장점을 취하며, 인간을 뛰어넘는 성능을 달성할 가능성을 제공한다.

인간 피드백 기반 강화학습은 보상 함수 설계의 어려움을 우회하는 방법이다. 명시적인 보상 함수를 엔지니어가 작성하는 대신, 인간의 선호도로부터 보상 모델을 학습한다. 절차는 두 단계로 구성된다. 첫 번째 단계에서는 여러 주행 궤적 쌍을 인간에게 보여주고, 어느 쪽이 더 나은지 평가하게 한다. 예를 들어, 두 개의 교차로 통과 영상을 보여주고 어느 쪽이 더 안전하고 부드러운지 선택하게 한다. 이러한 비교 데이터를 수집하여 보상 모델을 학습한다. 보상 모델은 주행 궤적을 입력으로 받아 점수를 출력하는 신경망이며, 인간이 선호하는 궤적에 더 높은 점수를 부여하도록 학습된다.

두 번째 단계에서는 학습된 보상 모델을 사용하여 강화학습을 수행한다. 에이전트는 환경과 상호작용하며, 각 행동에 대해 보상 모델로부터 보상을 받는다. 이 보상을 최대화하도록 정책을 학습한다. 학습이 진행됨에 따라 새로운 정책 행동에 대한 인간 피드백을 추가로 수집하여 보상 모델을 개선할 수 있다.

인간 피드백 기반 강화학습의 장점은 복잡하고 주관적인 목표를 다룰 수 있다는 것이다. 승차감, 자연스러움, 공손함 같은 개념은 수식으로 정의하기 어렵지만, 인간은 직관적으로 평가할 수 있다. 또한 인간의 암묵적 지식을 시스템에 전달할 수 있다. 교통 예절, 문화적 차이 등 명시적으로 표현되지 않은 규범들을 학습할 수 있다.

그러나 인간 피드백 수집에는 비용이 든다. 수천 또는 수만 쌍의 비교가 필요하며, 각각에 대해 인간이 신중하게 평가해야 한다. 또한 인간 평가자 간의 불일치, 평가자의 피로와 실수, 그리고 평가 기준의 모호함 등이 노이즈를 만든다. 능동 학습 기법을 사용하여 효율성을 높일 수 있다. 시스템은 가장 정보가 많은 비교 쌍, 즉 모델의 불확실성이 큰 경우를 우선적으로 인간에게 질의한다.

자율주행 분야에서 인간 피드백 기반 강화학습의 적용은 아직 초기 단계이지만, 유망한 결과들이 보고되고 있다. Wayve는 주행 스타일을 개선하기 위해 이 방법을 사용했다. 기술적으로 안전하지만 승객에게 불편함을 주는 행동들을 인간 피드백을 통해 식별하고 개선했다. 예를 들어, 너무 급격한 브레이크, 불필요하게 넓은 회전, 소심한 합류 등을 인간 평가자가 지적하면, 시스템은 이를 학습하여 더 자연스러운 주행 스타일을 획득했다.

게임 이론 기반 강화학습은 다중 에이전트 환경에서 상호작용을 고려하는 방법이다. 자율주행 차량은 혼자 주행하는 것이 아니라 다른 차량, 보행자, 자전거 등과 도로를 공유한다. 이들 각각은 자신의 목표를 가지고 행동하며, 서로 영향을 주고받는다. 단순히 다른 에이전트를 환경의 일부로 취급하면, 그들의 적응적 행동을 놓칠 수 있다.

게임 이론은 전략적 상호작용을 분석하는 수학적 틀을 제공한다. 내쉬 균형 개념은 각 에이전트가 다른 에이전트의 전략이 주어졌을 때 자신의 전략을 최적화한 상태를 나타낸다. 자율주행에서는 교차로 통과, 차선 변경, 합류 등의 상황이 게임으로 모델링될 수 있다. 각 차량은 안전하게 목표에 도달하려 하며, 다른 차량의 행동을 예측하고 이에 최적으로 대응한다.

레벨-k 추론은 인간의 제한된 합리성을 모델링하는 접근이다. 레벨-0 에이전트는 단순한 휴리스틱을 따르며, 다른 에이전트를 고려하지 않는다. 레벨-1 에이전트는 다른 에이전트가 레벨-0이라고 가정하고 최적 대응을 한다. 레벨-k 에이전트는 다른 에이전트가 레벨-(k-1)이라고 가정한다. 실제 인간 운전자는 보통 레벨-1이나 레벨-2 정도의 추론을 하는 것으로 알려져 있다. 자율주행 시스템이 이를 모델링하면, 인간 운전자와 더 자연스럽게 상호작용할 수 있다.

역 강화학습은 관찰된 행동으로부터 보상 함수를 추론한다. 다른 차량의 주행 로그를 보고, 그 차량이 암묵적으로 최적화하고 있는 목표가 무엇인지 역으로 계산한다. 예를 들어, 한 차량이 일관되게 빠른 속도를 유지하면서도 안전 거리를 확보한다면, 그 차량은 속도와 안전의 균형을 중시하는 보상 함수를 가진 것으로 추정할 수 있다. 이렇게 추론된 보상 함수를 사용하여 그 차량의 미래 행동을 예측할 수 있다.

다중 에이전트 강화학습 알고리즘들도 개발되고 있다. QMIX는 여러 에이전트의 Q-함수를 결합하여 중앙집중식으로 학습하되, 실행 시에는 각 에이전트가 독립적으로 행동한다. MADDPG는 중앙집중식 비평가와 분산 행위자를 사용하여 협력과 경쟁이 혼재된 환경에서 학습한다. 이러한 알고리즘들은 시뮬레이션 환경에서 흥미로운 행동 패턴을 보여주었으며, 실제 자율주행에 적용하는 연구가 진행 중이다.

학습 방법론의 선택은 데이터 가용성, 안전 요구사항, 성능 목표에 따라 달라진다. 대규모 인간 주행 데이터가 있다면 모방 학습이 빠르고 안전한 시작점을 제공한다. 더 나은 성능을 추구한다면 강화학습으로 정책을 fine-tuning할 수 있다. 주관적 품질을 중시한다면 인간 피드백을 활용한다. 복잡한 상호작용이 중요하다면 게임 이론적 접근을 고려한다. 실제로는 이들을 조합한 하이브리드 방법이 가장 효과적인 경우가 많으며, 자율주행 기업들은 자신의 상황에 맞는 최적의 조합을 찾기 위해 지속적으로 실험하고 있다.

#pagebreak()

= Foundation Models와 생성 AI의 융합

== Vision Transformer와 대규모 사전학습

컴퓨터 비전 분야는 오랫동안 합성곱 신경망의 지배를 받아왔다. 합성곱 연산은 이미지의 지역적 패턴을 효과적으로 포착하며, 평행 이동 불변성이라는 유용한 귀납적 편향을 제공한다. AlexNet이 2012년 ImageNet 대회에서 압도적 성능을 보인 이후, ResNet, EfficientNet 등 다양한 합성곱 기반 아키텍처들이 발전해왔다. 그러나 2020년 Google Research에서 발표한 Vision Transformer는 이러한 패러다임에 근본적인 도전을 제기했다.

Vision Transformer는 자연어 처리에서 성공을 거둔 Transformer 아키텍처를 이미지 인식에 적용한 것이다. Transformer의 핵심은 self-attention 메커니즘으로, 입력 시퀀스의 모든 위치 간의 관계를 직접 모델링한다. 이미지에 Transformer를 적용하기 위해, 이미지를 작은 패치들로 나누고 이를 시퀀스로 취급한다. 예를 들어, 224×224 이미지를 16×16 패치로 나누면 196개의 패치가 생성되며, 각 패치는 하나의 토큰으로 간주된다. 각 패치는 선형 변환을 통해 고차원 벡터로 임베딩되고, 위치 정보를 인코딩하는 벡터가 추가된다. 이렇게 만들어진 토큰 시퀀스가 Transformer encoder에 입력된다.

Transformer encoder는 여러 층의 self-attention과 feed-forward 네트워크로 구성된다. Self-attention은 각 토큰이 다른 모든 토큰과 상호작용하도록 하여, 이미지 전체의 문맥을 고려할 수 있게 한다. 이는 합성곱의 제한된 수용 영역과 대조적이다. 합성곱 신경망은 여러 층을 쌓아야 전역적 정보를 통합할 수 있지만, Vision Transformer는 단일 attention 층에서도 이미지의 먼 부분들 간의 관계를 직접 포착한다.

Vision Transformer의 초기 결과는 놀라웠다. 충분히 큰 데이터셋에서 학습하면 최고 성능의 합성곱 신경망과 동등하거나 더 나은 성능을 보였다. 특히 JFT-300M이라는 3억 개 이미지의 대규모 데이터셋에서 사전 학습한 Vision Transformer는 ImageNet에서 당시 최고 성능을 기록했다. 더 흥미로운 점은 모델 크기를 키울수록 성능이 지속적으로 향상되는 스케일링 법칙을 보인다는 것이다. 합성곱 신경망은 일정 크기 이상에서 성능 향상이 정체되는 경향이 있지만, Transformer는 더 많은 파라미터와 데이터를 투입할수록 성능이 계속 개선된다.

자율주행 분야에서 Vision Transformer의 도입은 인지 시스템의 성능을 크게 향상시켰다. 먼저 사전 학습의 이점이 크다. ImageNet이나 더 큰 데이터셋에서 사전 학습된 Vision Transformer는 일반적인 시각 표현을 학습하며, 이를 자율주행 작업에 전이할 수 있다. 적은 양의 자율주행 데이터로도 fine-tuning을 통해 좋은 성능을 달성할 수 있다. 이는 특히 드물게 발생하는 상황에서 유용하다. 예를 들어, 눈 오는 날의 주행 데이터는 제한적이지만, 사전 학습된 모델은 일반적인 눈 패턴을 이미 알고 있으므로 빠르게 적응할 수 있다.

둘째, Vision Transformer의 장거리 의존성 모델링 능력은 복잡한 장면 이해에 도움이 된다. 도심 주행에서는 여러 객체들이 서로 상호작용하며, 먼 곳의 교통 신호나 표지판이 현재 행동에 영향을 줄 수 있다. Vision Transformer는 이미지의 서로 다른 부분들 간의 관계를 효과적으로 포착하여, 이러한 복잡한 문맥을 이해할 수 있다.

셋째, Vision Transformer는 다양한 해상도와 종횡비의 이미지를 다루기 용이하다. 자율주행에서는 여러 카메라가 다른 해상도와 시야각을 가질 수 있으며, Vision Transformer는 패치 단위로 처리하므로 이러한 변화에 유연하게 대응한다.

Tesla는 FSD 시스템에서 Vision Transformer를 적극 활용하고 있다. 다중 카메라 이미지를 처리하는 백본 네트워크로 Transformer를 사용하며, 이를 통해 8개 카메라의 정보를 효과적으로 통합한다. 각 카메라 이미지에서 추출된 특징들이 Transformer의 attention 메커니즘을 통해 상호작용하며, 전체 차량 주변의 일관된 표현을 생성한다.

Vision Transformer의 성공은 대규모 사전 학습의 중요성을 재확인시켰다. Foundation Models라는 개념은 범용적인 대규모 모델을 먼저 학습하고, 이를 다양한 downstream 작업에 적용한다는 아이디어이다. GPT-3, BERT와 같은 언어 모델들이 이 접근의 효과를 입증했으며, 이제 비전과 자율주행에서도 동일한 패러다임이 자리잡고 있다.

CLIP은 Vision과 Language를 결합한 Foundation Model의 대표적 예이다. OpenAI가 개발한 CLIP은 4억 개의 이미지-텍스트 쌍으로 학습되었으며, 이미지와 텍스트를 공동 임베딩 공간에 매핑한다. 학습은 contrastive learning 방식으로 이루어진다. 매칭되는 이미지-텍스트 쌍은 임베딩 공간에서 가깝게, 그렇지 않은 쌍은 멀게 배치되도록 학습한다. 이를 통해 CLIP은 시각적 개념과 언어적 설명 간의 연결을 학습한다.

CLIP의 흥미로운 특성은 zero-shot 일반화 능력이다. 학습 중에 보지 못한 새로운 객체 클래스도 텍스트 설명만 제공하면 인식할 수 있다. 예를 들어, CLIP이 "공사 중인 도로"라는 개념을 명시적으로 학습하지 않았더라도, 이 텍스트 설명과 유사한 이미지를 찾을 수 있다. 자율주행에서는 다양하고 예측 불가능한 상황을 마주치므로, 이러한 zero-shot 능력이 매우 유용하다.

자율주행에서 CLIP을 활용하는 방법은 여러 가지이다. 첫째, 이상 상황 감지에 사용할 수 있다. 정상적인 도로 장면과 다른 이상한 상황을 텍스트 쿼리로 찾을 수 있다. "도로를 막고 있는 쓰레기", "뒤집힌 차량" 같은 드문 상황을 학습 데이터 없이도 인식할 수 있다. 둘째, 사람의 자연어 명령을 이해하는 데 활용할 수 있다. "빨간 건물 앞에 세워주세요"와 같은 지시를 시각 정보와 연결하여 해석한다. 셋째, 플릿 데이터의 자동 레이블링에 사용할 수 있다. 텍스트 쿼리로 특정 유형의 장면을 검색하여, 레이블링 비용을 크게 줄일 수 있다.

최근에는 자율주행 전용 Foundation Models를 개발하려는 시도들이 나타나고 있다. 일반적인 이미지가 아닌 대규모 주행 영상으로 사전 학습하는 것이다. Waymo는 자사의 방대한 주행 데이터를 활용하여 자율주행 특화 사전 학습을 수행하고 있다. 수천만 마일의 주행 로그에서 추출한 수십억 프레임의 이미지와 LiDAR 포인트 클라우드로 대규모 모델을 학습한다. 이러한 모델은 자율주행에 특화된 표현을 학습하며, 일반 이미지에서 사전 학습한 모델보다 더 나은 성능을 보인다.

Self-supervised learning은 레이블 없이 데이터로부터 표현을 학습하는 방법이다. 주행 데이터는 풍부하지만, 모든 프레임에 정확한 레이블을 다는 것은 비용이 많이 든다. Self-supervised learning은 데이터 자체로부터 감독 신호를 만들어낸다. Masked image modeling은 이미지의 일부를 가리고 나머지로부터 가려진 부분을 복원하도록 학습한다. 이는 BERT의 masked language modeling과 유사한 아이디어이다. 모델은 문맥을 이용하여 빠진 정보를 추론하는 능력을 획득하며, 이는 downstream 작업에 유용한 표현을 제공한다.

Contrastive learning은 유사한 샘플들을 가깝게, 다른 샘플들을 멀게 배치하도록 학습한다. 자율주행에서는 시간적으로 가까운 프레임들이 유사한 장면을 담고 있으므로, 이들을 positive pair로 사용할 수 있다. 또한 데이터 증강 기법을 사용하여 동일한 장면의 다른 view를 생성하고, 이들을 유사하게 만들도록 학습할 수 있다.

Temporal consistency는 연속된 프레임 간의 일관성을 활용한다. 차량이 부드럽게 움직이므로, 연속 프레임의 특징 표현도 부드럽게 변해야 한다. 이 제약을 학습 목적함수에 추가하여 시간적으로 안정적인 표현을 학습한다. 또한 광학 흐름이나 차량의 자세 변화 정보를 사용하여, 연속 프레임 간의 대응 관계를 학습할 수 있다.

Multi-modal pre-training은 여러 센서의 데이터를 함께 활용하여 학습한다. 카메라, LiDAR, 레이더, GPS, IMU 등의 센서는 서로 보완적인 정보를 제공한다. 카메라는 시각적 세부사항과 색상 정보를 제공하고, LiDAR는 정확한 3차원 거리 정보를 제공하며, 레이더는 속도 정보와 악천후 강건성을 제공한다. 이들을 통합하여 학습하면 각 센서의 장점을 결합한 강력한 표현을 얻을 수 있다.

BEVFormer는 이러한 다중 모드 사전 학습의 예이다. 여러 카메라 이미지를 Bird's Eye View 공간으로 변환하며, 이 과정에서 공간적 관계와 3차원 구조를 학습한다. 사전 학습 단계에서는 대규모 주행 데이터로 BEV 표현을 학습하고, fine-tuning 단계에서는 특정 작업에 맞게 조정한다. 이는 3D 객체 검출, occupancy 예측, 경로 계획 등 다양한 downstream 작업에서 우수한 성능을 보인다.

Foundation Models의 또 다른 장점은 few-shot learning 능력이다. 새로운 환경이나 새로운 유형의 객체에 빠르게 적응할 수 있다. 예를 들어, 특정 국가에서 사용되는 독특한 교통 표지판이나, 특정 지역의 건축 양식 등을 몇 가지 예시만으로 학습할 수 있다. 이는 자율주행 시스템을 새로운 지역으로 확장할 때 매우 유용하다.

Prompt learning은 Foundation Models를 활용하는 새로운 패러다임이다. 모델의 파라미터를 수정하지 않고, 적절한 입력 프롬프트를 설계하여 원하는 작업을 수행하게 한다. 예를 들어, "이 장면에서 주행 가능한 영역은 어디인가?"라는 텍스트 프롬프트와 함께 이미지를 입력하면, 모델이 해당 영역을 출력한다. 이는 작업별로 별도의 모델을 학습할 필요 없이, 하나의 Foundation Model로 다양한 작업을 수행할 수 있게 한다.

Meta-learning 또는 learning to learn은 여러 작업에서 학습한 경험을 바탕으로, 새로운 작업에 빠르게 적응하는 능력을 학습한다. Foundation Model을 meta-learning 프레임워크로 학습하면, 적은 데이터로도 새로운 시나리오에 빠르게 fine-tuning될 수 있다. 이는 장기적으로 자율주행 시스템의 지속적인 진화와 개선을 가능하게 한다.

Vision Transformer와 Foundation Models의 발전은 자율주행을 데이터 중심, 스케일 중심의 패러다임으로 이끌고 있다. 더 많은 데이터, 더 큰 모델, 더 강력한 컴퓨팅이 성능 향상의 핵심 동력이 되고 있다. 이는 자원이 풍부한 대기업에게 유리하며, 실제로 Tesla, Waymo, Baidu와 같은 기업들이 막대한 투자를 통해 데이터와 컴퓨팅 인프라를 구축하고 있다. 동시에, 오픈소스 Foundation Models의 발전은 작은 기업과 연구자들에게도 최신 기술에 접근할 기회를 제공하고 있다. 향후 몇 년간 이러한 Foundation Models는 더욱 크고 강력해질 것이며, 자율주행의 핵심 기술로 자리잡을 것으로 전망된다.

#pagebreak()

_문서 계속 작성 중..._

#pagebreak()

= 결론

본 보고서는 End-to-End 자율주행 기술의 현황과 미래를 포괄적으로 분석하였다. 전통적인 모듈식 아키텍처에서 통합적인 End-to-End 학습으로의 패러다임 전환은 단순한 기술적 변화가 아니라, 자율주행 시스템을 설계하고 개발하는 근본적인 철학의 변화를 의미한다. 모방 학습, 강화학습, 인간 피드백 기반 학습 등 다양한 학습 방법론이 발전하고 있으며, 이들의 조합을 통해 인간 수준을 넘어서는 주행 성능을 달성할 가능성이 열리고 있다.

Foundation Models와 생성 AI의 융합은 자율주행 기술에 새로운 지평을 열었다. Vision Transformer는 대규모 사전 학습을 통해 강력한 시각 표현을 학습하며, 적은 데이터로도 새로운 상황에 빠르게 적응할 수 있게 한다. World Models는 미래를 예측하고 시뮬레이션하여 더 안전하고 지능적인 의사결정을 가능하게 한다. Diffusion Models는 현실적인 시나리오를 생성하여 드문 상황에 대한 대응 능력을 향상시킨다. Bird's Eye View 표현과 Query-based 아키텍처는 다중 센서 정보를 효과적으로 통합하고 3차원 공간을 이해하는 능력을 크게 개선했다.

글로벌 산업 생태계는 빠르게 진화하고 있다. Tesla는 vision-only 접근과 완전한 End-to-End 아키텍처로 독자적인 길을 걷고 있으며, 수백만 대의 플릿 데이터를 활용하여 지속적으로 시스템을 개선하고 있다. Waymo는 Level 4 Robotaxi 서비스를 여러 도시로 확장하며, 상업적 자율주행의 실현 가능성을 입증하고 있다. 중국 시장은 정부 지원과 대규모 투자, 빠른 상용화로 세계 자율주행 산업을 선도하고 있다. 한국은 후발 주자로서 인프라와 데이터 측면에서 도전에 직면해 있지만, 제조 역량과 반도체 기술을 바탕으로 기회를 모색하고 있다.

기술적으로는 여전히 해결해야 할 과제들이 남아 있다. Long-tail 이벤트 처리, 안전성의 통계적 검증, 새로운 환경에 대한 일반화, 실시간 처리를 위한 컴퓨팅 효율성 등은 상용화를 위해 극복해야 할 핵심 과제이다. 비기술적으로는 규제 프레임워크의 정립, 윤리적 의사결정 기준 마련, 대중의 신뢰 구축, 일자리 전환에 대한 사회적 대응 등이 필요하다.

향후 5년간 자율주행 기술은 급속히 발전할 것으로 전망된다. 2025-2026년에는 NOA 기능이 중급 차량에도 보급되고, Foundation Models가 표준 기술로 자리잡을 것이다. 2027-2028년에는 도시 전역에서 Level 3/Level 4 자율주행이 상용화되고, World Models 기반 planning이 핵심 기술로 부상할 것이다. 2029-2030년에는 제한된 영역에서 완전 무인 자율주행이 실현되고, Robotaxi의 경제성이 입증될 것이다. 이를 통해 교통사고가 감소하고, 모빌리티 접근성이 향상되며, 도시 구조와 생활 방식이 근본적으로 변화할 것이다.

자율주행의 미래는 단순히 운전의 자동화를 넘어, 더 안전하고 효율적이며 지속 가능한 교통 시스템을 실현하는 것이다. 기술 개발뿐만 아니라 사회적 합의, 규제 정비, 인프라 투자가 함께 이루어져야 진정한 자율주행 시대가 열릴 것이다. 한국은 이러한 글로벌 경쟁에서 기술력과 제조 역량을 바탕으로 독자적인 경쟁력을 확보할 수 있을 것으로 기대된다.
