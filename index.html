<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>End-to-End 자율주행 기술의 현황과 미래 전망</title>
    <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.0/dist/chart.umd.min.js"></script>
    <style>
        :root {
            --primary-color: #1e40af;
            --secondary-color: #3b82f6;
            --accent-color: #059669;
            --bg-color: #ffffff;
            --text-color: #1f2937;
            --gray-100: #f8fafc;
            --gray-200: #e2e8f0;
            --gray-300: #cbd5e1;
            --gray-600: #64748b;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Noto Sans KR', sans-serif;
            line-height: 1.7;
            color: var(--text-color);
            background: var(--bg-color);
            font-size: 16px;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }

        header {
            background: linear-gradient(135deg, var(--primary-color) 0%, var(--secondary-color) 100%);
            color: white;
            padding: 60px 20px;
            text-align: center;
            margin-bottom: 40px;
        }

        h1 {
            font-size: 2.5em;
            font-weight: 700;
            margin-bottom: 20px;
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.2em;
            opacity: 0.95;
            margin-bottom: 15px;
        }

        .meta {
            font-size: 1em;
            opacity: 0.9;
            margin-top: 20px;
        }

        nav {
            background: var(--gray-100);
            padding: 20px;
            border-radius: 8px;
            margin-bottom: 40px;
            border: 1px solid var(--gray-200);
        }

        nav h2 {
            font-size: 1.3em;
            margin-bottom: 15px;
            color: var(--primary-color);
        }

        nav ul {
            list-style: none;
        }

        nav li {
            margin: 8px 0;
        }

        nav a {
            color: var(--text-color);
            text-decoration: none;
            transition: color 0.2s;
            display: block;
            padding: 5px 10px;
            border-radius: 4px;
        }

        nav a:hover {
            color: var(--primary-color);
            background: white;
        }

        section {
            margin-bottom: 60px;
        }

        h2 {
            font-size: 2em;
            color: var(--primary-color);
            margin: 40px 0 20px 0;
            padding-bottom: 10px;
            border-bottom: 3px solid var(--primary-color);
        }

        h3 {
            font-size: 1.5em;
            color: var(--secondary-color);
            margin: 30px 0 15px 0;
        }

        h4 {
            font-size: 1.2em;
            color: var(--text-color);
            margin: 20px 0 10px 0;
        }

        p {
            margin: 15px 0;
            text-align: justify;
        }

        ul {
            margin: 15px 0;
            padding-left: 30px;
        }

        li {
            margin: 8px 0;
        }

        .highlight-box {
            background: var(--gray-100);
            padding: 25px;
            border-radius: 8px;
            margin: 25px 0;
            border-left: 4px solid var(--accent-color);
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 25px 0;
            background: white;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }

        th {
            background: var(--primary-color);
            color: white;
            padding: 15px;
            text-align: left;
            font-weight: 600;
        }

        td {
            padding: 12px 15px;
            border-bottom: 1px solid var(--gray-200);
        }

        tr:nth-child(even) {
            background: var(--gray-100);
        }

        .chart-container {
            position: relative;
            height: 400px;
            margin: 40px 0;
            background: white;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }

        .comparison-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin: 25px 0;
        }

        .comparison-card {
            background: white;
            padding: 25px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
            border-top: 4px solid var(--primary-color);
        }

        .comparison-card h4 {
            margin-top: 0;
            color: var(--primary-color);
        }

        .label {
            display: inline-block;
            background: var(--accent-color);
            color: white;
            padding: 4px 12px;
            border-radius: 4px;
            font-size: 0.85em;
            margin: 5px 5px 5px 0;
        }

        .roadmap-table {
            margin: 30px 0;
        }

        .roadmap-row {
            background: white;
            margin: 15px 0;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
            overflow: hidden;
        }

        .roadmap-header {
            background: var(--primary-color);
            color: white;
            padding: 15px 20px;
            cursor: pointer;
            display: flex;
            justify-content: space-between;
            align-items: center;
            font-weight: 600;
        }

        .roadmap-header:hover {
            background: var(--secondary-color);
        }

        .roadmap-content {
            padding: 20px;
            display: block;
        }

        footer {
            background: var(--gray-100);
            padding: 40px 20px;
            text-align: center;
            margin-top: 60px;
            color: var(--gray-600);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.8em;
            }

            h2 {
                font-size: 1.5em;
            }

            .chart-container {
                height: 300px;
            }
        }
    </style>
</head>
<body>
    <header>
        <div class="container">
            <h1>End-to-End 자율주행 기술의<br>현황과 미래 전망</h1>
            <p class="subtitle">Foundation Models와 생성 AI 기반 자율주행 시스템의<br>기술적 진화와 산업 생태계 분석</p>
            <p class="meta">한국전자통신연구원 (KETI) | 지능형 모빌리티 연구센터<br>2025년 10월</p>
        </div>
    </header>

    <div class="container">
        <nav>
            <h2>📑 목차</h2>
            <ul>
                <li><a href="#intro">1. 서론</a></li>
                <li><a href="#tech-foundation">2. End-to-End 자율주행의 기술적 기반</a></li>
                <li><a href="#timeline">3. 기술 발전 타임라인</a></li>
                <li><a href="#foundation-models">4. Foundation Models와 생성 AI</a></li>
                <li><a href="#industry">5. 글로벌 자율주행 산업 현황</a></li>
                <li><a href="#trends-2025">6. 2025년 기술 동향과 산업 전망</a></li>
                <li><a href="#challenges">7. 기술적 과제와 미래 전망</a></li>
                <li><a href="#conclusion">8. 결론</a></li>
            </ul>
        </nav>

        <section id="abstract">
            <div class="highlight-box">
                <h3>요약</h3>
                <p>본 보고서는 자율주행 기술의 최신 패러다임인 End-to-End 학습 방식과 Foundation Models의 융합을 중심으로, 글로벌 자율주행 산업의 기술적 발전 현황과 주요 기업들의 상용화 전략을 종합적으로 분석한다. 특히 Tesla의 FSD, Waymo의 Robotaxi, 그리고 중국 시장의 급속한 발전을 심층 분석하고, 2025년부터 2030년까지의 기술 로드맵과 산업 전망을 제시한다.</p>
            </div>
        </section>

        <section id="intro">
            <h2>1. 서론</h2>

            <h3>1.1 연구 배경 및 목적</h3>
            <p>자율주행 기술은 21세기 교통 혁명의 핵심으로 부상하고 있다. 지난 수십 년간 모듈식 아키텍처를 중심으로 발전해온 자율주행 시스템은, 최근 인공지능 기술의 비약적 발전과 함께 End-to-End 학습 방식이라는 새로운 패러다임으로 전환되고 있다.</p>

            <p>전통적인 모듈식 접근은 인지, 예측, 계획, 제어를 독립된 모듈로 구현하여 개발과 검증이 체계적이라는 장점이 있었으나, 모듈 간 정보 손실과 부분 최적화라는 근본적 한계를 가지고 있었다. End-to-End 학습은 센서 입력에서 제어 명령까지 전체 과정을 단일 신경망으로 학습하여 이러한 한계를 극복한다.</p>

            <p>1980년대 ALVINN에서 처음 제안되었으나 실용화되지 못했던 이 접근은, 2010년대 중반 딥러닝의 발전, GPU 성능 향상, 대규모 데이터 축적으로 재조명받기 시작했다. 특히 2016년 NVIDIA의 연구와 Tesla의 FSD 시스템 도입은 이 패러다임의 실현 가능성을 입증했다.</p>

            <p>최근에는 자연어 처리와 컴퓨터 비전에서 성공한 Foundation Models가 자율주행에 도입되고 있다. Vision Transformer, World Models, Diffusion Models 등이 인지, 예측, 시뮬레이션 능력을 혁신적으로 향상시키고 있으며, Bird's Eye View 표현과 결합하여 3차원 공간 이해 능력을 대폭 개선했다.</p>

            <h3>1.2 연구 방법 및 범위</h3>
            <p>본 연구는 문헌 조사, 산업 동향 분석, 기술 벤치마킹을 종합하는 접근 방식을 채택하였다. 학술 문헌은 CVPR, ICCV, NeurIPS, ICRA 등 주요 학회의 최근 3년간 논문과 arXiv 사전공개 논문을 중심으로 조사하였다.</p>

            <p>시간적 범위는 2020년부터 2025년 현재까지를 중심으로 하되 2030년까지의 미래 전망을 포함한다. 지리적 범위는 미국(기술 선도), 중국(빠른 상용화), 유럽(안전 규제), 한국(후발 주자)을 중점 다룬다.</p>
        </section>

        <section id="tech-foundation">
            <h2>2. End-to-End 자율주행의 기술적 기반</h2>

            <h3>2.1 패러다임의 전환: 모듈식에서 통합식으로</h3>

            <p>자율주행 기술의 발전 과정에서 가장 근본적인 변화는 시스템 아키텍처의 패러다임 전환이다. 전통적인 모듈식 아키텍처는 인지, 예측, 계획, 제어라는 네 가지 단계로 분해하고 각각을 독립적인 모듈로 구현하는 방식이었다. 이는 2007년 DARPA Urban Challenge에서 우승한 Carnegie Mellon University의 Boss 시스템에서 전형적으로 나타나며, 이후 십여 년간 표준적인 접근 방식으로 자리잡았다.</p>

            <h4>모듈식 아키텍처의 구조와 한계</h4>

            <p><strong>인지 모듈:</strong> 카메라, LiDAR, 레이더 등의 센서로부터 원시 데이터를 수집하여 주변 환경을 이해한다. 차량, 보행자, 자전거, 교통 표지판 등을 검출하고 분류하며, 차선과 도로 경계를 인식한다. 2015년 이후부터는 AlexNet, VGG, ResNet과 같은 깊은 합성곱 신경망이 핵심 기술로 자리잡았다.</p>

            <p><strong>예측 모듈:</strong> 인지 모듈이 출력한 객체 정보를 받아 각 객체의 미래 행동을 예측한다. 초기에는 칼만 필터와 같은 단순한 물리 기반 모델이 사용되었으나, 최근에는 순환 신경망과 Transformer 기반 모델들이 활용된다.</p>

            <p><strong>계획 모듈:</strong> 예측 결과를 바탕으로 자차의 주행 궤적을 생성한다. 경로 계획은 A-star, Dijkstra 같은 그래프 탐색 알고리즘으로, 행동 계획은 유한 상태 기계나 의사결정 트리로 구현된다.</p>

            <p><strong>제어 모듈:</strong> 계획된 궤적을 실제 차량 동작으로 변환한다. PID 제어기, 모델 예측 제어 등의 고전 제어 이론이 주로 사용된다.</p>

            <h4>모듈식 아키텍처의 근본적 한계</h4>

            <p><strong>정보 손실:</strong> 인지 모듈은 고차원의 센서 데이터를 객체 목록이라는 저차원 표현으로 압축한다. 1920x1080 해상도의 이미지는 약 200만 픽셀의 정보를 담고 있지만, 이를 처리한 출력은 수십 개의 bounding box로 요약된다. 미세한 질감, 조명 조건, 깊이의 불확실성 등 많은 정보가 소실된다.</p>

            <p><strong>오류의 누적:</strong> 인지 모듈이 차량을 잘못 검출하면, 예측 모듈은 잘못된 정보를 바탕으로 미래를 예측하고, 계획 모듈은 잘못된 예측을 바탕으로 위험한 궤적을 생성할 수 있다. 각 모듈의 오류가 연쇄적으로 증폭된다.</p>

            <p><strong>부분 최적화:</strong> 각 모듈은 자신만의 목적 함수를 최적화한다. 인지는 검출 정확도를, 예측은 예측 오차를, 계획은 궤적의 부드러움을 최적화하지만, 전체 시스템의 목표인 안전하고 효율적인 주행과는 다를 수 있다.</p>

            <h4>End-to-End 아키텍처의 등장</h4>

            <p>End-to-End 접근은 센서 입력에서 제어 출력까지의 전체 매핑을 하나의 신경망으로 직접 학습한다. 1989년 Pomerleau의 ALVINN 시스템에서 처음 시도되었으나, 2016년 NVIDIA의 연구에서 재조명받았다. 단 9층의 합성곱 신경망으로 카메라 이미지에서 조향각을 직접 예측할 수 있음을 보였으며, 72시간의 인간 주행 데이터로 학습되었다.</p>

            <p><strong>핵심 장점:</strong></p>
            <ul>
                <li><strong>전역 최적화:</strong> 전체 시스템을 하나의 목적 함수로 최적화할 수 있다. 주행 성능을 직접 측정하여 역전파로 모든 파라미터를 업데이트한다.</li>
                <li><strong>정보 보존:</strong> 모듈 간 인터페이스가 없으므로 정보 손실이 최소화된다. 센서 데이터의 모든 정보가 네트워크를 통해 흐른다.</li>
                <li><strong>암묵적 지식 학습:</strong> 인간이 명시적으로 프로그래밍하기 어려운 복잡한 패턴을 데이터로부터 자동으로 학습한다.</li>
                <li><strong>확장성:</strong> 더 많은 데이터와 더 큰 모델을 투입하면 성능이 지속적으로 향상되는 스케일링 법칙을 따른다.</li>
            </ul>

            <h3>2.2 모듈식 vs End-to-End 비교</h3>

            <table>
                <thead>
                    <tr>
                        <th>구분</th>
                        <th>모듈식 아키텍처</th>
                        <th>End-to-End 아키텍처</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>시스템 구조</strong></td>
                        <td>센서 → 인지 → 예측 → 계획 → 제어<br>각 모듈 독립적 개발</td>
                        <td>센서 → 통합 신경망 → 제어<br>전체를 하나로 학습</td>
                    </tr>
                    <tr>
                        <td><strong>최적화 방식</strong></td>
                        <td>모듈별 독립 최적화 (부분 최적화)</td>
                        <td>전체 통합 최적화 (전역 최적화)</td>
                    </tr>
                    <tr>
                        <td><strong>중간 표현</strong></td>
                        <td>구조화된 데이터 (객체 리스트, 차선 다항식)<br>→ 저차원 압축, 정보 손실</td>
                        <td>학습된 표현 (고차원 특징, BEV feature map)<br>→ 원시 데이터 정보 보존</td>
                    </tr>
                    <tr>
                        <td><strong>해석 가능성</strong></td>
                        <td>높음 - 각 모듈 출력 검사 가능</td>
                        <td>낮음 - 블랙박스 특성</td>
                    </tr>
                    <tr>
                        <td><strong>확장성</strong></td>
                        <td>제한적 - 인터페이스 의존성</td>
                        <td>우수함 - Scaling law 적용</td>
                    </tr>
                    <tr>
                        <td><strong>데이터 요구</strong></td>
                        <td>모듈별 레이블 데이터 (분리 가능)</td>
                        <td>대규모 입출력 쌍 필요</td>
                    </tr>
                    <tr>
                        <td><strong>대표 시스템</strong></td>
                        <td>Waymo Driver (초기), Apollo</td>
                        <td>Tesla FSD v12+, Wayve</td>
                    </tr>
                </tbody>
            </table>

            <h3>2.3 하이브리드 접근</h3>

            <p>현재의 추세는 순수한 End-to-End와 모듈식의 중간 지점을 탐색하는 것이다. 하이브리드 아키텍처는 End-to-End 학습의 장점을 취하면서도 일부 해석 가능성과 안전성 보장을 유지한다.</p>

            <p>Tesla의 FSD는 대표적인 하이브리드 접근이다. 시스템은 End-to-End 신경망을 핵심으로 하지만, 중간에 명시적인 3D 공간 표현(BEV)과 객체 리스트를 생성하며, 최종 계획 단계에서는 물리적 제약과 안전 규칙을 명시적으로 확인한다.</p>
        </section>

        <section id="timeline">
            <h2>3. End-to-End 자율주행 기술 발전 타임라인</h2>

            <div class="chart-container">
                <canvas id="timelineChart"></canvas>
            </div>

            <p>위 그래프는 1989년 ALVINN부터 2030년 상용화 전망까지 End-to-End 자율주행 기술의 발전 과정을 시각화한 것이다. 기술 성숙도는 연구 수준, 프로토타입, 제한적 배포, 대규모 배포의 단계로 나뉘며, 30년 이상의 긴 역사를 거쳐 점진적으로 발전해왔음을 보여준다.</p>

            <h3>주요 이정표</h3>

            <div class="highlight-box">
                <h4>1989 - ALVINN: 최초의 End-to-End 시스템</h4>
                <p>Carnegie Mellon의 Pomerleau가 개발한 ALVINN은 단순 신경망으로 도로 주행에 성공했다. 당시의 제한된 컴퓨팅 성능으로 간단한 환경에서만 작동했지만, End-to-End 학습의 가능성을 최초로 입증했다.</p>
            </div>

            <div class="highlight-box">
                <h4>2012 - Deep Learning 혁명</h4>
                <p>AlexNet이 ImageNet에서 압도적 성능을 보이며 딥러닝이 컴퓨터 비전의 주류로 부상했다. 이는 자율주행 인지 시스템에 CNN이 적용되는 계기가 되었다.</p>
            </div>

            <div class="highlight-box">
                <h4>2016 - NVIDIA End-to-End 주행</h4>
                <p>단 9층 CNN으로 카메라→조향각 직접 예측에 성공했다. 72시간의 데이터로 학습하여 다양한 도로에서 주행 가능함을 보였으며, End-to-End 학습의 실현 가능성을 재입증했다.</p>
            </div>

            <div class="highlight-box">
                <h4>2020-2021 - Vision Transformer와 Tesla Vision-Only</h4>
                <p>Google의 Vision Transformer가 CNN을 능가하며 새로운 백본으로 부상했다. Tesla는 FSD Beta에서 LiDAR를 제거하고 순수 카메라 기반 시스템을 채택했다.</p>
            </div>

            <div class="highlight-box">
                <h4>2023-2024 - Foundation Models 시대</h4>
                <p>World Models(GAIA-1), Diffusion Models, Occupancy Networks가 등장했다. 대규모 사전학습 모델이 자율주행의 표준 기술로 자리잡기 시작했다.</p>
            </div>

            <div class="highlight-box">
                <h4>2025-2030 - 상용화 가속</h4>
                <p>FSD v13/v14로 도심 성능이 대폭 향상되고, Level 3/4 규제 승인이 확대될 전망이다. 2028년 이후에는 Robotaxi가 제한적으로 대중화되고 MaaS 생태계가 형성될 것으로 예상된다.</p>
            </div>
        </section>

        <section id="foundation-models">
            <h2>4. Foundation Models와 생성 AI의 융합</h2>

            <p>Foundation Models는 대규모 데이터로 사전학습된 범용 모델로, 다양한 하위 작업에 전이 가능한 모델이다. 자연어 처리의 GPT, BERT와 컴퓨터 비전의 Vision Transformer가 대표적이며, 최근 이러한 기술들이 자율주행에 적용되어 인지, 예측, 시뮬레이션 능력을 혁신적으로 향상시키고 있다.</p>

            <h3>4.1 Vision Transformer (ViT)</h3>

            <p>2020년 Google Research에서 발표한 Vision Transformer는 자연어 처리에서 성공한 Transformer 아키텍처를 이미지 인식에 적용한 것이다. 기존 CNN과 달리 이미지를 16×16 픽셀의 작은 패치들로 나누고 이를 시퀀스로 취급하여 self-attention 메커니즘으로 처리한다.</p>

            <h4>자율주행에서의 활용</h4>

            <ul>
                <li><strong>다중 카메라 융합:</strong> Tesla FSD는 8개 카메라의 이미지를 ViT로 처리하여 통합된 특징 공간으로 변환한다. Self-attention은 카메라 간 시점을 암묵적으로 정렬하고 중복 정보를 제거한다.</li>
                <li><strong>시간적 일관성:</strong> 연속된 프레임 간의 관계를 모델링하여 움직이는 객체를 추적하고 일시적 가림을 처리한다.</li>
                <li><strong>장거리 의존성:</strong> CNN의 제한된 receptive field와 달리, ViT는 이미지 전체의 맥락을 한 번에 파악할 수 있다.</li>
                <li><strong>확장성:</strong> 모델 크기와 데이터를 늘리면 성능이 예측 가능하게 향상되는 스케일링 법칙을 따른다.</li>
            </ul>

            <p>ViT는 BEV Transformer의 백본으로 널리 사용되며, Tesla, Waymo, Wayve 등 주요 기업들이 채택하고 있다.</p>

            <h3>4.2 World Models</h3>

            <p>World Models는 자율주행 시스템이 환경의 미래 상태를 예측하고 시뮬레이션할 수 있게 하는 생성 모델이다. 과거 관측으로부터 미래 프레임을 생성하며, 가능한 행동의 결과를 미리 시뮬레이션하여 안전한 결정을 내릴 수 있게 한다.</p>

            <h4>대표적인 World Models</h4>

            <ul>
                <li><strong>GAIA-1 (Wayve, 2023):</strong> 비디오, 행동, 텍스트를 조건으로 미래 주행 장면을 생성하는 생성 AI 모델이다. "차량이 좌회전한다"는 텍스트와 과거 프레임을 입력하면, 좌회전 후의 장면을 사실적으로 생성한다.</li>
                <li><strong>NVIDIA Neural World Model:</strong> 물리 엔진과 신경망을 결합하여 현실적인 주행 시나리오를 시뮬레이션한다. 에이전트의 행동과 환경의 동역학을 함께 학습한다.</li>
                <li><strong>DreamerV3:</strong> 모델 기반 강화학습을 위한 world model로, 실제 경험 없이 내부 시뮬레이션만으로 정책을 학습할 수 있다.</li>
            </ul>

            <h4>자율주행에서의 활용</h4>

            <p><strong>예측:</strong> 다른 차량과 보행자의 미래 궤적을 예측하여 충돌을 사전에 회피한다. 단순히 현재 속도를 외삽하는 것이 아니라, 환경과의 상호작용을 고려한 현실적인 예측을 제공한다.</p>

            <p><strong>계획:</strong> 여러 행동을 내부적으로 시뮬레이션하여 가장 안전하고 효율적인 경로를 선택한다. 모델 기반 강화학습으로 실제 주행 없이도 정책을 개선할 수 있다.</p>

            <p><strong>데이터 증강:</strong> 드물게 발생하는 위험 상황(긴급 제동, 끼어들기, 보행자 돌출 등)을 합성하여 시스템을 더 강건하게 학습시킨다.</p>

            <h3>4.3 Diffusion Models</h3>

            <p>Diffusion Models는 2020년대 초반 이미지 생성 분야에서 GAN을 능가하는 성능을 보인 생성 AI 기법이다. 노이즈에서 점진적으로 이미지를 복원하는 과정을 학습하며, 안정적이고 다양한 샘플을 생성할 수 있다.</p>

            <h4>자율주행에서의 활용</h4>

            <ul>
                <li><strong>궤적 생성:</strong> 여러 제약 조건(도로 경계, 장애물 회피, 승차감)을 만족하는 부드러운 주행 궤적을 생성한다.</li>
                <li><strong>시나리오 합성:</strong> 드문 위험 상황을 현실적으로 합성하여 테스트 데이터를 확장한다.</li>
                <li><strong>센서 시뮬레이션:</strong> 다양한 날씨, 조명 조건에서의 카메라/LiDAR 데이터를 생성하여 도메인 갭을 줄인다.</li>
                <li><strong>예측 불확실성:</strong> 여러 가능한 미래를 샘플링하여 확률적 예측을 제공한다.</li>
            </ul>

            <h3>4.4 Bird's Eye View (BEV) 표현</h3>

            <p>Bird's Eye View는 주변 환경을 차량 위에서 내려다본 것처럼 조감도로 표현하는 방식이다. 다중 카메라의 시점을 통합하고 3차원 공간 관계를 명확히 표현할 수 있어, End-to-End 자율주행 시스템의 핵심 중간 표현으로 자리잡았다.</p>

            <h4>BEV의 장점</h4>

            <ul>
                <li><strong>시점 통합:</strong> 전방, 후방, 측면 카메라의 이미지를 하나의 일관된 공간으로 변환한다.</li>
                <li><strong>3D 이해:</strong> 객체의 위치, 방향, 크기를 지면 평면에서 직접 표현하여 거리 추정이 정확하다.</li>
                <li><strong>기하학적 일관성:</strong> 물리적 제약(차량은 겹치지 않음)을 자연스럽게 표현한다.</li>
                <li><strong>계획과의 통합:</strong> BEV 공간에서 직접 궤적을 생성하여 인지-계획 파이프라인을 단순화한다.</li>
            </ul>

            <h4>대표적인 BEV 아키텍처</h4>

            <ul>
                <li><strong>LSS (Lift-Splat-Shoot):</strong> 이미지 특징을 3D로 lifting하고, BEV로 splat하여 투영한다. 명시적 깊이 추정을 사용한다.</li>
                <li><strong>BEVFormer:</strong> Transformer 기반으로 temporal attention으로 시간 정보를 통합하고, spatial cross-attention으로 이미지를 BEV로 변환한다.</li>
                <li><strong>BEVDet:</strong> 효율적인 BEV 변환과 객체 검출을 통합한 실시간 시스템이다.</li>
                <li><strong>Tesla Occupancy Network:</strong> BEV 공간을 voxel grid로 표현하여 각 셀의 점유 확률을 예측한다. 명시적인 객체 검출 없이도 주행 가능 영역을 파악한다.</li>
            </ul>
        </section>

        <section id="industry">
            <h2>5. 글로벌 자율주행 산업의 현황</h2>

            <h3>5.1 Tesla: Vision-Only End-to-End 접근</h3>

            <p>Tesla는 자율주행 산업에서 가장 급진적인 접근을 취하고 있다. 2021년 LiDAR를 완전히 제거하고 순수 카메라 기반 시스템으로 전환했으며, 2023년 FSD v12부터는 완전한 End-to-End 신경망 아키텍처를 채택했다.</p>

            <h4>핵심 전략</h4>

            <ul>
                <li><strong>센서 구성:</strong> 8개의 카메라만 사용 (센서 비용 < $1,000). LiDAR, 레이더, HD맵 없이 순수 시각 정보만으로 주행한다.</li>
                <li><strong>아키텍처:</strong> FSD v12부터 완전한 End-to-End 신경망. 카메라 이미지 → BEV 표현 → Occupancy Network → 궤적 계획 → 제어 명령이 하나의 통합 네트워크로 처리된다.</li>
                <li><strong>데이터 스케일:</strong> 전 세계 수백만 대의 Tesla 차량이 데이터를 수집하는 거대한 플릿. Shadow mode로 실제 인간 운전과 FSD 예측을 비교하여 학습 데이터를 생성한다.</li>
                <li><strong>컴퓨팅 인프라:</strong> Dojo 슈퍼컴퓨터로 13만 5천 개의 GPU를 운영하며 대규모 모델을 학습한다.</li>
                <li><strong>배포 전략:</strong> Over-the-air 업데이트로 빠른 반복 개발. 2주마다 새로운 버전을 배포하며 지속적으로 개선한다.</li>
            </ul>

            <h4>FSD v13/v14의 진화</h4>

            <p><strong>FSD v13 (2025 초):</strong> End-to-End 신경망 비중이 더욱 확대되었다. 도시 환경에서의 성능 향상이 특히 두드러지며, 사용자 보고에 따르면 인간 개입 빈도가 30-40% 감소했다. 복잡한 교차로와 비보호 좌회전 상황에서의 의사결정이 개선되었다.</p>

            <p><strong>FSD v14 (2025 중반 예정):</strong> Occupancy Network의 전면적 적용이 핵심 혁신이다. 명시적으로 정의되지 않은 객체나 복잡한 형태의 장애물(공사 장비, 쓰레진 짐 등)에 대한 강건성이 크게 향상될 것으로 기대된다.</p>

            <h3>5.2 Waymo: Multi-Sensor Level 4 달성</h3>

            <p>Waymo는 Google의 자율주행 프로젝트에서 시작하여, 2018년 독립 회사로 분사했다. 안전성을 최우선으로 하는 보수적 접근으로, 2020년 Phoenix에서 세계 최초로 완전 무인 Robotaxi 서비스를 시작했다.</p>

            <h4>핵심 전략</h4>

            <ul>
                <li><strong>센서 구성:</strong> 카메라 29대 + LiDAR 5대 + 레이더 6대 (센서 비용 > $100,000). 다중 센서 융합으로 redundancy를 확보하여 안전성을 극대화한다.</li>
                <li><strong>아키텍처:</strong> 초기에는 모듈식 아키텍처였으나, 최근 일부 모듈에 End-to-End 학습을 도입하는 하이브리드 접근으로 진화 중이다.</li>
                <li><strong>운영 전략:</strong> 제한된 지역에서 철저한 매핑과 검증 후 단계적 확장. Phoenix, San Francisco, Los Angeles, Austin에서 Level 4 서비스를 운영 중이다.</li>
                <li><strong>상용화 현황:</strong> 주당 15만 회 이상의 유료 승차 서비스를 제공하고 있으며 (2025년 기준), 2024년 말 100만 건의 누적 무인 승차를 달성했다.</li>
                <li><strong>안전 검증:</strong> 수억 마일의 시뮬레이션과 수천만 마일의 실도로 주행 데이터로 통계적 안전성을 입증했다.</li>
            </ul>

            <h3>5.3 Tesla vs Waymo 비교</h3>

            <div class="comparison-grid">
                <div class="comparison-card">
                    <h4>Tesla FSD</h4>
                    <p><span class="label">Vision-Only</span> <span class="label">End-to-End</span> <span class="label">Mass Market</span></p>
                    <ul style="margin-top: 15px; line-height: 1.8;">
                        <li><strong>센서:</strong> 카메라 8대 (< $1,000)</li>
                        <li><strong>시스템:</strong> 완전 E2E 신경망 (v12+)</li>
                        <li><strong>자율성:</strong> Level 2+ (감독 필요)</li>
                        <li><strong>데이터:</strong> 수백만 대 플릿</li>
                        <li><strong>배포:</strong> 빠른 OTA 업데이트</li>
                        <li><strong>시장:</strong> 대중 시장, 개인 차량</li>
                        <li><strong>컴퓨팅:</strong> 135k GPU (Dojo)</li>
                    </ul>
                </div>

                <div class="comparison-card">
                    <h4>Waymo Driver</h4>
                    <p><span class="label">Multi-Sensor</span> <span class="label">Safety First</span> <span class="label">Robotaxi</span></p>
                    <ul style="margin-top: 15px; line-height: 1.8;">
                        <li><strong>센서:</strong> 카메라 29대 + LiDAR 5대 + 레이더 6대 (> $100k)</li>
                        <li><strong>시스템:</strong> 하이브리드 (모듈식 + E2E)</li>
                        <li><strong>자율성:</strong> Level 4 (완전 무인 달성)</li>
                        <li><strong>운영:</strong> Phoenix, SF, LA, Austin</li>
                        <li><strong>배포:</strong> 점진적 확장, 철저한 검증</li>
                        <li><strong>시장:</strong> Robotaxi 서비스 (주당 150k+ 승차)</li>
                        <li><strong>안전:</strong> 수억 마일 시뮬레이션</li>
                    </ul>
                </div>
            </div>

            <div class="chart-container">
                <canvas id="comparisonChart"></canvas>
            </div>

            <p>위 레이더 차트는 Tesla와 Waymo의 5가지 핵심 지표를 비교한 것이다:</p>
            <ul>
                <li><strong>비용 효율성:</strong> Tesla가 저렴한 센서로 압도적 우위</li>
                <li><strong>안전성:</strong> Waymo가 다중 센서와 철저한 검증으로 우위</li>
                <li><strong>확장성:</strong> Tesla가 대중 시장 접근으로 우위</li>
                <li><strong>데이터 규모:</strong> Tesla가 수백만 플릿으로 압도적 우위</li>
                <li><strong>상용화 수준:</strong> Tesla는 Level 2+, Waymo는 제한적이지만 Level 4 달성</li>
            </ul>

            <h3>5.4 중국 시장: 빠른 추격</h3>

            <p>중국은 정부의 강력한 지원, 대규모 투자, 빠른 규제 승인으로 자율주행 분야에서 급속히 성장하고 있다. Baidu Apollo, Pony.ai, AutoX, WeRide 등이 주요 기업이며, 도심 NOA 기능을 탑재한 차량이 급증하고 있다.</p>
        </section>

        <section id="trends-2025">
            <h2>6. 2025년 기술 동향과 산업 전망</h2>

            <h3>6.1 FSD v13과 v14의 진화</h3>
            <p>2025년은 Tesla FSD의 중요한 전환점이 되고 있다. FSD v13은 2025년 초 광범위하게 배포되어 End-to-End 신경망 아키텍처의 비중이 더욱 확대되었다. 도시 환경에서의 성능 향상이 특히 두드러지며, 사용자 보고에 따르면 인간 개입 빈도가 30-40% 감소했다.</p>

            <p>FSD v14는 2025년 중반 출시를 목표로 개발 중이며, Occupancy Network의 전면적 적용이 핵심 혁신이다. 명시적으로 정의되지 않은 객체나 복잡한 형태의 장애물에 대한 강건성이 크게 향상될 것으로 기대된다.</p>

            <h3>6.2 중국 시장의 급속한 발전</h3>
            <p>중국은 2025년 현재 자율주행 분야에서 가장 빠르게 성장하고 있는 시장이다. 정부의 적극적인 지원, 대규모 투자, 빠른 규제 승인이 맞물려 상용화 속도가 매우 빠르다. 특히 도심 NOA 기능을 탑재한 차량이 급증하고 있으며, Robotaxi 시범 서비스도 여러 도시에서 운영 중이다.</p>

            <h3>6.3 한국의 도전과 기회</h3>
            <p>한국은 후발 주자로서 여러 도전에 직면해 있다. 가장 큰 문제는 데이터와 컴퓨팅 인프라의 격차이다. Tesla는 13만 5천 개의 GPU를 보유한 반면, 한국 기업들은 수십에서 수백 개 수준에 머물러 있다.</p>

            <p>그러나 세계적인 자동차 제조 역량, 반도체 기술, 5G 인프라는 기회 요소이다. 현대차와 기아는 자체 자율주행 스택을 개발 중이며, 정부도 규제 샌드박스를 통해 기술 개발을 지원하고 있다.</p>
        </section>

        <section id="challenges">
            <h2>7. 기술적 과제와 미래 전망</h2>

            <h3>7.1 극복해야 할 핵심 과제</h3>

            <h4>Long-tail 문제</h4>
            <p>드물게 발생하는 복잡한 상황(long-tail events)은 자율주행 시스템의 가장 큰 도전이다. 학습 데이터에서 충분히 보지 못한 상황에서 시스템은 예측할 수 없는 행동을 할 수 있다. 예를 들어, 도로에 쓰러진 냉장고, 역주행 차량, 낙하물, 공사 중인 구역의 복잡한 임시 표지판 등이 이에 해당한다.</p>

            <p><strong>해결 방안:</strong> 생성 AI를 활용한 시나리오 합성, 시뮬레이션을 통한 데이터 증강, World Models를 이용한 사전 시뮬레이션 등이 연구되고 있다. Tesla는 플릿에서 드문 상황을 자동 감지하여 라벨링하고 재학습에 활용한다.</p>

            <h4>안전성의 통계적 검증</h4>
            <p>자율주행 시스템이 인간 운전자보다 안전하다는 것을 통계적으로 입증하려면 수십억 마일의 주행 데이터가 필요하다. NHTSA는 Level 4 시스템이 인간보다 안전함을 입증하려면 80억 마일 이상의 무사고 주행이 필요하다고 추정한다.</p>

            <p><strong>해결 방안:</strong> 시뮬레이션, 형식 검증, 시나리오 기반 테스트 등을 조합한 다층적 접근이 필요하다. Waymo는 수억 마일의 시뮬레이션과 수천만 마일의 실도로 주행을 결합하여 안전성을 입증하고 있다.</p>

            <h4>일반화: 새로운 환경으로의 적응</h4>
            <p>특정 지역에서 학습된 시스템을 다른 지역으로 전이하는 것은 어렵다. 도로 구조, 교통 패턴, 운전 문화가 다르기 때문이다. 예를 들어, 미국에서 학습된 시스템은 한국의 좁은 골목길과 공격적인 운전 문화에 적응하기 어렵다.</p>

            <p><strong>해결 방안:</strong> Foundation Models의 전이 학습 능력을 활용하고, 도메인 적응 기법(domain adaptation)을 적용한다. 소량의 현지 데이터로 fine-tuning하여 빠르게 적응시키는 연구가 진행 중이다.</p>

            <h4>컴퓨팅 효율성</h4>
            <p>실시간 처리를 위해서는 밀리초 단위의 지연시간이 필요하며, 차량의 전력 제약도 고려해야 한다. 대규모 Transformer 모델은 수백 GFLOPS의 연산을 요구하지만, 차량의 ECU는 제한된 전력 예산(수백 와트) 내에서 작동해야 한다.</p>

            <p><strong>해결 방안:</strong> 모델 경량화(pruning, quantization), 지식 증류(knowledge distillation), 효율적 아키텍처 설계(MobileNet, EfficientNet), 전용 하드웨어 가속기(Tesla FSD chip, NVIDIA DRIVE Orin) 등이 활용되고 있다.</p>

            <h3>7.2 2030년까지의 기술 로드맵</h3>

            <div class="roadmap-table">
                <div class="roadmap-row">
                    <div class="roadmap-header">
                        <span><strong>2025-2026:</strong> NOA의 대중화와 Foundation Models의 표준화</span>
                        <span>▼</span>
                    </div>
                    <div class="roadmap-content">
                        <p><strong>주요 기술:</strong></p>
                        <ul>
                            <li>고속도로 NOA는 이미 보급, 도심 NOA가 빠르게 확대</li>
                            <li>Vision-centric 접근이 주류로 자리잡음</li>
                            <li>Foundation Models(ViT, BEV Transformer)가 표준 기술로 채택</li>
                            <li>Occupancy Networks가 인지 시스템의 핵심으로 부상</li>
                        </ul>
                        <p><strong>산업 동향:</strong></p>
                        <ul>
                            <li>중국 시장의 급성장, 미국과 기술 격차 축소</li>
                            <li>Robotaxi 서비스가 10개 이상 도시로 확대</li>
                            <li>Level 3 규제 승인이 일부 국가에서 시작</li>
                        </ul>
                    </div>
                </div>

                <div class="roadmap-row">
                    <div class="roadmap-header">
                        <span><strong>2027-2028:</strong> Level 3/4의 상용화와 World Models의 통합</span>
                        <span>▼</span>
                    </div>
                    <div class="roadmap-content">
                        <p><strong>주요 기술:</strong></p>
                        <ul>
                            <li>World Models가 계획 시스템의 핵심으로 부상</li>
                            <li>BEV와 Occupancy Networks가 표준 아키텍처로 확립</li>
                            <li>생성 AI를 활용한 시나리오 합성이 일반화</li>
                            <li>멀티모달 Foundation Models(비전+언어)가 등장</li>
                        </ul>
                        <p><strong>산업 동향:</strong></p>
                        <ul>
                            <li>제한된 조건의 Level 3가 주요 시장에서 상용화</li>
                            <li>일부 도시에서 Level 4 Robotaxi가 24시간 운영</li>
                            <li>자율주행 트럭이 고속도로 물류에 투입</li>
                            <li>보험 및 법적 프레임워크 정립</li>
                        </ul>
                    </div>
                </div>

                <div class="roadmap-row">
                    <div class="roadmap-header">
                        <span><strong>2029-2030:</strong> 완전 무인 자율주행의 실현</span>
                        <span>▼</span>
                    </div>
                    <div class="roadmap-content">
                        <p><strong>주요 기술:</strong></p>
                        <ul>
                            <li>통합된 Vision-Language-Action Models 등장</li>
                            <li>자율주행 시스템이 복잡한 지시를 자연어로 이해</li>
                            <li>실시간 학습(online learning)으로 지속적 적응</li>
                            <li>Vehicle-to-Everything(V2X) 통신과 통합</li>
                        </ul>
                        <p><strong>산업 동향:</strong></p>
                        <ul>
                            <li>Robotaxi 서비스가 주요 도시에서 24시간 운영</li>
                            <li>자율주행이 도시 교통의 상당 비중 차지</li>
                            <li>MaaS(Mobility as a Service) 생태계 형성</li>
                            <li>개인 차량 소유 패러다임이 변화 시작</li>
                            <li>경제성이 입증되어 투자 회수 시작</li>
                        </ul>
                    </div>
                </div>
            </div>

            <h3>7.3 미래 전망: 2030년 이후</h3>

            <p>2030년대에는 자율주행이 단순한 운송 수단을 넘어 도시 구조와 생활 방식을 근본적으로 변화시킬 것으로 예상된다. 주차장 수요 감소로 도시 공간이 재편되고, 출퇴근 시간이 생산적 시간으로 전환되며, 고령자와 장애인의 이동성이 크게 향상될 것이다.</p>

            <p>기술적으로는 자율주행 시스템이 인간 수준을 넘어 초인적 성능에 도달할 가능성이 있다. 360도 시야, 밀리초 반응 시간, 피로하지 않는 주의력, 집단 학습을 통한 지속적 개선 등이 인간을 능가하는 요소들이다.</p>
        </section>

        <section id="conclusion">
            <h2>8. 결론</h2>

            <p>본 보고서는 End-to-End 자율주행 기술의 현황과 미래를 포괄적으로 분석하였다. 전통적인 모듈식 아키텍처에서 통합적인 End-to-End 학습으로의 패러다임 전환은 자율주행 시스템을 설계하고 개발하는 근본적인 철학의 변화를 의미한다.</p>

            <p>Foundation Models와 생성 AI의 융합은 자율주행 기술에 새로운 지평을 열었다. Vision Transformer, World Models, Diffusion Models, BEV 표현 등이 인지, 예측, 계획 능력을 혁신적으로 향상시키고 있다. 이러한 기술들은 자연어 처리와 컴퓨터 비전에서 검증된 스케일링 법칙을 따르며, 더 많은 데이터와 더 큰 모델을 투입할수록 성능이 예측 가능하게 향상된다.</p>

            <p>글로벌 산업 생태계는 빠르게 진화하고 있다. Tesla는 vision-only 접근과 완전한 End-to-End 아키텍처로 대중 시장을 겨냥하며, Waymo는 다중 센서와 철저한 검증으로 Level 4를 달성했다. 두 접근 모두 장단점이 있으며, 시장은 다양한 니즈를 충족하기 위해 여러 솔루션을 수용할 것이다.</p>

            <p>중국 시장은 정부 지원과 빠른 상용화로 세계를 선도하고 있으며, 한국은 후발 주자로서 도전과 기회를 동시에 맞이하고 있다. 데이터와 컴퓨팅 인프라의 격차를 극복하고, 자동차 제조 역량과 반도체 기술을 활용하는 전략이 필요하다.</p>

            <p>향후 5년간 자율주행 기술은 급속히 발전할 것으로 전망된다. 2025-2026년에는 NOA 기능이 대중화되고 Foundation Models가 표준화될 것이다. 2027-2028년에는 Level 3/4가 상용화되고 World Models가 핵심 기술로 부상할 것이다. 2029-2030년에는 완전 무인 자율주행이 제한된 영역에서 실현되고, Robotaxi의 경제성이 입증될 것이다.</p>

            <p>자율주행 기술은 단순한 운송 수단의 혁신을 넘어, 도시 구조, 생활 방식, 경제 생태계를 근본적으로 변화시킬 잠재력을 가지고 있다. 이러한 변화를 성공적으로 이끌기 위해서는 기술적 혁신뿐만 아니라 규제 프레임워크의 정립, 윤리적 기준 마련, 대중의 신뢰 구축, 사회적 합의 형성이 함께 이루어져야 한다.</p>

            <p>End-to-End 학습과 Foundation Models의 결합은 자율주행을 인간의 능력을 모방하는 단계에서, 인간을 넘어서는 초인적 성능으로 진화시킬 가능성을 보여준다. 다음 10년은 이 기술이 실험실에서 일상으로, 가능성에서 현실로 전환되는 결정적 시기가 될 것이다.</p>
        </section>
    </div>

    <footer>
        <div class="container">
            <p><strong>End-to-End 자율주행 기술의 현황과 미래 전망</strong></p>
            <p style="margin-top: 10px;">한국전자통신연구원 (KETI) | 지능형 모빌리티 연구센터</p>
            <p style="margin-top: 20px; font-size: 0.9em;">본 보고서는 공개된 학술 문헌, 산업 자료, 기술 벤치마크를 종합하여 작성되었습니다.</p>
            <p style="margin-top: 10px; font-size: 0.9em;">© 2025 KETI. Generated with Claude Code.</p>
        </div>
    </footer>

    <script>
        // Timeline Chart
        const timelineCtx = document.getElementById('timelineChart').getContext('2d');
        new Chart(timelineCtx, {
            type: 'line',
            data: {
                labels: ['1989', '2007', '2012', '2016', '2017-19', '2020', '2021', '2022', '2023', '2024', '2025', '2030'],
                datasets: [{
                    label: 'End-to-End 자율주행 기술 성숙도',
                    data: [1, 0.5, 1.5, 2.5, 3, 4, 5, 6, 7, 8, 8.5, 10],
                    borderColor: '#3b82f6',
                    backgroundColor: 'rgba(59, 130, 246, 0.1)',
                    tension: 0.4,
                    fill: true,
                    pointRadius: 6,
                    pointHoverRadius: 8,
                    pointBackgroundColor: '#1e40af'
                }]
            },
            options: {
                responsive: true,
                maintainAspectRatio: false,
                plugins: {
                    title: {
                        display: true,
                        text: 'End-to-End 자율주행 기술 발전 타임라인 (1989-2030)',
                        font: { size: 16, weight: 'bold' }
                    },
                    legend: {
                        display: false
                    },
                    tooltip: {
                        callbacks: {
                            label: function(context) {
                                const milestones = [
                                    'ALVINN - 최초 E2E 시스템',
                                    'DARPA Challenge - 모듈식 정점',
                                    'AlexNet - 딥러닝 혁명',
                                    'NVIDIA - E2E 재조명',
                                    'Transformer 등장',
                                    'Vision Transformer',
                                    'Tesla Vision-Only',
                                    'FSD v11-v12',
                                    '생성 AI 통합',
                                    'Foundation Models 표준화',
                                    'FSD v13-v14, Level 3 논의',
                                    '상용화 가속, Level 4 대중화'
                                ];
                                return milestones[context.dataIndex];
                            }
                        }
                    }
                },
                scales: {
                    y: {
                        beginAtZero: true,
                        max: 10,
                        title: {
                            display: true,
                            text: '기술 성숙도 (0: 연구 → 10: 대규모 상용화)'
                        }
                    },
                    x: {
                        title: {
                            display: true,
                            text: '연도'
                        }
                    }
                }
            }
        });

        // Comparison Radar Chart
        const comparisonCtx = document.getElementById('comparisonChart').getContext('2d');
        new Chart(comparisonCtx, {
            type: 'radar',
            data: {
                labels: ['비용 효율성', '안전성 검증', '확장성', '데이터 규모', '상용화 수준'],
                datasets: [
                    {
                        label: 'Tesla FSD',
                        data: [95, 70, 90, 100, 75],
                        borderColor: '#3b82f6',
                        backgroundColor: 'rgba(59, 130, 246, 0.2)',
                        pointBackgroundColor: '#3b82f6'
                    },
                    {
                        label: 'Waymo Driver',
                        data: [30, 95, 60, 75, 85],
                        borderColor: '#059669',
                        backgroundColor: 'rgba(5, 150, 105, 0.2)',
                        pointBackgroundColor: '#059669'
                    }
                ]
            },
            options: {
                responsive: true,
                maintainAspectRatio: false,
                plugins: {
                    title: {
                        display: true,
                        text: 'Tesla FSD vs Waymo Driver 다차원 비교',
                        font: { size: 16, weight: 'bold' }
                    },
                    legend: {
                        position: 'top'
                    }
                },
                scales: {
                    r: {
                        beginAtZero: true,
                        max: 100,
                        ticks: {
                            stepSize: 20
                        }
                    }
                }
            }
        });

        // Smooth scroll for navigation links
        document.querySelectorAll('nav a').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();
                const target = document.querySelector(this.getAttribute('href'));
                target.scrollIntoView({ behavior: 'smooth', block: 'start' });
            });
        });
    </script>
</body>
</html>
